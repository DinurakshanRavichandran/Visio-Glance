{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DinurakshanRavichandran/Visio-Glance/blob/OCT-eye-disease-detection/CourseworkModel2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waLrmcwJmvwT"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA Manual"
      ],
      "metadata": {
        "id": "hlz5JkYqRbF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to the zipped dataset in your Drive\n",
        "zip_path = '/content/drive/MyDrive/Machine learning/Dataset/OCT Dataset/Train/archive.zip'\n",
        "\n",
        "# Set extraction path in Colab\n",
        "extract_path = '/content/OCT_Dataset'\n",
        "\n",
        "# Unzipping the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset unzipped successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZlBNNNwY_9S",
        "outputId": "af1b9c98-6a0c-4b2f-c33d-e9fa6727ecff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset unzipped successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def compute_brightness_contrast(image_path):\n",
        "    \"\"\"\n",
        "    Compute the brightness (mean pixel intensity) and contrast (standard deviation of pixel intensity).\n",
        "    \"\"\"\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n",
        "    brightness = np.mean(image)  # Average pixel intensity\n",
        "    contrast = np.std(image)  # Standard deviation of pixel intensity\n",
        "    return brightness, contrast\n",
        "\n",
        "def analyze_brightness_contrast(image_dir):\n",
        "    \"\"\"\n",
        "    Analyze brightness and contrast for all images in a given directory.\n",
        "    \"\"\"\n",
        "    brightness_values = []\n",
        "    contrast_values = []\n",
        "\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    for img_file in tqdm(image_files, desc=\"Processing images\"):\n",
        "        img_path = os.path.join(image_dir, img_file)\n",
        "        brightness, contrast = compute_brightness_contrast(img_path)\n",
        "        brightness_values.append(brightness)\n",
        "        contrast_values.append(contrast)\n",
        "\n",
        "    # Plot distributions\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(brightness_values, bins=30, color='blue', alpha=0.7)\n",
        "    plt.xlabel(\"Brightness\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(\"Brightness Distribution\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(contrast_values, bins=30, color='red', alpha=0.7)\n",
        "    plt.xlabel(\"Contrast\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(\"Contrast Distribution\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example Usage\n",
        "train_dir = '/content/OCT_Dataset/OCT2017 /train'\n",
        "analyze_brightness_contrast(train_dir)\n",
        "\n",
        "val_dir = '/content/OCT_Dataset/OCT2017 /val'\n",
        "analyze_brightness_contrast(val_dir)\n",
        "\n",
        "test_dir = '/content/OCT_Dataset/OCT2017 /test'\n",
        "analyze_brightness_contrast(test_dir)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "9b-6BgOyRdzk",
        "outputId": "b6e5ca3b-7473-4de5-dbe2-090ea5ecee6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_class_distribution(dataset_path):\n",
        "    \"\"\"\n",
        "    Plot the distribution of classes in a dataset.\n",
        "    \"\"\"\n",
        "    class_counts = {}\n",
        "\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            num_images = len([f for f in os.listdir(class_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "            class_counts[class_name] = num_images\n",
        "\n",
        "    # Plot distribution\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')\n",
        "    plt.xlabel(\"Class\")\n",
        "    plt.ylabel(\"Number of Images\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.title(\"Class Distribution\")\n",
        "    plt.show()\n",
        "\n",
        "# Example Usage\n",
        "train_dir = '/content/OCT_Dataset/OCT2017 /train'\n",
        "plot_class_distribution(train_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "XZv8ZOBjZpMm",
        "outputId": "b5b33f98-42ca-407b-930f-b97e3e6d604c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def compute_pixel_diversity(image_dir):\n",
        "    \"\"\"\n",
        "    Calculate the diversity of pixel values across all images.\n",
        "    \"\"\"\n",
        "    unique_pixels = set()\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    for img_file in tqdm(image_files, desc=\"Analyzing pixel diversity\"):\n",
        "        img_path = os.path.join(image_dir, img_file)\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Grayscale for simplicity\n",
        "        unique_pixels.update(np.unique(image))  # Add unique pixel values to the set\n",
        "\n",
        "    print(f\"Total unique pixel values across dataset: {len(unique_pixels)}\")\n",
        "\n",
        "# Example Usage\n",
        "train_dir = '/content/OCT_Dataset/OCT2017 /train'\n",
        "compute_pixel_diversity(train_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgTYYeUzZqIr",
        "outputId": "46ae84a4-12e9-4043-e6e3-58838d4f1e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing pixel diversity: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique pixel values across dataset: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imagehash\n",
        "from PIL import Image\n",
        "\n",
        "def find_duplicate_images(image_dir):\n",
        "    \"\"\"\n",
        "    Detect duplicate images based on perceptual hashing.\n",
        "    \"\"\"\n",
        "    image_hashes = {}\n",
        "    duplicates = []\n",
        "\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    for img_file in tqdm(image_files, desc=\"Checking for duplicates\"):\n",
        "        img_path = os.path.join(image_dir, img_file)\n",
        "        image = Image.open(img_path)\n",
        "        img_hash = imagehash.phash(image)  # Perceptual hash\n",
        "\n",
        "        if img_hash in image_hashes:\n",
        "            duplicates.append((img_file, image_hashes[img_hash]))  # Store duplicate pairs\n",
        "        else:\n",
        "            image_hashes[img_hash] = img_file\n",
        "\n",
        "    if duplicates:\n",
        "        print(f\"Found {len(duplicates)} duplicate images:\")\n",
        "        for dup in duplicates:\n",
        "            print(f\"{dup[0]} is a duplicate of {dup[1]}\")\n",
        "    else:\n",
        "        print(\"No duplicates found.\")\n",
        "\n",
        "# Example Usage\n",
        "train_dir = '/content/OCT_Dataset/OCT2017 /train'\n",
        "find_duplicate_images(train_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "fUgMnc3GazNv",
        "outputId": "35d83025-af3e-4111-91ff-96339751a104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'imagehash'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-230e935cad64>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimagehash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_duplicate_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imagehash'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_image_overlap(train_dir, val_dir, test_dir):\n",
        "    \"\"\"\n",
        "    Check for common images across train, validation, and test sets.\n",
        "    \"\"\"\n",
        "    train_images = set(os.listdir(train_dir))\n",
        "    val_images = set(os.listdir(val_dir))\n",
        "    test_images = set(os.listdir(test_dir))\n",
        "\n",
        "    train_val_overlap = train_images.intersection(val_images)\n",
        "    train_test_overlap = train_images.intersection(test_images)\n",
        "    val_test_overlap = val_images.intersection(test_images)\n",
        "\n",
        "    print(f\"Train-Val Overlap: {len(train_val_overlap)} images\")\n",
        "    print(f\"Train-Test Overlap: {len(train_test_overlap)} images\")\n",
        "    print(f\"Val-Test Overlap: {len(val_test_overlap)} images\")\n",
        "\n",
        "    if train_val_overlap:\n",
        "        print(\"⚠️ Warning: The following images are in both Train and Validation set:\")\n",
        "        print(train_val_overlap)\n",
        "    if train_test_overlap:\n",
        "        print(\"⚠️ Warning: The following images are in both Train and Test set:\")\n",
        "        print(train_test_overlap)\n",
        "    if val_test_overlap:\n",
        "        print(\"⚠️ Warning: The following images are in both Validation and Test set:\")\n",
        "        print(val_test_overlap)\n",
        "\n",
        "# Example Usage\n",
        "train_dir = \"/path/to/train/images\"\n",
        "val_dir = \"/path/to/val/images\"\n",
        "test_dir = \"/path/to/test/images\"\n",
        "check_image_overlap(train_dir, val_dir, test_dir)\n"
      ],
      "metadata": {
        "id": "MncWHtd9bMjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFL33XUlMNIV"
      },
      "source": [
        "# Model 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-S1izf9MyYp"
      },
      "source": [
        "## Preprocessing and pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EGyJ6kfxehm"
      },
      "source": [
        "1. Set Up Your Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLVYWsnOxiuS",
        "outputId": "93f9047d-f946-48df-d7fe-9c82e2d55fdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment ready!\n"
          ]
        }
      ],
      "source": [
        "# Core Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Average\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# For reproducibility\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Environment ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GE3p7wiy38e",
        "outputId": "ade068ae-2809-42ae-ac6a-b80de36915ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset unzipped successfully!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to the zipped dataset in your Drive\n",
        "zip_path = '/content/drive/MyDrive/Machine learning/Dataset/OCT Dataset/Train/archive.zip'\n",
        "\n",
        "# Set extraction path in Colab\n",
        "extract_path = '/content/OCT_Dataset'\n",
        "\n",
        "# Unzipping the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset unzipped successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOp5CQG7xmCB"
      },
      "source": [
        "2. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idz9gr8KzI76"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eJ73b0Py01u",
        "outputId": "38134bf1-fb8c-4bfd-b423-ef661c72416a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data limited to 8,000 images per class.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def limit_images_per_class(src_dir, dest_dir, max_images=8000):\n",
        "    \"\"\"\n",
        "    Copies a limited number of images from source to destination for each class.\n",
        "    Parameters:\n",
        "        src_dir (str): Source directory where the images are stored.\n",
        "        dest_dir (str): Destination directory to store the limited images.\n",
        "        max_images (int): Maximum number of images per class.\n",
        "    \"\"\"\n",
        "    # Ensure destination directory exists\n",
        "    if not os.path.exists(dest_dir):\n",
        "        os.makedirs(dest_dir)\n",
        "\n",
        "    # Iterate through each class folder in the source directory\n",
        "    for category in os.listdir(src_dir):\n",
        "        category_path = os.path.join(src_dir, category)\n",
        "        dest_category_path = os.path.join(dest_dir, category)\n",
        "\n",
        "        if not os.path.exists(dest_category_path):\n",
        "            os.makedirs(dest_category_path)\n",
        "\n",
        "        # Get list of images\n",
        "        images = [\n",
        "            os.path.join(category_path, img)\n",
        "            for img in os.listdir(category_path)\n",
        "            if img.endswith(('.jpeg', '.jpg', '.png'))\n",
        "        ]\n",
        "\n",
        "        # Shuffle and select limited images\n",
        "        random.shuffle(images)\n",
        "        limited_images = images[:max_images]\n",
        "\n",
        "        # Copy limited images to destination folder\n",
        "        for img_path in limited_images:\n",
        "            shutil.copy(img_path, os.path.join(dest_category_path, os.path.basename(img_path)))\n",
        "\n",
        "# Source and destination paths\n",
        "src_train_dir = '/content/OCT_Dataset/OCT2017 /train'\n",
        "dest_train_dir = '/content/OCT_Dataset/OCT2017 /train_limited'\n",
        "\n",
        "# Apply the function\n",
        "limit_images_per_class(src_train_dir, dest_train_dir, max_images=8000)\n",
        "\n",
        "print(\"Training data limited to 8,000 images per class.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILiQ0lqkx4mt"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/OCT_Dataset/OCT2017 /train_limited'\n",
        "val_dir = '/content/OCT_Dataset/OCT2017 /val'\n",
        "test_dir = '/content/OCT_Dataset/OCT2017 /test'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LDSowgyx7Uk"
      },
      "source": [
        "Define Data Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9MJpgCkyAuq",
        "outputId": "b6cfedc5-4ad9-4574-82c5-fd92781849da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 83484 images belonging to 4 classes.\n",
            "Found 32 images belonging to 4 classes.\n",
            "Found 968 images belonging to 4 classes.\n",
            "Data generators with grayscale conversion created successfully.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Preprocess grayscale images to RGB\n",
        "def preprocess_grayscale_to_rgb(image):\n",
        "    \"\"\"\n",
        "    Converts a grayscale image to RGB and applies normalization for EfficientNet.\n",
        "    \"\"\"\n",
        "    image = tf.image.decode_jpeg(image, channels=1)  # Decode as grayscale\n",
        "    image = tf.image.resize(image, [224, 224])      # Resize to target dimensions\n",
        "    image = tf.image.grayscale_to_rgb(image)        # Convert grayscale to RGB\n",
        "    image = tf.keras.applications.efficientnet.preprocess_input(image)  # Normalize for EfficientNet\n",
        "    return image\n",
        "\n",
        "# Wrap function for use in ImageDataGenerator\n",
        "def grayscale_to_rgb_preprocessing_function(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    return preprocess_grayscale_to_rgb(image)\n",
        "\n",
        "# Create data generators\n",
        "def create_data_generators_with_grayscale_conversion(train_dir, val_dir, test_dir, img_height=224, img_width=224, batch_size=32):\n",
        "    \"\"\"\n",
        "    Creates data generators with grayscale-to-RGB conversion and augmentations.\n",
        "\n",
        "    Parameters:\n",
        "        train_dir (str): Path to the training directory.\n",
        "        val_dir (str): Path to the validation directory.\n",
        "        test_dir (str): Path to the testing directory.\n",
        "        img_height (int): Target image height (default: 224).\n",
        "        img_width (int): Target image width (default: 224).\n",
        "        batch_size (int): Batch size for generators (default: 32).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_generator, val_generator, test_generator)\n",
        "    \"\"\"\n",
        "    # Training data generator with augmentation\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest',\n",
        "        preprocessing_function=grayscale_to_rgb_preprocessing_function  # Apply grayscale-to-RGB preprocessing\n",
        "    )\n",
        "\n",
        "    # Validation and testing data generators (only normalization)\n",
        "    val_test_datagen = ImageDataGenerator(preprocessing_function=grayscale_to_rgb_preprocessing_function)\n",
        "\n",
        "    # Training data generator\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,                  # Training directory\n",
        "        target_size=(img_height, img_width),  # Resize images\n",
        "        batch_size=batch_size,      # Batch size\n",
        "        class_mode='categorical'    # For multi-class classification\n",
        "    )\n",
        "\n",
        "    # Validation data generator\n",
        "    val_generator = val_test_datagen.flow_from_directory(\n",
        "        val_dir,                    # Validation directory\n",
        "        target_size=(img_height, img_width),  # Resize images\n",
        "        batch_size=batch_size,      # Batch size\n",
        "        class_mode='categorical'    # For multi-class classification\n",
        "    )\n",
        "\n",
        "    # Test data generator\n",
        "    test_generator = val_test_datagen.flow_from_directory(\n",
        "        test_dir,                   # Testing directory\n",
        "        target_size=(img_height, img_width),  # Resize images\n",
        "        batch_size=batch_size,      # Batch size\n",
        "        class_mode='categorical'    # For multi-class classification\n",
        "    )\n",
        "\n",
        "    print(\"Data generators with grayscale conversion created successfully.\")\n",
        "    return train_generator, val_generator, test_generator\n",
        "\n",
        "# Paths to dataset\n",
        "train_dir = '/content/OCT_Dataset/OCT2017 /train'\n",
        "val_dir = '/content/OCT_Dataset/OCT2017 /val'\n",
        "test_dir = '/content/OCT_Dataset/OCT2017 /test'\n",
        "\n",
        "# Generate the data generators\n",
        "train_generator, val_generator, test_generator = create_data_generators_with_grayscale_conversion(train_dir, val_dir, test_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHcvMyhgM4Tt"
      },
      "source": [
        "## Train individual models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lhaJYtTyDZ_"
      },
      "source": [
        "4. Train Individual Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "hjFCN-J_yHBU",
        "outputId": "52064e0c-556b-4f71-a117-35ba1337be3a"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ad4ed97ebbd0>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mefficientnet_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tune_efficientnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mhistory_efficientnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mefficientnet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mefficientnet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fine_tuned_efficientnet.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-8c54b902a9b1>\u001b[0m in \u001b[0;36mgrayscale_to_rgb_preprocessing_function\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Wrap function for use in ImageDataGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrayscale_to_rgb_preprocessing_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocess_grayscale_to_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "def fine_tune_efficientnet():\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Unfreeze some layers for fine-tuning\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:200]:  # Freeze first 200 layers (adjustable)\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)  # Dropout for regularization\n",
        "    output = layers.Dense(4, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "    # Compile with a lower learning rate\n",
        "    optimizer = optimizers.Adam(learning_rate=1e-4)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "efficientnet_model = fine_tune_efficientnet()\n",
        "history_efficientnet = efficientnet_model.fit(train_generator, validation_data=val_generator, epochs=12)\n",
        "efficientnet_model.save('fine_tuned_efficientnet.h5')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RENem0hsyKra",
        "outputId": "4c95472a-4385-4451-d34d-51524e01482d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n",
            "Epoch 1/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m592s\u001b[0m 564ms/step - accuracy: 0.3914 - loss: 1.2745 - val_accuracy: 0.3125 - val_loss: 5.8072\n",
            "Epoch 2/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m578s\u001b[0m 575ms/step - accuracy: 0.5320 - loss: 1.0769 - val_accuracy: 0.5938 - val_loss: 0.9632\n",
            "Epoch 3/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 568ms/step - accuracy: 0.5742 - loss: 0.9895 - val_accuracy: 0.4375 - val_loss: 1.4035\n",
            "Epoch 4/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 557ms/step - accuracy: 0.6012 - loss: 0.9422 - val_accuracy: 0.5312 - val_loss: 1.0157\n",
            "Epoch 5/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 554ms/step - accuracy: 0.6258 - loss: 0.8929 - val_accuracy: 0.5938 - val_loss: 1.8408\n",
            "Epoch 6/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 548ms/step - accuracy: 0.6385 - loss: 0.8729 - val_accuracy: 0.8125 - val_loss: 0.6564\n",
            "Epoch 7/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m570s\u001b[0m 556ms/step - accuracy: 0.6526 - loss: 0.8380 - val_accuracy: 0.6562 - val_loss: 0.8641\n",
            "Epoch 8/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 549ms/step - accuracy: 0.6637 - loss: 0.8230 - val_accuracy: 0.6250 - val_loss: 1.1007\n",
            "Epoch 9/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 553ms/step - accuracy: 0.6720 - loss: 0.7989 - val_accuracy: 0.6875 - val_loss: 0.9964\n",
            "Epoch 10/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m552s\u001b[0m 547ms/step - accuracy: 0.6871 - loss: 0.7726 - val_accuracy: 0.5625 - val_loss: 1.0754\n",
            "Epoch 11/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 547ms/step - accuracy: 0.6991 - loss: 0.7551 - val_accuracy: 0.8125 - val_loss: 0.4664\n",
            "Epoch 12/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 548ms/step - accuracy: 0.7058 - loss: 0.7353 - val_accuracy: 0.8125 - val_loss: 0.4500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "def fine_tune_resnet():\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Unfreeze last few layers for fine-tuning\n",
        "    for layer in base_model.layers[:143]:  # Freeze first 143 layers (adjustable)\n",
        "        layer.trainable = False\n",
        "    for layer in base_model.layers[143:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)  # Dropout for regularization\n",
        "    output = layers.Dense(4, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "    # Compile with a low learning rate for fine-tuning\n",
        "    optimizer = optimizers.Adam(learning_rate=1e-4)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "resnet_model = fine_tune_resnet()\n",
        "history_resnet = resnet_model.fit(train_generator, validation_data=val_generator, epochs=12)\n",
        "resnet_model.save('fine_tuned_resnet.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbmHdzIAyNLA",
        "outputId": "1840a8c1-f8ee-4f57-8be7-5266a6219c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m661s\u001b[0m 627ms/step - accuracy: 0.6589 - loss: 0.8312 - val_accuracy: 0.9062 - val_loss: 0.3138\n",
            "Epoch 2/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m647s\u001b[0m 608ms/step - accuracy: 0.7702 - loss: 0.6000 - val_accuracy: 0.9062 - val_loss: 0.3083\n",
            "Epoch 3/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 606ms/step - accuracy: 0.7847 - loss: 0.5611 - val_accuracy: 0.9062 - val_loss: 0.2697\n",
            "Epoch 4/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 599ms/step - accuracy: 0.7859 - loss: 0.5491 - val_accuracy: 0.8438 - val_loss: 0.3913\n",
            "Epoch 5/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 599ms/step - accuracy: 0.7971 - loss: 0.5278 - val_accuracy: 0.9375 - val_loss: 0.2154\n",
            "Epoch 6/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 597ms/step - accuracy: 0.7963 - loss: 0.5368 - val_accuracy: 0.8750 - val_loss: 0.2852\n",
            "Epoch 7/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 608ms/step - accuracy: 0.8041 - loss: 0.5105 - val_accuracy: 0.9062 - val_loss: 0.2050\n",
            "Epoch 8/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m603s\u001b[0m 592ms/step - accuracy: 0.8083 - loss: 0.5037 - val_accuracy: 0.9062 - val_loss: 0.2034\n",
            "Epoch 9/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 594ms/step - accuracy: 0.8074 - loss: 0.5048 - val_accuracy: 0.8750 - val_loss: 0.2753\n",
            "Epoch 10/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 591ms/step - accuracy: 0.8098 - loss: 0.5028 - val_accuracy: 0.9375 - val_loss: 0.2193\n",
            "Epoch 11/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m640s\u001b[0m 609ms/step - accuracy: 0.8160 - loss: 0.4832 - val_accuracy: 0.9375 - val_loss: 0.2082\n",
            "Epoch 12/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m633s\u001b[0m 629ms/step - accuracy: 0.8139 - loss: 0.4826 - val_accuracy: 0.9375 - val_loss: 0.2131\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import DenseNet121\n",
        "\n",
        "def create_densenet():\n",
        "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    x = base_model.output\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "    output = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "densenet_model = create_densenet()\n",
        "history_densenet = densenet_model.fit(train_generator, validation_data=val_generator, epochs=12)\n",
        "densenet_model.save('densenet_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKqtwgKN9hR-",
        "outputId": "f6d99f71-dfed-4f96-be75-5535115f40f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.9426 - loss: 0.1915\n",
            "Test Loss: 0.1857\n",
            "Test Accuracy: 0.9432\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have your test_generator defined as before\n",
        "test_loss, test_accuracy = densenet_model.evaluate(test_generator)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zubW_0JtyP2U"
      },
      "source": [
        "5. Create and Train the Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BwWjewq9yTFP",
        "outputId": "9459dcbd-395f-4348-d119-887d6812dcc0"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'efficientnet_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-815c2541dba7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load trained models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mefficientnet_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'efficientnet_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresnet_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resnet_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdensenet_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'densenet_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    192\u001b[0m         )\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    559\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'efficientnet_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ],
      "source": [
        "# Load trained models\n",
        "efficientnet_model = load_model('fine_tuned_efficientnet.h5')\n",
        "resnet_model = load_model('fine_tuned_resnet.h5')\n",
        "densenet_model = load_model('densenet_model.h5')\n",
        "\n",
        "# Freeze all models\n",
        "for model in [efficientnet_model, resnet_model, densenet_model]:\n",
        "    model.trainable = False\n",
        "\n",
        "# Create ensemble output\n",
        "efficientnet_output = efficientnet_model.output\n",
        "resnet_output = resnet_model.output\n",
        "densenet_output = densenet_model.output\n",
        "\n",
        "ensemble_output = Average()([efficientnet_output, resnet_output, densenet_output])\n",
        "\n",
        "# Create ensemble model\n",
        "ensemble_model = Model(\n",
        "    inputs=[efficientnet_model.input, resnet_model.input, densenet_model.input],\n",
        "    outputs=ensemble_output\n",
        ")\n",
        "\n",
        "ensemble_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the ensemble (optional fine-tuning)\n",
        "history_ensemble = ensemble_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=5\n",
        ")\n",
        "\n",
        "ensemble_model.save('ensemble_model.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuQTnocnyV6L"
      },
      "source": [
        "Evaluate the Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHPw3XZy4fAA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5JLu571yZo-"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = ensemble_model.evaluate(test_generator)\n",
        "print(f\"Ensemble Test Accuracy: {test_accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ5DBXrvxu8q"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqxerra0MUeZ"
      },
      "source": [
        "# Model 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVkTDcEfKwoK"
      },
      "source": [
        "## Preprocessing and Data pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpWACMG_NWJs"
      },
      "source": [
        "Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ala29d3XMXUa",
        "outputId": "5c3f0ddd-9c6d-4b86-b723-ade39971a0e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment ready!\n"
          ]
        }
      ],
      "source": [
        "# Core Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Average\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# For reproducibility\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Environment ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRGY_QjsNQM1"
      },
      "source": [
        "Unzipping the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAEOMIteNUYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ce659c1-751a-4cbf-f557-268cb508f3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset unzipped successfully!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to the zipped dataset in your Drive\n",
        "zip_path = '/content/drive/MyDrive/Machine learning/Dataset/OCT Dataset/Train/archive.zip'\n",
        "\n",
        "# Set extraction path in Colab\n",
        "extract_path = '/content/OCT_Dataset'\n",
        "\n",
        "# Unzipping the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset unzipped successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCTIy0-mNczm"
      },
      "source": [
        "Limiting images per class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL-q-2B0NVZR",
        "outputId": "595cfa55-ba7b-42bd-cc4b-74bfe3c4f737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data limited to 8,000 images per class.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def limit_images_per_class(src_dir, dest_dir, max_images=8000):\n",
        "    \"\"\"\n",
        "    Copies a limited number of images from source to destination for each class.\n",
        "    Parameters:\n",
        "        src_dir (str): Source directory where the images are stored.\n",
        "        dest_dir (str): Destination directory to store the limited images.\n",
        "        max_images (int): Maximum number of images per class.\n",
        "    \"\"\"\n",
        "    # Ensure destination directory exists\n",
        "    if not os.path.exists(dest_dir):\n",
        "        os.makedirs(dest_dir)\n",
        "\n",
        "    # Iterate through each class folder in the source directory\n",
        "    for category in os.listdir(src_dir):\n",
        "        category_path = os.path.join(src_dir, category)\n",
        "        dest_category_path = os.path.join(dest_dir, category)\n",
        "\n",
        "        if not os.path.exists(dest_category_path):\n",
        "            os.makedirs(dest_category_path)\n",
        "\n",
        "        # Get list of images\n",
        "        images = [\n",
        "            os.path.join(category_path, img)\n",
        "            for img in os.listdir(category_path)\n",
        "            if img.endswith(('.jpeg', '.jpg', '.png'))\n",
        "        ]\n",
        "\n",
        "        # Shuffle and select limited images\n",
        "        random.shuffle(images)\n",
        "        limited_images = images[:max_images]\n",
        "\n",
        "        # Copy limited images to destination folder\n",
        "        for img_path in limited_images:\n",
        "            shutil.copy(img_path, os.path.join(dest_category_path, os.path.basename(img_path)))\n",
        "\n",
        "# Source and destination paths\n",
        "src_train_dir = '/content/OCT_Dataset/OCT2017 /train'\n",
        "dest_train_dir = '/content/OCT_Dataset/OCT2017 /train_limited'\n",
        "\n",
        "# Apply the function\n",
        "limit_images_per_class(src_train_dir, dest_train_dir, max_images=8000)\n",
        "\n",
        "print(\"Training data limited to 8,000 images per class.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS82vsBfNi6j"
      },
      "source": [
        "Setting paths for training, testing and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dgfo2oB-NwaE"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/OCT_Dataset/OCT2017 /train_limited'\n",
        "val_dir = '/content/OCT_Dataset/OCT2017 /val'\n",
        "test_dir = '/content/OCT_Dataset/OCT2017 /test'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VyfsZN2Dz7m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HhjT1wZNpOY"
      },
      "source": [
        "Pre processing and creating data generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8Xoek1BN49y",
        "outputId": "7d26308c-e7a9-49be-fb0c-40c1f7587c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 32000 images belonging to 4 classes.\n",
            "Found 32 images belonging to 4 classes.\n",
            "Found 968 images belonging to 4 classes.\n",
            "Data generators configured with grayscale-to-RGB conversion.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Preprocess grayscale images to RGB\n",
        "def preprocess_grayscale_to_rgb(image):\n",
        "    \"\"\"\n",
        "    Converts a grayscale NumPy array to RGB and applies normalization for EfficientNet.\n",
        "    \"\"\"\n",
        "    # Check if the image is already RGB\n",
        "    if image.shape[-1] == 3:\n",
        "        # If already RGB, skip the conversion\n",
        "        image = tf.image.resize(image, [224, 224])\n",
        "    else:\n",
        "        if len(image.shape) == 2:  # If grayscale, add channel dimension\n",
        "            image = tf.expand_dims(image, axis=-1)  # Add channel dimension\n",
        "        image = tf.image.resize(image, [224, 224])  # Resize to target dimensions\n",
        "        image = tf.image.grayscale_to_rgb(image)  # Convert grayscale to RGB\n",
        "    image = tf.keras.applications.efficientnet.preprocess_input(image)  # Normalize for EfficientNet\n",
        "    return image\n",
        "\n",
        "\n",
        "# Create data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    preprocessing_function=preprocess_grayscale_to_rgb  # Use updated function\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(preprocessing_function=preprocess_grayscale_to_rgb)\n",
        "\n",
        "# Training data generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    color_mode='rgb',  # Ensure grayscale input\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Validation data generator\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Testing data generator\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # Maintain order for evaluation\n",
        ")\n",
        "\n",
        "print(\"Data generators configured with grayscale-to-RGB conversion.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GXswZaSKf1p"
      },
      "source": [
        "## Individual model training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3xGWFc81RqX"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqlSeys4Ki7g"
      },
      "source": [
        "EfficientNetB0 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69Mj-VfIK7Gk",
        "outputId": "a3dd7c91-e181-4d94-b6d7-4aabeb7449ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m575s\u001b[0m 534ms/step - accuracy: 0.7260 - loss: 0.6781 - val_accuracy: 0.9688 - val_loss: 0.0536\n",
            "Epoch 2/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 512ms/step - accuracy: 0.8716 - loss: 0.3543 - val_accuracy: 1.0000 - val_loss: 0.0217\n",
            "Epoch 3/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m507s\u001b[0m 504ms/step - accuracy: 0.8909 - loss: 0.3055 - val_accuracy: 0.9688 - val_loss: 0.0942\n",
            "Epoch 4/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 503ms/step - accuracy: 0.9009 - loss: 0.2759 - val_accuracy: 0.9688 - val_loss: 0.0613\n",
            "Epoch 5/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 510ms/step - accuracy: 0.9049 - loss: 0.2666 - val_accuracy: 1.0000 - val_loss: 0.0075\n",
            "Epoch 6/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 510ms/step - accuracy: 0.9109 - loss: 0.2445 - val_accuracy: 1.0000 - val_loss: 0.0085\n",
            "Epoch 7/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 501ms/step - accuracy: 0.9143 - loss: 0.2373 - val_accuracy: 1.0000 - val_loss: 0.0129\n",
            "Epoch 8/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 501ms/step - accuracy: 0.9189 - loss: 0.2291 - val_accuracy: 1.0000 - val_loss: 0.0090\n",
            "Epoch 9/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 502ms/step - accuracy: 0.9231 - loss: 0.2181 - val_accuracy: 1.0000 - val_loss: 0.0118\n",
            "Epoch 10/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 501ms/step - accuracy: 0.9230 - loss: 0.2102 - val_accuracy: 1.0000 - val_loss: 0.0106\n",
            "Epoch 11/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 499ms/step - accuracy: 0.9242 - loss: 0.2094 - val_accuracy: 1.0000 - val_loss: 0.0145\n",
            "Epoch 12/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 503ms/step - accuracy: 0.9246 - loss: 0.2022 - val_accuracy: 1.0000 - val_loss: 0.0084\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EfficientNetB0 model trained and saved successfully.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "def fine_tune_efficientnet():\n",
        "    \"\"\"\n",
        "    Fine-tunes EfficientNetB0 with a frozen base and custom classification head.\n",
        "    \"\"\"\n",
        "    # Load EfficientNetB0 with pre-trained ImageNet weights\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Freeze the first 200 layers\n",
        "    for layer in base_model.layers[:200]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Add custom layers on top of the base model\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)  # Global pooling\n",
        "    x = layers.Dense(512, activation='relu')(x)  # Fully connected layer\n",
        "    x = layers.Dropout(0.5)(x)  # Dropout for regularization\n",
        "    output = layers.Dense(4, activation='softmax')(x)  # Output layer for 4 classes\n",
        "\n",
        "    # Create the model\n",
        "    model = models.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "    # Compile the model with Adam optimizer and categorical crossentropy loss\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Instantiate the model\n",
        "efficientnet_model = fine_tune_efficientnet()\n",
        "\n",
        "# Train the model using the data generator\n",
        "history_efficientnet = efficientnet_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=12\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "#efficientnet_model.save('efficientnet.h5')\n",
        "efficientnet_model.save('/content/drive/MyDrive/DSGP Files/Models/efficientnet.h5')\n",
        "\n",
        "\n",
        "# Output message for clarity\n",
        "print(\"EfficientNetB0 model trained and saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNqyi6j5LFav"
      },
      "source": [
        "ResNet50 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyZOGkTPLKTI",
        "outputId": "afec5e81-fedd-4a76-fb2e-57ad7f3a7ad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 601ms/step - accuracy: 0.7684 - loss: 0.6014 - val_accuracy: 0.9688 - val_loss: 0.0927\n",
            "Epoch 2/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 586ms/step - accuracy: 0.8902 - loss: 0.3141 - val_accuracy: 0.9688 - val_loss: 0.0872\n",
            "Epoch 3/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m579s\u001b[0m 576ms/step - accuracy: 0.9026 - loss: 0.2735 - val_accuracy: 0.9688 - val_loss: 0.0677\n",
            "Epoch 4/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 573ms/step - accuracy: 0.9084 - loss: 0.2544 - val_accuracy: 0.9688 - val_loss: 0.0539\n",
            "Epoch 5/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m573s\u001b[0m 570ms/step - accuracy: 0.9181 - loss: 0.2333 - val_accuracy: 0.9688 - val_loss: 0.0564\n",
            "Epoch 6/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 573ms/step - accuracy: 0.9182 - loss: 0.2298 - val_accuracy: 1.0000 - val_loss: 0.0277\n",
            "Epoch 7/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 573ms/step - accuracy: 0.9250 - loss: 0.2188 - val_accuracy: 0.9375 - val_loss: 0.0904\n",
            "Epoch 8/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m625s\u001b[0m 576ms/step - accuracy: 0.9282 - loss: 0.2037 - val_accuracy: 0.9688 - val_loss: 0.0486\n",
            "Epoch 9/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 572ms/step - accuracy: 0.9278 - loss: 0.2028 - val_accuracy: 1.0000 - val_loss: 0.0087\n",
            "Epoch 10/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m580s\u001b[0m 577ms/step - accuracy: 0.9306 - loss: 0.1960 - val_accuracy: 1.0000 - val_loss: 0.0297\n",
            "Epoch 11/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 571ms/step - accuracy: 0.9357 - loss: 0.1836 - val_accuracy: 0.9688 - val_loss: 0.0499\n",
            "Epoch 12/12\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m577s\u001b[0m 573ms/step - accuracy: 0.9354 - loss: 0.1812 - val_accuracy: 1.0000 - val_loss: 0.0215\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "def fine_tune_resnet():\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    for layer in base_model.layers[:143]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    output = layers.Dense(4, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "resnet_model = fine_tune_resnet()\n",
        "history_resnet = resnet_model.fit(train_generator, validation_data=val_generator, epochs=12)\n",
        "#resnet_model.save('resnet.h5')\n",
        "resnet_model.save('/content/drive/MyDrive/DSGP Files/Models/resnet.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9GjTcUOLOKq"
      },
      "source": [
        "Dense Net model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "qqkpxiTPMbxO",
        "outputId": "c4bb6388-cb91-48b8-ecde-59a640d81eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m 473/1000\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3:51:06\u001b[0m 26s/step - accuracy: 0.6672 - loss: 0.8701"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9c7762fb0e29>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Train the model with class weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m history_densenet = densenet_model.fit(\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "def create_densenet():\n",
        "    # Load DenseNet121 with ImageNet weights\n",
        "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Unfreeze the last 30 layers for fine-tuning\n",
        "    for layer in base_model.layers[-30:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # Add custom layers on top\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.5)(x)  # Increased dropout for regularization\n",
        "    x = layers.Dense(512, activation='relu')(x)  # Increased units in dense layer\n",
        "    x = layers.Dropout(0.3)(x)  # Additional dropout layer\n",
        "    output = layers.Dense(4, activation='softmax')(x)  # Output layer for 4 classes\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Compute class weights for handling imbalances\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Create the DenseNet model\n",
        "densenet_model = create_densenet()\n",
        "\n",
        "# Train the model with class weights\n",
        "history_densenet = densenet_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=12,\n",
        "    class_weight=class_weights_dict\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "#densenet_model.save('densenet_model.h5')\n",
        "densenet_model.save('/content/drive/MyDrive/DSGP Files/Models/densenet_model.h5')\n",
        "\n",
        "# Output message\n",
        "print(\"DenseNet model trained and saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkBt7BGZ0s_s"
      },
      "source": [
        "### Model Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDzUFNrKE8D8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define function to evaluate and compute metrics\n",
        "def evaluate_model(model, generator, class_names):\n",
        "    # Predict on test data\n",
        "    predictions = model.predict(generator)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    true_classes = generator.classes\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(true_classes, predicted_classes, target_names=class_names)\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(true_classes, predicted_classes)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    return report, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt__4FZ0FAWL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load models from their respective files\n",
        "densenet_model = load_model('densenet_model.h5')\n",
        "efficientnet_model = load_model('efficientnet.h5')\n",
        "resnet_model = load_model('resnet.h5')\n",
        "\n",
        "# Class labels\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Evaluate DenseNet model\n",
        "print(\"DenseNet Model Evaluation:\")\n",
        "densenet_report, densenet_cm = evaluate_model(densenet_model, test_generator, class_labels)\n",
        "\n",
        "# Evaluate ResNet model\n",
        "print(\"\\nResNet Model Evaluation:\")\n",
        "resnet_report, resnet_cm = evaluate_model(resnet_model, test_generator, class_labels)\n",
        "\n",
        "# Evaluate EfficientNet model\n",
        "print(\"\\nEfficientNet Model Evaluation:\")\n",
        "efficientnet_report, efficientnet_cm = evaluate_model(efficientnet_model, test_generator, class_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOAXLe2fFFTK"
      },
      "outputs": [],
      "source": [
        "def plot_classification_report(report, title='Classification Metrics', figsize=(10, 8)):\n",
        "    lines = report.split(\"\\n\")\n",
        "    classes, precision, recall, f1_score, support = [], [], [], [], []\n",
        "\n",
        "    for line in lines[2:(len(lines) - 5)]:\n",
        "        row = line.split()\n",
        "        classes.append(row[0])\n",
        "        precision.append(float(row[1]))\n",
        "        recall.append(float(row[2]))\n",
        "        f1_score.append(float(row[3]))\n",
        "        support.append(int(row[4]))\n",
        "\n",
        "    x = np.arange(len(classes))\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.bar(x - 0.2, precision, width=0.2, label='Precision', color='blue')\n",
        "    plt.bar(x, recall, width=0.2, label='Recall', color='green')\n",
        "    plt.bar(x + 0.2, f1_score, width=0.2, label='F1-score', color='orange')\n",
        "\n",
        "    plt.xticks(x, classes, rotation=45)\n",
        "    plt.xlabel(\"Classes\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(title)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for DenseNet\n",
        "plot_classification_report(densenet_report, title='DenseNet Metrics')\n",
        "\n",
        "# Plot for ResNet\n",
        "plot_classification_report(resnet_report, title='ResNet Metrics')\n",
        "\n",
        "# Plot for EfficientNet\n",
        "plot_classification_report(efficientnet_report, title='EfficientNet Metrics')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DonhSaGmFMhx"
      },
      "outputs": [],
      "source": [
        "def predict_on_sample(generator, model, class_labels):\n",
        "    # Get a batch of images and labels\n",
        "    images, labels = next(generator)\n",
        "    true_labels = np.argmax(labels, axis=1)\n",
        "\n",
        "    predictions = model.predict(images)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Plot a few samples\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].astype('uint8'))\n",
        "        true_label = class_labels[true_labels[i]]\n",
        "        predicted_label = class_labels[predicted_labels[i]]\n",
        "        plt.title(f\"True: {true_label}\\nPredicted: {predicted_label}\")\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Predict and visualize DenseNet results\n",
        "predict_on_sample(test_generator, densenet_model, class_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVyTuBz4GAiv"
      },
      "source": [
        "## Testing file format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3gCkabiGDgj",
        "outputId": "158261ff-75c6-41c2-d895-f25d499e8ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "efficientnet.h5 exists. Size: 41273352 bytes\n",
            "efficientnet.h5 opened successfully. Keys: ['model_weights', 'optimizer_weights']\n",
            "resnet.h5 exists. Size: 104857600 bytes\n",
            "Error opening resnet.h5: Unable to synchronously open file (truncated file: eof = 104857600, sblock->base_addr = 0, stored_eof = 227403720)\n",
            "densenet_model.h5 exists. Size: 91800496 bytes\n",
            "densenet_model.h5 opened successfully. Keys: ['model_weights', 'optimizer_weights']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import h5py\n",
        "\n",
        "file_paths = [\"efficientnet.h5\", \"resnet.h5\", \"densenet_model.h5\"]\n",
        "for path in file_paths:\n",
        "    if os.path.exists(path):\n",
        "        print(f\"{path} exists. Size: {os.path.getsize(path)} bytes\")\n",
        "        try:\n",
        "            with h5py.File(path, 'r') as f:\n",
        "                print(f\"{path} opened successfully. Keys: {list(f.keys())}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error opening {path}: {e}\")\n",
        "    else:\n",
        "        print(f\"{path} does not exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uzwnpEgGTAm",
        "outputId": "812d70a5-7bf3-43d1-e62e-d326c5793022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "efficientnet.h5 loaded successfully.\n",
            "Error loading resnet.h5: Unable to synchronously open file (truncated file: eof = 114294784, sblock->base_addr = 0, stored_eof = 227403720)\n",
            "densenet_model.h5 loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_paths = [\"efficientnet.h5\", \"resnet.h5\", \"densenet_model.h5\"]\n",
        "for path in model_paths:\n",
        "    try:\n",
        "        model = load_model(path, compile=False)\n",
        "        print(f\"{path} loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IZzWFEOGWzQ",
        "outputId": "f22c8935-af5f-47c8-fd4d-29308cdfdd84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Layer names in EfficientNet:\n",
            "input_layer_6\n",
            "rescaling_12\n",
            "normalization_6\n",
            "rescaling_13\n",
            "stem_conv_pad\n",
            "stem_conv\n",
            "stem_bn\n",
            "stem_activation\n",
            "block1a_dwconv\n",
            "block1a_bn\n",
            "block1a_activation\n",
            "block1a_se_squeeze\n",
            "block1a_se_reshape\n",
            "block1a_se_reduce\n",
            "block1a_se_expand\n",
            "block1a_se_excite\n",
            "block1a_project_conv\n",
            "block1a_project_bn\n",
            "block2a_expand_conv\n",
            "block2a_expand_bn\n",
            "block2a_expand_activation\n",
            "block2a_dwconv_pad\n",
            "block2a_dwconv\n",
            "block2a_bn\n",
            "block2a_activation\n",
            "block2a_se_squeeze\n",
            "block2a_se_reshape\n",
            "block2a_se_reduce\n",
            "block2a_se_expand\n",
            "block2a_se_excite\n",
            "block2a_project_conv\n",
            "block2a_project_bn\n",
            "block2b_expand_conv\n",
            "block2b_expand_bn\n",
            "block2b_expand_activation\n",
            "block2b_dwconv\n",
            "block2b_bn\n",
            "block2b_activation\n",
            "block2b_se_squeeze\n",
            "block2b_se_reshape\n",
            "block2b_se_reduce\n",
            "block2b_se_expand\n",
            "block2b_se_excite\n",
            "block2b_project_conv\n",
            "block2b_project_bn\n",
            "block2b_drop\n",
            "block2b_add\n",
            "block3a_expand_conv\n",
            "block3a_expand_bn\n",
            "block3a_expand_activation\n",
            "block3a_dwconv_pad\n",
            "block3a_dwconv\n",
            "block3a_bn\n",
            "block3a_activation\n",
            "block3a_se_squeeze\n",
            "block3a_se_reshape\n",
            "block3a_se_reduce\n",
            "block3a_se_expand\n",
            "block3a_se_excite\n",
            "block3a_project_conv\n",
            "block3a_project_bn\n",
            "block3b_expand_conv\n",
            "block3b_expand_bn\n",
            "block3b_expand_activation\n",
            "block3b_dwconv\n",
            "block3b_bn\n",
            "block3b_activation\n",
            "block3b_se_squeeze\n",
            "block3b_se_reshape\n",
            "block3b_se_reduce\n",
            "block3b_se_expand\n",
            "block3b_se_excite\n",
            "block3b_project_conv\n",
            "block3b_project_bn\n",
            "block3b_drop\n",
            "block3b_add\n",
            "block4a_expand_conv\n",
            "block4a_expand_bn\n",
            "block4a_expand_activation\n",
            "block4a_dwconv_pad\n",
            "block4a_dwconv\n",
            "block4a_bn\n",
            "block4a_activation\n",
            "block4a_se_squeeze\n",
            "block4a_se_reshape\n",
            "block4a_se_reduce\n",
            "block4a_se_expand\n",
            "block4a_se_excite\n",
            "block4a_project_conv\n",
            "block4a_project_bn\n",
            "block4b_expand_conv\n",
            "block4b_expand_bn\n",
            "block4b_expand_activation\n",
            "block4b_dwconv\n",
            "block4b_bn\n",
            "block4b_activation\n",
            "block4b_se_squeeze\n",
            "block4b_se_reshape\n",
            "block4b_se_reduce\n",
            "block4b_se_expand\n",
            "block4b_se_excite\n",
            "block4b_project_conv\n",
            "block4b_project_bn\n",
            "block4b_drop\n",
            "block4b_add\n",
            "block4c_expand_conv\n",
            "block4c_expand_bn\n",
            "block4c_expand_activation\n",
            "block4c_dwconv\n",
            "block4c_bn\n",
            "block4c_activation\n",
            "block4c_se_squeeze\n",
            "block4c_se_reshape\n",
            "block4c_se_reduce\n",
            "block4c_se_expand\n",
            "block4c_se_excite\n",
            "block4c_project_conv\n",
            "block4c_project_bn\n",
            "block4c_drop\n",
            "block4c_add\n",
            "block5a_expand_conv\n",
            "block5a_expand_bn\n",
            "block5a_expand_activation\n",
            "block5a_dwconv\n",
            "block5a_bn\n",
            "block5a_activation\n",
            "block5a_se_squeeze\n",
            "block5a_se_reshape\n",
            "block5a_se_reduce\n",
            "block5a_se_expand\n",
            "block5a_se_excite\n",
            "block5a_project_conv\n",
            "block5a_project_bn\n",
            "block5b_expand_conv\n",
            "block5b_expand_bn\n",
            "block5b_expand_activation\n",
            "block5b_dwconv\n",
            "block5b_bn\n",
            "block5b_activation\n",
            "block5b_se_squeeze\n",
            "block5b_se_reshape\n",
            "block5b_se_reduce\n",
            "block5b_se_expand\n",
            "block5b_se_excite\n",
            "block5b_project_conv\n",
            "block5b_project_bn\n",
            "block5b_drop\n",
            "block5b_add\n",
            "block5c_expand_conv\n",
            "block5c_expand_bn\n",
            "block5c_expand_activation\n",
            "block5c_dwconv\n",
            "block5c_bn\n",
            "block5c_activation\n",
            "block5c_se_squeeze\n",
            "block5c_se_reshape\n",
            "block5c_se_reduce\n",
            "block5c_se_expand\n",
            "block5c_se_excite\n",
            "block5c_project_conv\n",
            "block5c_project_bn\n",
            "block5c_drop\n",
            "block5c_add\n",
            "block6a_expand_conv\n",
            "block6a_expand_bn\n",
            "block6a_expand_activation\n",
            "block6a_dwconv_pad\n",
            "block6a_dwconv\n",
            "block6a_bn\n",
            "block6a_activation\n",
            "block6a_se_squeeze\n",
            "block6a_se_reshape\n",
            "block6a_se_reduce\n",
            "block6a_se_expand\n",
            "block6a_se_excite\n",
            "block6a_project_conv\n",
            "block6a_project_bn\n",
            "block6b_expand_conv\n",
            "block6b_expand_bn\n",
            "block6b_expand_activation\n",
            "block6b_dwconv\n",
            "block6b_bn\n",
            "block6b_activation\n",
            "block6b_se_squeeze\n",
            "block6b_se_reshape\n",
            "block6b_se_reduce\n",
            "block6b_se_expand\n",
            "block6b_se_excite\n",
            "block6b_project_conv\n",
            "block6b_project_bn\n",
            "block6b_drop\n",
            "block6b_add\n",
            "block6c_expand_conv\n",
            "block6c_expand_bn\n",
            "block6c_expand_activation\n",
            "block6c_dwconv\n",
            "block6c_bn\n",
            "block6c_activation\n",
            "block6c_se_squeeze\n",
            "block6c_se_reshape\n",
            "block6c_se_reduce\n",
            "block6c_se_expand\n",
            "block6c_se_excite\n",
            "block6c_project_conv\n",
            "block6c_project_bn\n",
            "block6c_drop\n",
            "block6c_add\n",
            "block6d_expand_conv\n",
            "block6d_expand_bn\n",
            "block6d_expand_activation\n",
            "block6d_dwconv\n",
            "block6d_bn\n",
            "block6d_activation\n",
            "block6d_se_squeeze\n",
            "block6d_se_reshape\n",
            "block6d_se_reduce\n",
            "block6d_se_expand\n",
            "block6d_se_excite\n",
            "block6d_project_conv\n",
            "block6d_project_bn\n",
            "block6d_drop\n",
            "block6d_add\n",
            "block7a_expand_conv\n",
            "block7a_expand_bn\n",
            "block7a_expand_activation\n",
            "block7a_dwconv\n",
            "block7a_bn\n",
            "block7a_activation\n",
            "block7a_se_squeeze\n",
            "block7a_se_reshape\n",
            "block7a_se_reduce\n",
            "block7a_se_expand\n",
            "block7a_se_excite\n",
            "block7a_project_conv\n",
            "block7a_project_bn\n",
            "top_conv\n",
            "top_bn\n",
            "top_activation\n",
            "global_average_pooling2d_6\n",
            "dense_12\n",
            "dropout_6\n",
            "dense_13\n",
            "No duplicate layer names in EfficientNet.\n",
            "\n",
            "Layer names in ResNet:\n",
            "input_layer\n",
            "conv1_pad\n",
            "conv1_conv\n",
            "conv1_bn\n",
            "conv1_relu\n",
            "pool1_pad\n",
            "pool1_pool\n",
            "conv2_block1_1_conv\n",
            "conv2_block1_1_bn\n",
            "conv2_block1_1_relu\n",
            "conv2_block1_2_conv\n",
            "conv2_block1_2_bn\n",
            "conv2_block1_2_relu\n",
            "conv2_block1_0_conv\n",
            "conv2_block1_3_conv\n",
            "conv2_block1_0_bn\n",
            "conv2_block1_3_bn\n",
            "conv2_block1_add\n",
            "conv2_block1_out\n",
            "conv2_block2_1_conv\n",
            "conv2_block2_1_bn\n",
            "conv2_block2_1_relu\n",
            "conv2_block2_2_conv\n",
            "conv2_block2_2_bn\n",
            "conv2_block2_2_relu\n",
            "conv2_block2_3_conv\n",
            "conv2_block2_3_bn\n",
            "conv2_block2_add\n",
            "conv2_block2_out\n",
            "conv2_block3_1_conv\n",
            "conv2_block3_1_bn\n",
            "conv2_block3_1_relu\n",
            "conv2_block3_2_conv\n",
            "conv2_block3_2_bn\n",
            "conv2_block3_2_relu\n",
            "conv2_block3_3_conv\n",
            "conv2_block3_3_bn\n",
            "conv2_block3_add\n",
            "conv2_block3_out\n",
            "conv3_block1_1_conv\n",
            "conv3_block1_1_bn\n",
            "conv3_block1_1_relu\n",
            "conv3_block1_2_conv\n",
            "conv3_block1_2_bn\n",
            "conv3_block1_2_relu\n",
            "conv3_block1_0_conv\n",
            "conv3_block1_3_conv\n",
            "conv3_block1_0_bn\n",
            "conv3_block1_3_bn\n",
            "conv3_block1_add\n",
            "conv3_block1_out\n",
            "conv3_block2_1_conv\n",
            "conv3_block2_1_bn\n",
            "conv3_block2_1_relu\n",
            "conv3_block2_2_conv\n",
            "conv3_block2_2_bn\n",
            "conv3_block2_2_relu\n",
            "conv3_block2_3_conv\n",
            "conv3_block2_3_bn\n",
            "conv3_block2_add\n",
            "conv3_block2_out\n",
            "conv3_block3_1_conv\n",
            "conv3_block3_1_bn\n",
            "conv3_block3_1_relu\n",
            "conv3_block3_2_conv\n",
            "conv3_block3_2_bn\n",
            "conv3_block3_2_relu\n",
            "conv3_block3_3_conv\n",
            "conv3_block3_3_bn\n",
            "conv3_block3_add\n",
            "conv3_block3_out\n",
            "conv3_block4_1_conv\n",
            "conv3_block4_1_bn\n",
            "conv3_block4_1_relu\n",
            "conv3_block4_2_conv\n",
            "conv3_block4_2_bn\n",
            "conv3_block4_2_relu\n",
            "conv3_block4_3_conv\n",
            "conv3_block4_3_bn\n",
            "conv3_block4_add\n",
            "conv3_block4_out\n",
            "conv4_block1_1_conv\n",
            "conv4_block1_1_bn\n",
            "conv4_block1_1_relu\n",
            "conv4_block1_2_conv\n",
            "conv4_block1_2_bn\n",
            "conv4_block1_2_relu\n",
            "conv4_block1_0_conv\n",
            "conv4_block1_3_conv\n",
            "conv4_block1_0_bn\n",
            "conv4_block1_3_bn\n",
            "conv4_block1_add\n",
            "conv4_block1_out\n",
            "conv4_block2_1_conv\n",
            "conv4_block2_1_bn\n",
            "conv4_block2_1_relu\n",
            "conv4_block2_2_conv\n",
            "conv4_block2_2_bn\n",
            "conv4_block2_2_relu\n",
            "conv4_block2_3_conv\n",
            "conv4_block2_3_bn\n",
            "conv4_block2_add\n",
            "conv4_block2_out\n",
            "conv4_block3_1_conv\n",
            "conv4_block3_1_bn\n",
            "conv4_block3_1_relu\n",
            "conv4_block3_2_conv\n",
            "conv4_block3_2_bn\n",
            "conv4_block3_2_relu\n",
            "conv4_block3_3_conv\n",
            "conv4_block3_3_bn\n",
            "conv4_block3_add\n",
            "conv4_block3_out\n",
            "conv4_block4_1_conv\n",
            "conv4_block4_1_bn\n",
            "conv4_block4_1_relu\n",
            "conv4_block4_2_conv\n",
            "conv4_block4_2_bn\n",
            "conv4_block4_2_relu\n",
            "conv4_block4_3_conv\n",
            "conv4_block4_3_bn\n",
            "conv4_block4_add\n",
            "conv4_block4_out\n",
            "conv4_block5_1_conv\n",
            "conv4_block5_1_bn\n",
            "conv4_block5_1_relu\n",
            "conv4_block5_2_conv\n",
            "conv4_block5_2_bn\n",
            "conv4_block5_2_relu\n",
            "conv4_block5_3_conv\n",
            "conv4_block5_3_bn\n",
            "conv4_block5_add\n",
            "conv4_block5_out\n",
            "conv4_block6_1_conv\n",
            "conv4_block6_1_bn\n",
            "conv4_block6_1_relu\n",
            "conv4_block6_2_conv\n",
            "conv4_block6_2_bn\n",
            "conv4_block6_2_relu\n",
            "conv4_block6_3_conv\n",
            "conv4_block6_3_bn\n",
            "conv4_block6_add\n",
            "conv4_block6_out\n",
            "conv5_block1_1_conv\n",
            "conv5_block1_1_bn\n",
            "conv5_block1_1_relu\n",
            "conv5_block1_2_conv\n",
            "conv5_block1_2_bn\n",
            "conv5_block1_2_relu\n",
            "conv5_block1_0_conv\n",
            "conv5_block1_3_conv\n",
            "conv5_block1_0_bn\n",
            "conv5_block1_3_bn\n",
            "conv5_block1_add\n",
            "conv5_block1_out\n",
            "conv5_block2_1_conv\n",
            "conv5_block2_1_bn\n",
            "conv5_block2_1_relu\n",
            "conv5_block2_2_conv\n",
            "conv5_block2_2_bn\n",
            "conv5_block2_2_relu\n",
            "conv5_block2_3_conv\n",
            "conv5_block2_3_bn\n",
            "conv5_block2_add\n",
            "conv5_block2_out\n",
            "conv5_block3_1_conv\n",
            "conv5_block3_1_bn\n",
            "conv5_block3_1_relu\n",
            "conv5_block3_2_conv\n",
            "conv5_block3_2_bn\n",
            "conv5_block3_2_relu\n",
            "conv5_block3_3_conv\n",
            "conv5_block3_3_bn\n",
            "conv5_block3_add\n",
            "conv5_block3_out\n",
            "global_average_pooling2d\n",
            "dense\n",
            "dropout\n",
            "dense_1\n",
            "No duplicate layer names in ResNet.\n",
            "\n",
            "Layer names in DenseNet:\n",
            "input_layer\n",
            "zero_padding2d\n",
            "conv1_conv\n",
            "conv1_bn\n",
            "conv1_relu\n",
            "zero_padding2d_1\n",
            "pool1\n",
            "conv2_block1_0_bn\n",
            "conv2_block1_0_relu\n",
            "conv2_block1_1_conv\n",
            "conv2_block1_1_bn\n",
            "conv2_block1_1_relu\n",
            "conv2_block1_2_conv\n",
            "conv2_block1_concat\n",
            "conv2_block2_0_bn\n",
            "conv2_block2_0_relu\n",
            "conv2_block2_1_conv\n",
            "conv2_block2_1_bn\n",
            "conv2_block2_1_relu\n",
            "conv2_block2_2_conv\n",
            "conv2_block2_concat\n",
            "conv2_block3_0_bn\n",
            "conv2_block3_0_relu\n",
            "conv2_block3_1_conv\n",
            "conv2_block3_1_bn\n",
            "conv2_block3_1_relu\n",
            "conv2_block3_2_conv\n",
            "conv2_block3_concat\n",
            "conv2_block4_0_bn\n",
            "conv2_block4_0_relu\n",
            "conv2_block4_1_conv\n",
            "conv2_block4_1_bn\n",
            "conv2_block4_1_relu\n",
            "conv2_block4_2_conv\n",
            "conv2_block4_concat\n",
            "conv2_block5_0_bn\n",
            "conv2_block5_0_relu\n",
            "conv2_block5_1_conv\n",
            "conv2_block5_1_bn\n",
            "conv2_block5_1_relu\n",
            "conv2_block5_2_conv\n",
            "conv2_block5_concat\n",
            "conv2_block6_0_bn\n",
            "conv2_block6_0_relu\n",
            "conv2_block6_1_conv\n",
            "conv2_block6_1_bn\n",
            "conv2_block6_1_relu\n",
            "conv2_block6_2_conv\n",
            "conv2_block6_concat\n",
            "pool2_bn\n",
            "pool2_relu\n",
            "pool2_conv\n",
            "pool2_pool\n",
            "conv3_block1_0_bn\n",
            "conv3_block1_0_relu\n",
            "conv3_block1_1_conv\n",
            "conv3_block1_1_bn\n",
            "conv3_block1_1_relu\n",
            "conv3_block1_2_conv\n",
            "conv3_block1_concat\n",
            "conv3_block2_0_bn\n",
            "conv3_block2_0_relu\n",
            "conv3_block2_1_conv\n",
            "conv3_block2_1_bn\n",
            "conv3_block2_1_relu\n",
            "conv3_block2_2_conv\n",
            "conv3_block2_concat\n",
            "conv3_block3_0_bn\n",
            "conv3_block3_0_relu\n",
            "conv3_block3_1_conv\n",
            "conv3_block3_1_bn\n",
            "conv3_block3_1_relu\n",
            "conv3_block3_2_conv\n",
            "conv3_block3_concat\n",
            "conv3_block4_0_bn\n",
            "conv3_block4_0_relu\n",
            "conv3_block4_1_conv\n",
            "conv3_block4_1_bn\n",
            "conv3_block4_1_relu\n",
            "conv3_block4_2_conv\n",
            "conv3_block4_concat\n",
            "conv3_block5_0_bn\n",
            "conv3_block5_0_relu\n",
            "conv3_block5_1_conv\n",
            "conv3_block5_1_bn\n",
            "conv3_block5_1_relu\n",
            "conv3_block5_2_conv\n",
            "conv3_block5_concat\n",
            "conv3_block6_0_bn\n",
            "conv3_block6_0_relu\n",
            "conv3_block6_1_conv\n",
            "conv3_block6_1_bn\n",
            "conv3_block6_1_relu\n",
            "conv3_block6_2_conv\n",
            "conv3_block6_concat\n",
            "conv3_block7_0_bn\n",
            "conv3_block7_0_relu\n",
            "conv3_block7_1_conv\n",
            "conv3_block7_1_bn\n",
            "conv3_block7_1_relu\n",
            "conv3_block7_2_conv\n",
            "conv3_block7_concat\n",
            "conv3_block8_0_bn\n",
            "conv3_block8_0_relu\n",
            "conv3_block8_1_conv\n",
            "conv3_block8_1_bn\n",
            "conv3_block8_1_relu\n",
            "conv3_block8_2_conv\n",
            "conv3_block8_concat\n",
            "conv3_block9_0_bn\n",
            "conv3_block9_0_relu\n",
            "conv3_block9_1_conv\n",
            "conv3_block9_1_bn\n",
            "conv3_block9_1_relu\n",
            "conv3_block9_2_conv\n",
            "conv3_block9_concat\n",
            "conv3_block10_0_bn\n",
            "conv3_block10_0_relu\n",
            "conv3_block10_1_conv\n",
            "conv3_block10_1_bn\n",
            "conv3_block10_1_relu\n",
            "conv3_block10_2_conv\n",
            "conv3_block10_concat\n",
            "conv3_block11_0_bn\n",
            "conv3_block11_0_relu\n",
            "conv3_block11_1_conv\n",
            "conv3_block11_1_bn\n",
            "conv3_block11_1_relu\n",
            "conv3_block11_2_conv\n",
            "conv3_block11_concat\n",
            "conv3_block12_0_bn\n",
            "conv3_block12_0_relu\n",
            "conv3_block12_1_conv\n",
            "conv3_block12_1_bn\n",
            "conv3_block12_1_relu\n",
            "conv3_block12_2_conv\n",
            "conv3_block12_concat\n",
            "pool3_bn\n",
            "pool3_relu\n",
            "pool3_conv\n",
            "pool3_pool\n",
            "conv4_block1_0_bn\n",
            "conv4_block1_0_relu\n",
            "conv4_block1_1_conv\n",
            "conv4_block1_1_bn\n",
            "conv4_block1_1_relu\n",
            "conv4_block1_2_conv\n",
            "conv4_block1_concat\n",
            "conv4_block2_0_bn\n",
            "conv4_block2_0_relu\n",
            "conv4_block2_1_conv\n",
            "conv4_block2_1_bn\n",
            "conv4_block2_1_relu\n",
            "conv4_block2_2_conv\n",
            "conv4_block2_concat\n",
            "conv4_block3_0_bn\n",
            "conv4_block3_0_relu\n",
            "conv4_block3_1_conv\n",
            "conv4_block3_1_bn\n",
            "conv4_block3_1_relu\n",
            "conv4_block3_2_conv\n",
            "conv4_block3_concat\n",
            "conv4_block4_0_bn\n",
            "conv4_block4_0_relu\n",
            "conv4_block4_1_conv\n",
            "conv4_block4_1_bn\n",
            "conv4_block4_1_relu\n",
            "conv4_block4_2_conv\n",
            "conv4_block4_concat\n",
            "conv4_block5_0_bn\n",
            "conv4_block5_0_relu\n",
            "conv4_block5_1_conv\n",
            "conv4_block5_1_bn\n",
            "conv4_block5_1_relu\n",
            "conv4_block5_2_conv\n",
            "conv4_block5_concat\n",
            "conv4_block6_0_bn\n",
            "conv4_block6_0_relu\n",
            "conv4_block6_1_conv\n",
            "conv4_block6_1_bn\n",
            "conv4_block6_1_relu\n",
            "conv4_block6_2_conv\n",
            "conv4_block6_concat\n",
            "conv4_block7_0_bn\n",
            "conv4_block7_0_relu\n",
            "conv4_block7_1_conv\n",
            "conv4_block7_1_bn\n",
            "conv4_block7_1_relu\n",
            "conv4_block7_2_conv\n",
            "conv4_block7_concat\n",
            "conv4_block8_0_bn\n",
            "conv4_block8_0_relu\n",
            "conv4_block8_1_conv\n",
            "conv4_block8_1_bn\n",
            "conv4_block8_1_relu\n",
            "conv4_block8_2_conv\n",
            "conv4_block8_concat\n",
            "conv4_block9_0_bn\n",
            "conv4_block9_0_relu\n",
            "conv4_block9_1_conv\n",
            "conv4_block9_1_bn\n",
            "conv4_block9_1_relu\n",
            "conv4_block9_2_conv\n",
            "conv4_block9_concat\n",
            "conv4_block10_0_bn\n",
            "conv4_block10_0_relu\n",
            "conv4_block10_1_conv\n",
            "conv4_block10_1_bn\n",
            "conv4_block10_1_relu\n",
            "conv4_block10_2_conv\n",
            "conv4_block10_concat\n",
            "conv4_block11_0_bn\n",
            "conv4_block11_0_relu\n",
            "conv4_block11_1_conv\n",
            "conv4_block11_1_bn\n",
            "conv4_block11_1_relu\n",
            "conv4_block11_2_conv\n",
            "conv4_block11_concat\n",
            "conv4_block12_0_bn\n",
            "conv4_block12_0_relu\n",
            "conv4_block12_1_conv\n",
            "conv4_block12_1_bn\n",
            "conv4_block12_1_relu\n",
            "conv4_block12_2_conv\n",
            "conv4_block12_concat\n",
            "conv4_block13_0_bn\n",
            "conv4_block13_0_relu\n",
            "conv4_block13_1_conv\n",
            "conv4_block13_1_bn\n",
            "conv4_block13_1_relu\n",
            "conv4_block13_2_conv\n",
            "conv4_block13_concat\n",
            "conv4_block14_0_bn\n",
            "conv4_block14_0_relu\n",
            "conv4_block14_1_conv\n",
            "conv4_block14_1_bn\n",
            "conv4_block14_1_relu\n",
            "conv4_block14_2_conv\n",
            "conv4_block14_concat\n",
            "conv4_block15_0_bn\n",
            "conv4_block15_0_relu\n",
            "conv4_block15_1_conv\n",
            "conv4_block15_1_bn\n",
            "conv4_block15_1_relu\n",
            "conv4_block15_2_conv\n",
            "conv4_block15_concat\n",
            "conv4_block16_0_bn\n",
            "conv4_block16_0_relu\n",
            "conv4_block16_1_conv\n",
            "conv4_block16_1_bn\n",
            "conv4_block16_1_relu\n",
            "conv4_block16_2_conv\n",
            "conv4_block16_concat\n",
            "conv4_block17_0_bn\n",
            "conv4_block17_0_relu\n",
            "conv4_block17_1_conv\n",
            "conv4_block17_1_bn\n",
            "conv4_block17_1_relu\n",
            "conv4_block17_2_conv\n",
            "conv4_block17_concat\n",
            "conv4_block18_0_bn\n",
            "conv4_block18_0_relu\n",
            "conv4_block18_1_conv\n",
            "conv4_block18_1_bn\n",
            "conv4_block18_1_relu\n",
            "conv4_block18_2_conv\n",
            "conv4_block18_concat\n",
            "conv4_block19_0_bn\n",
            "conv4_block19_0_relu\n",
            "conv4_block19_1_conv\n",
            "conv4_block19_1_bn\n",
            "conv4_block19_1_relu\n",
            "conv4_block19_2_conv\n",
            "conv4_block19_concat\n",
            "conv4_block20_0_bn\n",
            "conv4_block20_0_relu\n",
            "conv4_block20_1_conv\n",
            "conv4_block20_1_bn\n",
            "conv4_block20_1_relu\n",
            "conv4_block20_2_conv\n",
            "conv4_block20_concat\n",
            "conv4_block21_0_bn\n",
            "conv4_block21_0_relu\n",
            "conv4_block21_1_conv\n",
            "conv4_block21_1_bn\n",
            "conv4_block21_1_relu\n",
            "conv4_block21_2_conv\n",
            "conv4_block21_concat\n",
            "conv4_block22_0_bn\n",
            "conv4_block22_0_relu\n",
            "conv4_block22_1_conv\n",
            "conv4_block22_1_bn\n",
            "conv4_block22_1_relu\n",
            "conv4_block22_2_conv\n",
            "conv4_block22_concat\n",
            "conv4_block23_0_bn\n",
            "conv4_block23_0_relu\n",
            "conv4_block23_1_conv\n",
            "conv4_block23_1_bn\n",
            "conv4_block23_1_relu\n",
            "conv4_block23_2_conv\n",
            "conv4_block23_concat\n",
            "conv4_block24_0_bn\n",
            "conv4_block24_0_relu\n",
            "conv4_block24_1_conv\n",
            "conv4_block24_1_bn\n",
            "conv4_block24_1_relu\n",
            "conv4_block24_2_conv\n",
            "conv4_block24_concat\n",
            "pool4_bn\n",
            "pool4_relu\n",
            "pool4_conv\n",
            "pool4_pool\n",
            "conv5_block1_0_bn\n",
            "conv5_block1_0_relu\n",
            "conv5_block1_1_conv\n",
            "conv5_block1_1_bn\n",
            "conv5_block1_1_relu\n",
            "conv5_block1_2_conv\n",
            "conv5_block1_concat\n",
            "conv5_block2_0_bn\n",
            "conv5_block2_0_relu\n",
            "conv5_block2_1_conv\n",
            "conv5_block2_1_bn\n",
            "conv5_block2_1_relu\n",
            "conv5_block2_2_conv\n",
            "conv5_block2_concat\n",
            "conv5_block3_0_bn\n",
            "conv5_block3_0_relu\n",
            "conv5_block3_1_conv\n",
            "conv5_block3_1_bn\n",
            "conv5_block3_1_relu\n",
            "conv5_block3_2_conv\n",
            "conv5_block3_concat\n",
            "conv5_block4_0_bn\n",
            "conv5_block4_0_relu\n",
            "conv5_block4_1_conv\n",
            "conv5_block4_1_bn\n",
            "conv5_block4_1_relu\n",
            "conv5_block4_2_conv\n",
            "conv5_block4_concat\n",
            "conv5_block5_0_bn\n",
            "conv5_block5_0_relu\n",
            "conv5_block5_1_conv\n",
            "conv5_block5_1_bn\n",
            "conv5_block5_1_relu\n",
            "conv5_block5_2_conv\n",
            "conv5_block5_concat\n",
            "conv5_block6_0_bn\n",
            "conv5_block6_0_relu\n",
            "conv5_block6_1_conv\n",
            "conv5_block6_1_bn\n",
            "conv5_block6_1_relu\n",
            "conv5_block6_2_conv\n",
            "conv5_block6_concat\n",
            "conv5_block7_0_bn\n",
            "conv5_block7_0_relu\n",
            "conv5_block7_1_conv\n",
            "conv5_block7_1_bn\n",
            "conv5_block7_1_relu\n",
            "conv5_block7_2_conv\n",
            "conv5_block7_concat\n",
            "conv5_block8_0_bn\n",
            "conv5_block8_0_relu\n",
            "conv5_block8_1_conv\n",
            "conv5_block8_1_bn\n",
            "conv5_block8_1_relu\n",
            "conv5_block8_2_conv\n",
            "conv5_block8_concat\n",
            "conv5_block9_0_bn\n",
            "conv5_block9_0_relu\n",
            "conv5_block9_1_conv\n",
            "conv5_block9_1_bn\n",
            "conv5_block9_1_relu\n",
            "conv5_block9_2_conv\n",
            "conv5_block9_concat\n",
            "conv5_block10_0_bn\n",
            "conv5_block10_0_relu\n",
            "conv5_block10_1_conv\n",
            "conv5_block10_1_bn\n",
            "conv5_block10_1_relu\n",
            "conv5_block10_2_conv\n",
            "conv5_block10_concat\n",
            "conv5_block11_0_bn\n",
            "conv5_block11_0_relu\n",
            "conv5_block11_1_conv\n",
            "conv5_block11_1_bn\n",
            "conv5_block11_1_relu\n",
            "conv5_block11_2_conv\n",
            "conv5_block11_concat\n",
            "conv5_block12_0_bn\n",
            "conv5_block12_0_relu\n",
            "conv5_block12_1_conv\n",
            "conv5_block12_1_bn\n",
            "conv5_block12_1_relu\n",
            "conv5_block12_2_conv\n",
            "conv5_block12_concat\n",
            "conv5_block13_0_bn\n",
            "conv5_block13_0_relu\n",
            "conv5_block13_1_conv\n",
            "conv5_block13_1_bn\n",
            "conv5_block13_1_relu\n",
            "conv5_block13_2_conv\n",
            "conv5_block13_concat\n",
            "conv5_block14_0_bn\n",
            "conv5_block14_0_relu\n",
            "conv5_block14_1_conv\n",
            "conv5_block14_1_bn\n",
            "conv5_block14_1_relu\n",
            "conv5_block14_2_conv\n",
            "conv5_block14_concat\n",
            "conv5_block15_0_bn\n",
            "conv5_block15_0_relu\n",
            "conv5_block15_1_conv\n",
            "conv5_block15_1_bn\n",
            "conv5_block15_1_relu\n",
            "conv5_block15_2_conv\n",
            "conv5_block15_concat\n",
            "conv5_block16_0_bn\n",
            "conv5_block16_0_relu\n",
            "conv5_block16_1_conv\n",
            "conv5_block16_1_bn\n",
            "conv5_block16_1_relu\n",
            "conv5_block16_2_conv\n",
            "conv5_block16_concat\n",
            "bn\n",
            "relu\n",
            "global_average_pooling2d\n",
            "dropout\n",
            "dense\n",
            "dropout_1\n",
            "dense_1\n",
            "No duplicate layer names in DenseNet.\n"
          ]
        }
      ],
      "source": [
        "def print_layer_names(model, model_name):\n",
        "    \"\"\"\n",
        "    Prints the layer names in the given model to identify duplicates.\n",
        "    \"\"\"\n",
        "    print(f\"\\nLayer names in {model_name}:\")\n",
        "    layer_names = [layer.name for layer in model.layers]\n",
        "    for name in layer_names:\n",
        "        print(name)\n",
        "    # Check for duplicates\n",
        "    duplicates = [name for name in set(layer_names) if layer_names.count(name) > 1]\n",
        "    if duplicates:\n",
        "        print(f\"\\nWARNING: Duplicate layer names found in {model_name}: {duplicates}\")\n",
        "    else:\n",
        "        print(f\"No duplicate layer names in {model_name}.\")\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load and check each model\n",
        "efficientnet_model = load_model('efficientnet.h5', compile=False)\n",
        "resnet_model = load_model('/content/drive/MyDrive/DSGP Files/Models/resnet.h5', compile=False)\n",
        "densenet_model = load_model('densenet_model.h5', compile=False)\n",
        "\n",
        "print_layer_names(efficientnet_model, \"EfficientNet\")\n",
        "print_layer_names(resnet_model, \"ResNet\")\n",
        "print_layer_names(densenet_model, \"DenseNet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BfdH2jiGZuU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHvFFJ-1hiNC"
      },
      "source": [
        "## Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaJiQxX6bGmL"
      },
      "source": [
        "### Averaging approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UmXUKAWj_u6z",
        "outputId": "86028d27-eb47-4955-ee10-de212531a672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading models...\n",
            "Debugging EfficientNet model...\n",
            "Model name: functional_6\n",
            "Layer name: input_layer_6\n",
            "Layer name: rescaling_12\n",
            "Layer name: normalization_6\n",
            "Layer name: rescaling_13\n",
            "Layer name: stem_conv_pad\n",
            "Layer name: stem_conv\n",
            "Layer name: stem_bn\n",
            "Layer name: stem_activation\n",
            "Layer name: block1a_dwconv\n",
            "Layer name: block1a_bn\n",
            "Layer name: block1a_activation\n",
            "Layer name: block1a_se_squeeze\n",
            "Layer name: block1a_se_reshape\n",
            "Layer name: block1a_se_reduce\n",
            "Layer name: block1a_se_expand\n",
            "Layer name: block1a_se_excite\n",
            "Layer name: block1a_project_conv\n",
            "Layer name: block1a_project_bn\n",
            "Layer name: block2a_expand_conv\n",
            "Layer name: block2a_expand_bn\n",
            "Layer name: block2a_expand_activation\n",
            "Layer name: block2a_dwconv_pad\n",
            "Layer name: block2a_dwconv\n",
            "Layer name: block2a_bn\n",
            "Layer name: block2a_activation\n",
            "Layer name: block2a_se_squeeze\n",
            "Layer name: block2a_se_reshape\n",
            "Layer name: block2a_se_reduce\n",
            "Layer name: block2a_se_expand\n",
            "Layer name: block2a_se_excite\n",
            "Layer name: block2a_project_conv\n",
            "Layer name: block2a_project_bn\n",
            "Layer name: block2b_expand_conv\n",
            "Layer name: block2b_expand_bn\n",
            "Layer name: block2b_expand_activation\n",
            "Layer name: block2b_dwconv\n",
            "Layer name: block2b_bn\n",
            "Layer name: block2b_activation\n",
            "Layer name: block2b_se_squeeze\n",
            "Layer name: block2b_se_reshape\n",
            "Layer name: block2b_se_reduce\n",
            "Layer name: block2b_se_expand\n",
            "Layer name: block2b_se_excite\n",
            "Layer name: block2b_project_conv\n",
            "Layer name: block2b_project_bn\n",
            "Layer name: block2b_drop\n",
            "Layer name: block2b_add\n",
            "Layer name: block3a_expand_conv\n",
            "Layer name: block3a_expand_bn\n",
            "Layer name: block3a_expand_activation\n",
            "Layer name: block3a_dwconv_pad\n",
            "Layer name: block3a_dwconv\n",
            "Layer name: block3a_bn\n",
            "Layer name: block3a_activation\n",
            "Layer name: block3a_se_squeeze\n",
            "Layer name: block3a_se_reshape\n",
            "Layer name: block3a_se_reduce\n",
            "Layer name: block3a_se_expand\n",
            "Layer name: block3a_se_excite\n",
            "Layer name: block3a_project_conv\n",
            "Layer name: block3a_project_bn\n",
            "Layer name: block3b_expand_conv\n",
            "Layer name: block3b_expand_bn\n",
            "Layer name: block3b_expand_activation\n",
            "Layer name: block3b_dwconv\n",
            "Layer name: block3b_bn\n",
            "Layer name: block3b_activation\n",
            "Layer name: block3b_se_squeeze\n",
            "Layer name: block3b_se_reshape\n",
            "Layer name: block3b_se_reduce\n",
            "Layer name: block3b_se_expand\n",
            "Layer name: block3b_se_excite\n",
            "Layer name: block3b_project_conv\n",
            "Layer name: block3b_project_bn\n",
            "Layer name: block3b_drop\n",
            "Layer name: block3b_add\n",
            "Layer name: block4a_expand_conv\n",
            "Layer name: block4a_expand_bn\n",
            "Layer name: block4a_expand_activation\n",
            "Layer name: block4a_dwconv_pad\n",
            "Layer name: block4a_dwconv\n",
            "Layer name: block4a_bn\n",
            "Layer name: block4a_activation\n",
            "Layer name: block4a_se_squeeze\n",
            "Layer name: block4a_se_reshape\n",
            "Layer name: block4a_se_reduce\n",
            "Layer name: block4a_se_expand\n",
            "Layer name: block4a_se_excite\n",
            "Layer name: block4a_project_conv\n",
            "Layer name: block4a_project_bn\n",
            "Layer name: block4b_expand_conv\n",
            "Layer name: block4b_expand_bn\n",
            "Layer name: block4b_expand_activation\n",
            "Layer name: block4b_dwconv\n",
            "Layer name: block4b_bn\n",
            "Layer name: block4b_activation\n",
            "Layer name: block4b_se_squeeze\n",
            "Layer name: block4b_se_reshape\n",
            "Layer name: block4b_se_reduce\n",
            "Layer name: block4b_se_expand\n",
            "Layer name: block4b_se_excite\n",
            "Layer name: block4b_project_conv\n",
            "Layer name: block4b_project_bn\n",
            "Layer name: block4b_drop\n",
            "Layer name: block4b_add\n",
            "Layer name: block4c_expand_conv\n",
            "Layer name: block4c_expand_bn\n",
            "Layer name: block4c_expand_activation\n",
            "Layer name: block4c_dwconv\n",
            "Layer name: block4c_bn\n",
            "Layer name: block4c_activation\n",
            "Layer name: block4c_se_squeeze\n",
            "Layer name: block4c_se_reshape\n",
            "Layer name: block4c_se_reduce\n",
            "Layer name: block4c_se_expand\n",
            "Layer name: block4c_se_excite\n",
            "Layer name: block4c_project_conv\n",
            "Layer name: block4c_project_bn\n",
            "Layer name: block4c_drop\n",
            "Layer name: block4c_add\n",
            "Layer name: block5a_expand_conv\n",
            "Layer name: block5a_expand_bn\n",
            "Layer name: block5a_expand_activation\n",
            "Layer name: block5a_dwconv\n",
            "Layer name: block5a_bn\n",
            "Layer name: block5a_activation\n",
            "Layer name: block5a_se_squeeze\n",
            "Layer name: block5a_se_reshape\n",
            "Layer name: block5a_se_reduce\n",
            "Layer name: block5a_se_expand\n",
            "Layer name: block5a_se_excite\n",
            "Layer name: block5a_project_conv\n",
            "Layer name: block5a_project_bn\n",
            "Layer name: block5b_expand_conv\n",
            "Layer name: block5b_expand_bn\n",
            "Layer name: block5b_expand_activation\n",
            "Layer name: block5b_dwconv\n",
            "Layer name: block5b_bn\n",
            "Layer name: block5b_activation\n",
            "Layer name: block5b_se_squeeze\n",
            "Layer name: block5b_se_reshape\n",
            "Layer name: block5b_se_reduce\n",
            "Layer name: block5b_se_expand\n",
            "Layer name: block5b_se_excite\n",
            "Layer name: block5b_project_conv\n",
            "Layer name: block5b_project_bn\n",
            "Layer name: block5b_drop\n",
            "Layer name: block5b_add\n",
            "Layer name: block5c_expand_conv\n",
            "Layer name: block5c_expand_bn\n",
            "Layer name: block5c_expand_activation\n",
            "Layer name: block5c_dwconv\n",
            "Layer name: block5c_bn\n",
            "Layer name: block5c_activation\n",
            "Layer name: block5c_se_squeeze\n",
            "Layer name: block5c_se_reshape\n",
            "Layer name: block5c_se_reduce\n",
            "Layer name: block5c_se_expand\n",
            "Layer name: block5c_se_excite\n",
            "Layer name: block5c_project_conv\n",
            "Layer name: block5c_project_bn\n",
            "Layer name: block5c_drop\n",
            "Layer name: block5c_add\n",
            "Layer name: block6a_expand_conv\n",
            "Layer name: block6a_expand_bn\n",
            "Layer name: block6a_expand_activation\n",
            "Layer name: block6a_dwconv_pad\n",
            "Layer name: block6a_dwconv\n",
            "Layer name: block6a_bn\n",
            "Layer name: block6a_activation\n",
            "Layer name: block6a_se_squeeze\n",
            "Layer name: block6a_se_reshape\n",
            "Layer name: block6a_se_reduce\n",
            "Layer name: block6a_se_expand\n",
            "Layer name: block6a_se_excite\n",
            "Layer name: block6a_project_conv\n",
            "Layer name: block6a_project_bn\n",
            "Layer name: block6b_expand_conv\n",
            "Layer name: block6b_expand_bn\n",
            "Layer name: block6b_expand_activation\n",
            "Layer name: block6b_dwconv\n",
            "Layer name: block6b_bn\n",
            "Layer name: block6b_activation\n",
            "Layer name: block6b_se_squeeze\n",
            "Layer name: block6b_se_reshape\n",
            "Layer name: block6b_se_reduce\n",
            "Layer name: block6b_se_expand\n",
            "Layer name: block6b_se_excite\n",
            "Layer name: block6b_project_conv\n",
            "Layer name: block6b_project_bn\n",
            "Layer name: block6b_drop\n",
            "Layer name: block6b_add\n",
            "Layer name: block6c_expand_conv\n",
            "Layer name: block6c_expand_bn\n",
            "Layer name: block6c_expand_activation\n",
            "Layer name: block6c_dwconv\n",
            "Layer name: block6c_bn\n",
            "Layer name: block6c_activation\n",
            "Layer name: block6c_se_squeeze\n",
            "Layer name: block6c_se_reshape\n",
            "Layer name: block6c_se_reduce\n",
            "Layer name: block6c_se_expand\n",
            "Layer name: block6c_se_excite\n",
            "Layer name: block6c_project_conv\n",
            "Layer name: block6c_project_bn\n",
            "Layer name: block6c_drop\n",
            "Layer name: block6c_add\n",
            "Layer name: block6d_expand_conv\n",
            "Layer name: block6d_expand_bn\n",
            "Layer name: block6d_expand_activation\n",
            "Layer name: block6d_dwconv\n",
            "Layer name: block6d_bn\n",
            "Layer name: block6d_activation\n",
            "Layer name: block6d_se_squeeze\n",
            "Layer name: block6d_se_reshape\n",
            "Layer name: block6d_se_reduce\n",
            "Layer name: block6d_se_expand\n",
            "Layer name: block6d_se_excite\n",
            "Layer name: block6d_project_conv\n",
            "Layer name: block6d_project_bn\n",
            "Layer name: block6d_drop\n",
            "Layer name: block6d_add\n",
            "Layer name: block7a_expand_conv\n",
            "Layer name: block7a_expand_bn\n",
            "Layer name: block7a_expand_activation\n",
            "Layer name: block7a_dwconv\n",
            "Layer name: block7a_bn\n",
            "Layer name: block7a_activation\n",
            "Layer name: block7a_se_squeeze\n",
            "Layer name: block7a_se_reshape\n",
            "Layer name: block7a_se_reduce\n",
            "Layer name: block7a_se_expand\n",
            "Layer name: block7a_se_excite\n",
            "Layer name: block7a_project_conv\n",
            "Layer name: block7a_project_bn\n",
            "Layer name: top_conv\n",
            "Layer name: top_bn\n",
            "Layer name: top_activation\n",
            "Layer name: global_average_pooling2d_6\n",
            "Layer name: dense_12\n",
            "Layer name: dropout_6\n",
            "Layer name: dense_13\n",
            "Total layers in EfficientNet model: 242\n",
            "\n",
            "Debugging ResNet model...\n",
            "Model name: functional\n",
            "Layer name: input_layer\n",
            "Layer name: conv1_pad\n",
            "Layer name: conv1_conv\n",
            "Layer name: conv1_bn\n",
            "Layer name: conv1_relu\n",
            "Layer name: pool1_pad\n",
            "Layer name: pool1_pool\n",
            "Layer name: conv2_block1_1_conv\n",
            "Layer name: conv2_block1_1_bn\n",
            "Layer name: conv2_block1_1_relu\n",
            "Layer name: conv2_block1_2_conv\n",
            "Layer name: conv2_block1_2_bn\n",
            "Layer name: conv2_block1_2_relu\n",
            "Layer name: conv2_block1_0_conv\n",
            "Layer name: conv2_block1_3_conv\n",
            "Layer name: conv2_block1_0_bn\n",
            "Layer name: conv2_block1_3_bn\n",
            "Layer name: conv2_block1_add\n",
            "Layer name: conv2_block1_out\n",
            "Layer name: conv2_block2_1_conv\n",
            "Layer name: conv2_block2_1_bn\n",
            "Layer name: conv2_block2_1_relu\n",
            "Layer name: conv2_block2_2_conv\n",
            "Layer name: conv2_block2_2_bn\n",
            "Layer name: conv2_block2_2_relu\n",
            "Layer name: conv2_block2_3_conv\n",
            "Layer name: conv2_block2_3_bn\n",
            "Layer name: conv2_block2_add\n",
            "Layer name: conv2_block2_out\n",
            "Layer name: conv2_block3_1_conv\n",
            "Layer name: conv2_block3_1_bn\n",
            "Layer name: conv2_block3_1_relu\n",
            "Layer name: conv2_block3_2_conv\n",
            "Layer name: conv2_block3_2_bn\n",
            "Layer name: conv2_block3_2_relu\n",
            "Layer name: conv2_block3_3_conv\n",
            "Layer name: conv2_block3_3_bn\n",
            "Layer name: conv2_block3_add\n",
            "Layer name: conv2_block3_out\n",
            "Layer name: conv3_block1_1_conv\n",
            "Layer name: conv3_block1_1_bn\n",
            "Layer name: conv3_block1_1_relu\n",
            "Layer name: conv3_block1_2_conv\n",
            "Layer name: conv3_block1_2_bn\n",
            "Layer name: conv3_block1_2_relu\n",
            "Layer name: conv3_block1_0_conv\n",
            "Layer name: conv3_block1_3_conv\n",
            "Layer name: conv3_block1_0_bn\n",
            "Layer name: conv3_block1_3_bn\n",
            "Layer name: conv3_block1_add\n",
            "Layer name: conv3_block1_out\n",
            "Layer name: conv3_block2_1_conv\n",
            "Layer name: conv3_block2_1_bn\n",
            "Layer name: conv3_block2_1_relu\n",
            "Layer name: conv3_block2_2_conv\n",
            "Layer name: conv3_block2_2_bn\n",
            "Layer name: conv3_block2_2_relu\n",
            "Layer name: conv3_block2_3_conv\n",
            "Layer name: conv3_block2_3_bn\n",
            "Layer name: conv3_block2_add\n",
            "Layer name: conv3_block2_out\n",
            "Layer name: conv3_block3_1_conv\n",
            "Layer name: conv3_block3_1_bn\n",
            "Layer name: conv3_block3_1_relu\n",
            "Layer name: conv3_block3_2_conv\n",
            "Layer name: conv3_block3_2_bn\n",
            "Layer name: conv3_block3_2_relu\n",
            "Layer name: conv3_block3_3_conv\n",
            "Layer name: conv3_block3_3_bn\n",
            "Layer name: conv3_block3_add\n",
            "Layer name: conv3_block3_out\n",
            "Layer name: conv3_block4_1_conv\n",
            "Layer name: conv3_block4_1_bn\n",
            "Layer name: conv3_block4_1_relu\n",
            "Layer name: conv3_block4_2_conv\n",
            "Layer name: conv3_block4_2_bn\n",
            "Layer name: conv3_block4_2_relu\n",
            "Layer name: conv3_block4_3_conv\n",
            "Layer name: conv3_block4_3_bn\n",
            "Layer name: conv3_block4_add\n",
            "Layer name: conv3_block4_out\n",
            "Layer name: conv4_block1_1_conv\n",
            "Layer name: conv4_block1_1_bn\n",
            "Layer name: conv4_block1_1_relu\n",
            "Layer name: conv4_block1_2_conv\n",
            "Layer name: conv4_block1_2_bn\n",
            "Layer name: conv4_block1_2_relu\n",
            "Layer name: conv4_block1_0_conv\n",
            "Layer name: conv4_block1_3_conv\n",
            "Layer name: conv4_block1_0_bn\n",
            "Layer name: conv4_block1_3_bn\n",
            "Layer name: conv4_block1_add\n",
            "Layer name: conv4_block1_out\n",
            "Layer name: conv4_block2_1_conv\n",
            "Layer name: conv4_block2_1_bn\n",
            "Layer name: conv4_block2_1_relu\n",
            "Layer name: conv4_block2_2_conv\n",
            "Layer name: conv4_block2_2_bn\n",
            "Layer name: conv4_block2_2_relu\n",
            "Layer name: conv4_block2_3_conv\n",
            "Layer name: conv4_block2_3_bn\n",
            "Layer name: conv4_block2_add\n",
            "Layer name: conv4_block2_out\n",
            "Layer name: conv4_block3_1_conv\n",
            "Layer name: conv4_block3_1_bn\n",
            "Layer name: conv4_block3_1_relu\n",
            "Layer name: conv4_block3_2_conv\n",
            "Layer name: conv4_block3_2_bn\n",
            "Layer name: conv4_block3_2_relu\n",
            "Layer name: conv4_block3_3_conv\n",
            "Layer name: conv4_block3_3_bn\n",
            "Layer name: conv4_block3_add\n",
            "Layer name: conv4_block3_out\n",
            "Layer name: conv4_block4_1_conv\n",
            "Layer name: conv4_block4_1_bn\n",
            "Layer name: conv4_block4_1_relu\n",
            "Layer name: conv4_block4_2_conv\n",
            "Layer name: conv4_block4_2_bn\n",
            "Layer name: conv4_block4_2_relu\n",
            "Layer name: conv4_block4_3_conv\n",
            "Layer name: conv4_block4_3_bn\n",
            "Layer name: conv4_block4_add\n",
            "Layer name: conv4_block4_out\n",
            "Layer name: conv4_block5_1_conv\n",
            "Layer name: conv4_block5_1_bn\n",
            "Layer name: conv4_block5_1_relu\n",
            "Layer name: conv4_block5_2_conv\n",
            "Layer name: conv4_block5_2_bn\n",
            "Layer name: conv4_block5_2_relu\n",
            "Layer name: conv4_block5_3_conv\n",
            "Layer name: conv4_block5_3_bn\n",
            "Layer name: conv4_block5_add\n",
            "Layer name: conv4_block5_out\n",
            "Layer name: conv4_block6_1_conv\n",
            "Layer name: conv4_block6_1_bn\n",
            "Layer name: conv4_block6_1_relu\n",
            "Layer name: conv4_block6_2_conv\n",
            "Layer name: conv4_block6_2_bn\n",
            "Layer name: conv4_block6_2_relu\n",
            "Layer name: conv4_block6_3_conv\n",
            "Layer name: conv4_block6_3_bn\n",
            "Layer name: conv4_block6_add\n",
            "Layer name: conv4_block6_out\n",
            "Layer name: conv5_block1_1_conv\n",
            "Layer name: conv5_block1_1_bn\n",
            "Layer name: conv5_block1_1_relu\n",
            "Layer name: conv5_block1_2_conv\n",
            "Layer name: conv5_block1_2_bn\n",
            "Layer name: conv5_block1_2_relu\n",
            "Layer name: conv5_block1_0_conv\n",
            "Layer name: conv5_block1_3_conv\n",
            "Layer name: conv5_block1_0_bn\n",
            "Layer name: conv5_block1_3_bn\n",
            "Layer name: conv5_block1_add\n",
            "Layer name: conv5_block1_out\n",
            "Layer name: conv5_block2_1_conv\n",
            "Layer name: conv5_block2_1_bn\n",
            "Layer name: conv5_block2_1_relu\n",
            "Layer name: conv5_block2_2_conv\n",
            "Layer name: conv5_block2_2_bn\n",
            "Layer name: conv5_block2_2_relu\n",
            "Layer name: conv5_block2_3_conv\n",
            "Layer name: conv5_block2_3_bn\n",
            "Layer name: conv5_block2_add\n",
            "Layer name: conv5_block2_out\n",
            "Layer name: conv5_block3_1_conv\n",
            "Layer name: conv5_block3_1_bn\n",
            "Layer name: conv5_block3_1_relu\n",
            "Layer name: conv5_block3_2_conv\n",
            "Layer name: conv5_block3_2_bn\n",
            "Layer name: conv5_block3_2_relu\n",
            "Layer name: conv5_block3_3_conv\n",
            "Layer name: conv5_block3_3_bn\n",
            "Layer name: conv5_block3_add\n",
            "Layer name: conv5_block3_out\n",
            "Layer name: global_average_pooling2d\n",
            "Layer name: dense\n",
            "Layer name: dropout\n",
            "Layer name: dense_1\n",
            "Total layers in ResNet model: 179\n",
            "\n",
            "Debugging DenseNet model...\n",
            "Model name: functional\n",
            "Layer name: input_layer\n",
            "Layer name: zero_padding2d\n",
            "Layer name: conv1_conv\n",
            "Layer name: conv1_bn\n",
            "Layer name: conv1_relu\n",
            "Layer name: zero_padding2d_1\n",
            "Layer name: pool1\n",
            "Layer name: conv2_block1_0_bn\n",
            "Layer name: conv2_block1_0_relu\n",
            "Layer name: conv2_block1_1_conv\n",
            "Layer name: conv2_block1_1_bn\n",
            "Layer name: conv2_block1_1_relu\n",
            "Layer name: conv2_block1_2_conv\n",
            "Layer name: conv2_block1_concat\n",
            "Layer name: conv2_block2_0_bn\n",
            "Layer name: conv2_block2_0_relu\n",
            "Layer name: conv2_block2_1_conv\n",
            "Layer name: conv2_block2_1_bn\n",
            "Layer name: conv2_block2_1_relu\n",
            "Layer name: conv2_block2_2_conv\n",
            "Layer name: conv2_block2_concat\n",
            "Layer name: conv2_block3_0_bn\n",
            "Layer name: conv2_block3_0_relu\n",
            "Layer name: conv2_block3_1_conv\n",
            "Layer name: conv2_block3_1_bn\n",
            "Layer name: conv2_block3_1_relu\n",
            "Layer name: conv2_block3_2_conv\n",
            "Layer name: conv2_block3_concat\n",
            "Layer name: conv2_block4_0_bn\n",
            "Layer name: conv2_block4_0_relu\n",
            "Layer name: conv2_block4_1_conv\n",
            "Layer name: conv2_block4_1_bn\n",
            "Layer name: conv2_block4_1_relu\n",
            "Layer name: conv2_block4_2_conv\n",
            "Layer name: conv2_block4_concat\n",
            "Layer name: conv2_block5_0_bn\n",
            "Layer name: conv2_block5_0_relu\n",
            "Layer name: conv2_block5_1_conv\n",
            "Layer name: conv2_block5_1_bn\n",
            "Layer name: conv2_block5_1_relu\n",
            "Layer name: conv2_block5_2_conv\n",
            "Layer name: conv2_block5_concat\n",
            "Layer name: conv2_block6_0_bn\n",
            "Layer name: conv2_block6_0_relu\n",
            "Layer name: conv2_block6_1_conv\n",
            "Layer name: conv2_block6_1_bn\n",
            "Layer name: conv2_block6_1_relu\n",
            "Layer name: conv2_block6_2_conv\n",
            "Layer name: conv2_block6_concat\n",
            "Layer name: pool2_bn\n",
            "Layer name: pool2_relu\n",
            "Layer name: pool2_conv\n",
            "Layer name: pool2_pool\n",
            "Layer name: conv3_block1_0_bn\n",
            "Layer name: conv3_block1_0_relu\n",
            "Layer name: conv3_block1_1_conv\n",
            "Layer name: conv3_block1_1_bn\n",
            "Layer name: conv3_block1_1_relu\n",
            "Layer name: conv3_block1_2_conv\n",
            "Layer name: conv3_block1_concat\n",
            "Layer name: conv3_block2_0_bn\n",
            "Layer name: conv3_block2_0_relu\n",
            "Layer name: conv3_block2_1_conv\n",
            "Layer name: conv3_block2_1_bn\n",
            "Layer name: conv3_block2_1_relu\n",
            "Layer name: conv3_block2_2_conv\n",
            "Layer name: conv3_block2_concat\n",
            "Layer name: conv3_block3_0_bn\n",
            "Layer name: conv3_block3_0_relu\n",
            "Layer name: conv3_block3_1_conv\n",
            "Layer name: conv3_block3_1_bn\n",
            "Layer name: conv3_block3_1_relu\n",
            "Layer name: conv3_block3_2_conv\n",
            "Layer name: conv3_block3_concat\n",
            "Layer name: conv3_block4_0_bn\n",
            "Layer name: conv3_block4_0_relu\n",
            "Layer name: conv3_block4_1_conv\n",
            "Layer name: conv3_block4_1_bn\n",
            "Layer name: conv3_block4_1_relu\n",
            "Layer name: conv3_block4_2_conv\n",
            "Layer name: conv3_block4_concat\n",
            "Layer name: conv3_block5_0_bn\n",
            "Layer name: conv3_block5_0_relu\n",
            "Layer name: conv3_block5_1_conv\n",
            "Layer name: conv3_block5_1_bn\n",
            "Layer name: conv3_block5_1_relu\n",
            "Layer name: conv3_block5_2_conv\n",
            "Layer name: conv3_block5_concat\n",
            "Layer name: conv3_block6_0_bn\n",
            "Layer name: conv3_block6_0_relu\n",
            "Layer name: conv3_block6_1_conv\n",
            "Layer name: conv3_block6_1_bn\n",
            "Layer name: conv3_block6_1_relu\n",
            "Layer name: conv3_block6_2_conv\n",
            "Layer name: conv3_block6_concat\n",
            "Layer name: conv3_block7_0_bn\n",
            "Layer name: conv3_block7_0_relu\n",
            "Layer name: conv3_block7_1_conv\n",
            "Layer name: conv3_block7_1_bn\n",
            "Layer name: conv3_block7_1_relu\n",
            "Layer name: conv3_block7_2_conv\n",
            "Layer name: conv3_block7_concat\n",
            "Layer name: conv3_block8_0_bn\n",
            "Layer name: conv3_block8_0_relu\n",
            "Layer name: conv3_block8_1_conv\n",
            "Layer name: conv3_block8_1_bn\n",
            "Layer name: conv3_block8_1_relu\n",
            "Layer name: conv3_block8_2_conv\n",
            "Layer name: conv3_block8_concat\n",
            "Layer name: conv3_block9_0_bn\n",
            "Layer name: conv3_block9_0_relu\n",
            "Layer name: conv3_block9_1_conv\n",
            "Layer name: conv3_block9_1_bn\n",
            "Layer name: conv3_block9_1_relu\n",
            "Layer name: conv3_block9_2_conv\n",
            "Layer name: conv3_block9_concat\n",
            "Layer name: conv3_block10_0_bn\n",
            "Layer name: conv3_block10_0_relu\n",
            "Layer name: conv3_block10_1_conv\n",
            "Layer name: conv3_block10_1_bn\n",
            "Layer name: conv3_block10_1_relu\n",
            "Layer name: conv3_block10_2_conv\n",
            "Layer name: conv3_block10_concat\n",
            "Layer name: conv3_block11_0_bn\n",
            "Layer name: conv3_block11_0_relu\n",
            "Layer name: conv3_block11_1_conv\n",
            "Layer name: conv3_block11_1_bn\n",
            "Layer name: conv3_block11_1_relu\n",
            "Layer name: conv3_block11_2_conv\n",
            "Layer name: conv3_block11_concat\n",
            "Layer name: conv3_block12_0_bn\n",
            "Layer name: conv3_block12_0_relu\n",
            "Layer name: conv3_block12_1_conv\n",
            "Layer name: conv3_block12_1_bn\n",
            "Layer name: conv3_block12_1_relu\n",
            "Layer name: conv3_block12_2_conv\n",
            "Layer name: conv3_block12_concat\n",
            "Layer name: pool3_bn\n",
            "Layer name: pool3_relu\n",
            "Layer name: pool3_conv\n",
            "Layer name: pool3_pool\n",
            "Layer name: conv4_block1_0_bn\n",
            "Layer name: conv4_block1_0_relu\n",
            "Layer name: conv4_block1_1_conv\n",
            "Layer name: conv4_block1_1_bn\n",
            "Layer name: conv4_block1_1_relu\n",
            "Layer name: conv4_block1_2_conv\n",
            "Layer name: conv4_block1_concat\n",
            "Layer name: conv4_block2_0_bn\n",
            "Layer name: conv4_block2_0_relu\n",
            "Layer name: conv4_block2_1_conv\n",
            "Layer name: conv4_block2_1_bn\n",
            "Layer name: conv4_block2_1_relu\n",
            "Layer name: conv4_block2_2_conv\n",
            "Layer name: conv4_block2_concat\n",
            "Layer name: conv4_block3_0_bn\n",
            "Layer name: conv4_block3_0_relu\n",
            "Layer name: conv4_block3_1_conv\n",
            "Layer name: conv4_block3_1_bn\n",
            "Layer name: conv4_block3_1_relu\n",
            "Layer name: conv4_block3_2_conv\n",
            "Layer name: conv4_block3_concat\n",
            "Layer name: conv4_block4_0_bn\n",
            "Layer name: conv4_block4_0_relu\n",
            "Layer name: conv4_block4_1_conv\n",
            "Layer name: conv4_block4_1_bn\n",
            "Layer name: conv4_block4_1_relu\n",
            "Layer name: conv4_block4_2_conv\n",
            "Layer name: conv4_block4_concat\n",
            "Layer name: conv4_block5_0_bn\n",
            "Layer name: conv4_block5_0_relu\n",
            "Layer name: conv4_block5_1_conv\n",
            "Layer name: conv4_block5_1_bn\n",
            "Layer name: conv4_block5_1_relu\n",
            "Layer name: conv4_block5_2_conv\n",
            "Layer name: conv4_block5_concat\n",
            "Layer name: conv4_block6_0_bn\n",
            "Layer name: conv4_block6_0_relu\n",
            "Layer name: conv4_block6_1_conv\n",
            "Layer name: conv4_block6_1_bn\n",
            "Layer name: conv4_block6_1_relu\n",
            "Layer name: conv4_block6_2_conv\n",
            "Layer name: conv4_block6_concat\n",
            "Layer name: conv4_block7_0_bn\n",
            "Layer name: conv4_block7_0_relu\n",
            "Layer name: conv4_block7_1_conv\n",
            "Layer name: conv4_block7_1_bn\n",
            "Layer name: conv4_block7_1_relu\n",
            "Layer name: conv4_block7_2_conv\n",
            "Layer name: conv4_block7_concat\n",
            "Layer name: conv4_block8_0_bn\n",
            "Layer name: conv4_block8_0_relu\n",
            "Layer name: conv4_block8_1_conv\n",
            "Layer name: conv4_block8_1_bn\n",
            "Layer name: conv4_block8_1_relu\n",
            "Layer name: conv4_block8_2_conv\n",
            "Layer name: conv4_block8_concat\n",
            "Layer name: conv4_block9_0_bn\n",
            "Layer name: conv4_block9_0_relu\n",
            "Layer name: conv4_block9_1_conv\n",
            "Layer name: conv4_block9_1_bn\n",
            "Layer name: conv4_block9_1_relu\n",
            "Layer name: conv4_block9_2_conv\n",
            "Layer name: conv4_block9_concat\n",
            "Layer name: conv4_block10_0_bn\n",
            "Layer name: conv4_block10_0_relu\n",
            "Layer name: conv4_block10_1_conv\n",
            "Layer name: conv4_block10_1_bn\n",
            "Layer name: conv4_block10_1_relu\n",
            "Layer name: conv4_block10_2_conv\n",
            "Layer name: conv4_block10_concat\n",
            "Layer name: conv4_block11_0_bn\n",
            "Layer name: conv4_block11_0_relu\n",
            "Layer name: conv4_block11_1_conv\n",
            "Layer name: conv4_block11_1_bn\n",
            "Layer name: conv4_block11_1_relu\n",
            "Layer name: conv4_block11_2_conv\n",
            "Layer name: conv4_block11_concat\n",
            "Layer name: conv4_block12_0_bn\n",
            "Layer name: conv4_block12_0_relu\n",
            "Layer name: conv4_block12_1_conv\n",
            "Layer name: conv4_block12_1_bn\n",
            "Layer name: conv4_block12_1_relu\n",
            "Layer name: conv4_block12_2_conv\n",
            "Layer name: conv4_block12_concat\n",
            "Layer name: conv4_block13_0_bn\n",
            "Layer name: conv4_block13_0_relu\n",
            "Layer name: conv4_block13_1_conv\n",
            "Layer name: conv4_block13_1_bn\n",
            "Layer name: conv4_block13_1_relu\n",
            "Layer name: conv4_block13_2_conv\n",
            "Layer name: conv4_block13_concat\n",
            "Layer name: conv4_block14_0_bn\n",
            "Layer name: conv4_block14_0_relu\n",
            "Layer name: conv4_block14_1_conv\n",
            "Layer name: conv4_block14_1_bn\n",
            "Layer name: conv4_block14_1_relu\n",
            "Layer name: conv4_block14_2_conv\n",
            "Layer name: conv4_block14_concat\n",
            "Layer name: conv4_block15_0_bn\n",
            "Layer name: conv4_block15_0_relu\n",
            "Layer name: conv4_block15_1_conv\n",
            "Layer name: conv4_block15_1_bn\n",
            "Layer name: conv4_block15_1_relu\n",
            "Layer name: conv4_block15_2_conv\n",
            "Layer name: conv4_block15_concat\n",
            "Layer name: conv4_block16_0_bn\n",
            "Layer name: conv4_block16_0_relu\n",
            "Layer name: conv4_block16_1_conv\n",
            "Layer name: conv4_block16_1_bn\n",
            "Layer name: conv4_block16_1_relu\n",
            "Layer name: conv4_block16_2_conv\n",
            "Layer name: conv4_block16_concat\n",
            "Layer name: conv4_block17_0_bn\n",
            "Layer name: conv4_block17_0_relu\n",
            "Layer name: conv4_block17_1_conv\n",
            "Layer name: conv4_block17_1_bn\n",
            "Layer name: conv4_block17_1_relu\n",
            "Layer name: conv4_block17_2_conv\n",
            "Layer name: conv4_block17_concat\n",
            "Layer name: conv4_block18_0_bn\n",
            "Layer name: conv4_block18_0_relu\n",
            "Layer name: conv4_block18_1_conv\n",
            "Layer name: conv4_block18_1_bn\n",
            "Layer name: conv4_block18_1_relu\n",
            "Layer name: conv4_block18_2_conv\n",
            "Layer name: conv4_block18_concat\n",
            "Layer name: conv4_block19_0_bn\n",
            "Layer name: conv4_block19_0_relu\n",
            "Layer name: conv4_block19_1_conv\n",
            "Layer name: conv4_block19_1_bn\n",
            "Layer name: conv4_block19_1_relu\n",
            "Layer name: conv4_block19_2_conv\n",
            "Layer name: conv4_block19_concat\n",
            "Layer name: conv4_block20_0_bn\n",
            "Layer name: conv4_block20_0_relu\n",
            "Layer name: conv4_block20_1_conv\n",
            "Layer name: conv4_block20_1_bn\n",
            "Layer name: conv4_block20_1_relu\n",
            "Layer name: conv4_block20_2_conv\n",
            "Layer name: conv4_block20_concat\n",
            "Layer name: conv4_block21_0_bn\n",
            "Layer name: conv4_block21_0_relu\n",
            "Layer name: conv4_block21_1_conv\n",
            "Layer name: conv4_block21_1_bn\n",
            "Layer name: conv4_block21_1_relu\n",
            "Layer name: conv4_block21_2_conv\n",
            "Layer name: conv4_block21_concat\n",
            "Layer name: conv4_block22_0_bn\n",
            "Layer name: conv4_block22_0_relu\n",
            "Layer name: conv4_block22_1_conv\n",
            "Layer name: conv4_block22_1_bn\n",
            "Layer name: conv4_block22_1_relu\n",
            "Layer name: conv4_block22_2_conv\n",
            "Layer name: conv4_block22_concat\n",
            "Layer name: conv4_block23_0_bn\n",
            "Layer name: conv4_block23_0_relu\n",
            "Layer name: conv4_block23_1_conv\n",
            "Layer name: conv4_block23_1_bn\n",
            "Layer name: conv4_block23_1_relu\n",
            "Layer name: conv4_block23_2_conv\n",
            "Layer name: conv4_block23_concat\n",
            "Layer name: conv4_block24_0_bn\n",
            "Layer name: conv4_block24_0_relu\n",
            "Layer name: conv4_block24_1_conv\n",
            "Layer name: conv4_block24_1_bn\n",
            "Layer name: conv4_block24_1_relu\n",
            "Layer name: conv4_block24_2_conv\n",
            "Layer name: conv4_block24_concat\n",
            "Layer name: pool4_bn\n",
            "Layer name: pool4_relu\n",
            "Layer name: pool4_conv\n",
            "Layer name: pool4_pool\n",
            "Layer name: conv5_block1_0_bn\n",
            "Layer name: conv5_block1_0_relu\n",
            "Layer name: conv5_block1_1_conv\n",
            "Layer name: conv5_block1_1_bn\n",
            "Layer name: conv5_block1_1_relu\n",
            "Layer name: conv5_block1_2_conv\n",
            "Layer name: conv5_block1_concat\n",
            "Layer name: conv5_block2_0_bn\n",
            "Layer name: conv5_block2_0_relu\n",
            "Layer name: conv5_block2_1_conv\n",
            "Layer name: conv5_block2_1_bn\n",
            "Layer name: conv5_block2_1_relu\n",
            "Layer name: conv5_block2_2_conv\n",
            "Layer name: conv5_block2_concat\n",
            "Layer name: conv5_block3_0_bn\n",
            "Layer name: conv5_block3_0_relu\n",
            "Layer name: conv5_block3_1_conv\n",
            "Layer name: conv5_block3_1_bn\n",
            "Layer name: conv5_block3_1_relu\n",
            "Layer name: conv5_block3_2_conv\n",
            "Layer name: conv5_block3_concat\n",
            "Layer name: conv5_block4_0_bn\n",
            "Layer name: conv5_block4_0_relu\n",
            "Layer name: conv5_block4_1_conv\n",
            "Layer name: conv5_block4_1_bn\n",
            "Layer name: conv5_block4_1_relu\n",
            "Layer name: conv5_block4_2_conv\n",
            "Layer name: conv5_block4_concat\n",
            "Layer name: conv5_block5_0_bn\n",
            "Layer name: conv5_block5_0_relu\n",
            "Layer name: conv5_block5_1_conv\n",
            "Layer name: conv5_block5_1_bn\n",
            "Layer name: conv5_block5_1_relu\n",
            "Layer name: conv5_block5_2_conv\n",
            "Layer name: conv5_block5_concat\n",
            "Layer name: conv5_block6_0_bn\n",
            "Layer name: conv5_block6_0_relu\n",
            "Layer name: conv5_block6_1_conv\n",
            "Layer name: conv5_block6_1_bn\n",
            "Layer name: conv5_block6_1_relu\n",
            "Layer name: conv5_block6_2_conv\n",
            "Layer name: conv5_block6_concat\n",
            "Layer name: conv5_block7_0_bn\n",
            "Layer name: conv5_block7_0_relu\n",
            "Layer name: conv5_block7_1_conv\n",
            "Layer name: conv5_block7_1_bn\n",
            "Layer name: conv5_block7_1_relu\n",
            "Layer name: conv5_block7_2_conv\n",
            "Layer name: conv5_block7_concat\n",
            "Layer name: conv5_block8_0_bn\n",
            "Layer name: conv5_block8_0_relu\n",
            "Layer name: conv5_block8_1_conv\n",
            "Layer name: conv5_block8_1_bn\n",
            "Layer name: conv5_block8_1_relu\n",
            "Layer name: conv5_block8_2_conv\n",
            "Layer name: conv5_block8_concat\n",
            "Layer name: conv5_block9_0_bn\n",
            "Layer name: conv5_block9_0_relu\n",
            "Layer name: conv5_block9_1_conv\n",
            "Layer name: conv5_block9_1_bn\n",
            "Layer name: conv5_block9_1_relu\n",
            "Layer name: conv5_block9_2_conv\n",
            "Layer name: conv5_block9_concat\n",
            "Layer name: conv5_block10_0_bn\n",
            "Layer name: conv5_block10_0_relu\n",
            "Layer name: conv5_block10_1_conv\n",
            "Layer name: conv5_block10_1_bn\n",
            "Layer name: conv5_block10_1_relu\n",
            "Layer name: conv5_block10_2_conv\n",
            "Layer name: conv5_block10_concat\n",
            "Layer name: conv5_block11_0_bn\n",
            "Layer name: conv5_block11_0_relu\n",
            "Layer name: conv5_block11_1_conv\n",
            "Layer name: conv5_block11_1_bn\n",
            "Layer name: conv5_block11_1_relu\n",
            "Layer name: conv5_block11_2_conv\n",
            "Layer name: conv5_block11_concat\n",
            "Layer name: conv5_block12_0_bn\n",
            "Layer name: conv5_block12_0_relu\n",
            "Layer name: conv5_block12_1_conv\n",
            "Layer name: conv5_block12_1_bn\n",
            "Layer name: conv5_block12_1_relu\n",
            "Layer name: conv5_block12_2_conv\n",
            "Layer name: conv5_block12_concat\n",
            "Layer name: conv5_block13_0_bn\n",
            "Layer name: conv5_block13_0_relu\n",
            "Layer name: conv5_block13_1_conv\n",
            "Layer name: conv5_block13_1_bn\n",
            "Layer name: conv5_block13_1_relu\n",
            "Layer name: conv5_block13_2_conv\n",
            "Layer name: conv5_block13_concat\n",
            "Layer name: conv5_block14_0_bn\n",
            "Layer name: conv5_block14_0_relu\n",
            "Layer name: conv5_block14_1_conv\n",
            "Layer name: conv5_block14_1_bn\n",
            "Layer name: conv5_block14_1_relu\n",
            "Layer name: conv5_block14_2_conv\n",
            "Layer name: conv5_block14_concat\n",
            "Layer name: conv5_block15_0_bn\n",
            "Layer name: conv5_block15_0_relu\n",
            "Layer name: conv5_block15_1_conv\n",
            "Layer name: conv5_block15_1_bn\n",
            "Layer name: conv5_block15_1_relu\n",
            "Layer name: conv5_block15_2_conv\n",
            "Layer name: conv5_block15_concat\n",
            "Layer name: conv5_block16_0_bn\n",
            "Layer name: conv5_block16_0_relu\n",
            "Layer name: conv5_block16_1_conv\n",
            "Layer name: conv5_block16_1_bn\n",
            "Layer name: conv5_block16_1_relu\n",
            "Layer name: conv5_block16_2_conv\n",
            "Layer name: conv5_block16_concat\n",
            "Layer name: bn\n",
            "Layer name: relu\n",
            "Layer name: global_average_pooling2d\n",
            "Layer name: dropout\n",
            "Layer name: dense\n",
            "Layer name: dropout_1\n",
            "Layer name: dense_1\n",
            "Total layers in DenseNet model: 432\n",
            "\n",
            "Cloning models with unique prefixes...\n",
            "Renamed layer: efficientnet_input_layer_6\n",
            "Renamed layer: efficientnet_rescaling_12\n",
            "Renamed layer: efficientnet_normalization_6\n",
            "Renamed layer: efficientnet_rescaling_13\n",
            "Renamed layer: efficientnet_stem_conv_pad\n",
            "Renamed layer: efficientnet_stem_conv\n",
            "Renamed layer: efficientnet_stem_bn\n",
            "Renamed layer: efficientnet_stem_activation\n",
            "Renamed layer: efficientnet_block1a_dwconv\n",
            "Renamed layer: efficientnet_block1a_bn\n",
            "Renamed layer: efficientnet_block1a_activation\n",
            "Renamed layer: efficientnet_block1a_se_squeeze\n",
            "Renamed layer: efficientnet_block1a_se_reshape\n",
            "Renamed layer: efficientnet_block1a_se_reduce\n",
            "Renamed layer: efficientnet_block1a_se_expand\n",
            "Renamed layer: efficientnet_block1a_se_excite\n",
            "Renamed layer: efficientnet_block1a_project_conv\n",
            "Renamed layer: efficientnet_block1a_project_bn\n",
            "Renamed layer: efficientnet_block2a_expand_conv\n",
            "Renamed layer: efficientnet_block2a_expand_bn\n",
            "Renamed layer: efficientnet_block2a_expand_activation\n",
            "Renamed layer: efficientnet_block2a_dwconv_pad\n",
            "Renamed layer: efficientnet_block2a_dwconv\n",
            "Renamed layer: efficientnet_block2a_bn\n",
            "Renamed layer: efficientnet_block2a_activation\n",
            "Renamed layer: efficientnet_block2a_se_squeeze\n",
            "Renamed layer: efficientnet_block2a_se_reshape\n",
            "Renamed layer: efficientnet_block2a_se_reduce\n",
            "Renamed layer: efficientnet_block2a_se_expand\n",
            "Renamed layer: efficientnet_block2a_se_excite\n",
            "Renamed layer: efficientnet_block2a_project_conv\n",
            "Renamed layer: efficientnet_block2a_project_bn\n",
            "Renamed layer: efficientnet_block2b_expand_conv\n",
            "Renamed layer: efficientnet_block2b_expand_bn\n",
            "Renamed layer: efficientnet_block2b_expand_activation\n",
            "Renamed layer: efficientnet_block2b_dwconv\n",
            "Renamed layer: efficientnet_block2b_bn\n",
            "Renamed layer: efficientnet_block2b_activation\n",
            "Renamed layer: efficientnet_block2b_se_squeeze\n",
            "Renamed layer: efficientnet_block2b_se_reshape\n",
            "Renamed layer: efficientnet_block2b_se_reduce\n",
            "Renamed layer: efficientnet_block2b_se_expand\n",
            "Renamed layer: efficientnet_block2b_se_excite\n",
            "Renamed layer: efficientnet_block2b_project_conv\n",
            "Renamed layer: efficientnet_block2b_project_bn\n",
            "Renamed layer: efficientnet_block2b_drop\n",
            "Renamed layer: efficientnet_block2b_add\n",
            "Renamed layer: efficientnet_block3a_expand_conv\n",
            "Renamed layer: efficientnet_block3a_expand_bn\n",
            "Renamed layer: efficientnet_block3a_expand_activation\n",
            "Renamed layer: efficientnet_block3a_dwconv_pad\n",
            "Renamed layer: efficientnet_block3a_dwconv\n",
            "Renamed layer: efficientnet_block3a_bn\n",
            "Renamed layer: efficientnet_block3a_activation\n",
            "Renamed layer: efficientnet_block3a_se_squeeze\n",
            "Renamed layer: efficientnet_block3a_se_reshape\n",
            "Renamed layer: efficientnet_block3a_se_reduce\n",
            "Renamed layer: efficientnet_block3a_se_expand\n",
            "Renamed layer: efficientnet_block3a_se_excite\n",
            "Renamed layer: efficientnet_block3a_project_conv\n",
            "Renamed layer: efficientnet_block3a_project_bn\n",
            "Renamed layer: efficientnet_block3b_expand_conv\n",
            "Renamed layer: efficientnet_block3b_expand_bn\n",
            "Renamed layer: efficientnet_block3b_expand_activation\n",
            "Renamed layer: efficientnet_block3b_dwconv\n",
            "Renamed layer: efficientnet_block3b_bn\n",
            "Renamed layer: efficientnet_block3b_activation\n",
            "Renamed layer: efficientnet_block3b_se_squeeze\n",
            "Renamed layer: efficientnet_block3b_se_reshape\n",
            "Renamed layer: efficientnet_block3b_se_reduce\n",
            "Renamed layer: efficientnet_block3b_se_expand\n",
            "Renamed layer: efficientnet_block3b_se_excite\n",
            "Renamed layer: efficientnet_block3b_project_conv\n",
            "Renamed layer: efficientnet_block3b_project_bn\n",
            "Renamed layer: efficientnet_block3b_drop\n",
            "Renamed layer: efficientnet_block3b_add\n",
            "Renamed layer: efficientnet_block4a_expand_conv\n",
            "Renamed layer: efficientnet_block4a_expand_bn\n",
            "Renamed layer: efficientnet_block4a_expand_activation\n",
            "Renamed layer: efficientnet_block4a_dwconv_pad\n",
            "Renamed layer: efficientnet_block4a_dwconv\n",
            "Renamed layer: efficientnet_block4a_bn\n",
            "Renamed layer: efficientnet_block4a_activation\n",
            "Renamed layer: efficientnet_block4a_se_squeeze\n",
            "Renamed layer: efficientnet_block4a_se_reshape\n",
            "Renamed layer: efficientnet_block4a_se_reduce\n",
            "Renamed layer: efficientnet_block4a_se_expand\n",
            "Renamed layer: efficientnet_block4a_se_excite\n",
            "Renamed layer: efficientnet_block4a_project_conv\n",
            "Renamed layer: efficientnet_block4a_project_bn\n",
            "Renamed layer: efficientnet_block4b_expand_conv\n",
            "Renamed layer: efficientnet_block4b_expand_bn\n",
            "Renamed layer: efficientnet_block4b_expand_activation\n",
            "Renamed layer: efficientnet_block4b_dwconv\n",
            "Renamed layer: efficientnet_block4b_bn\n",
            "Renamed layer: efficientnet_block4b_activation\n",
            "Renamed layer: efficientnet_block4b_se_squeeze\n",
            "Renamed layer: efficientnet_block4b_se_reshape\n",
            "Renamed layer: efficientnet_block4b_se_reduce\n",
            "Renamed layer: efficientnet_block4b_se_expand\n",
            "Renamed layer: efficientnet_block4b_se_excite\n",
            "Renamed layer: efficientnet_block4b_project_conv\n",
            "Renamed layer: efficientnet_block4b_project_bn\n",
            "Renamed layer: efficientnet_block4b_drop\n",
            "Renamed layer: efficientnet_block4b_add\n",
            "Renamed layer: efficientnet_block4c_expand_conv\n",
            "Renamed layer: efficientnet_block4c_expand_bn\n",
            "Renamed layer: efficientnet_block4c_expand_activation\n",
            "Renamed layer: efficientnet_block4c_dwconv\n",
            "Renamed layer: efficientnet_block4c_bn\n",
            "Renamed layer: efficientnet_block4c_activation\n",
            "Renamed layer: efficientnet_block4c_se_squeeze\n",
            "Renamed layer: efficientnet_block4c_se_reshape\n",
            "Renamed layer: efficientnet_block4c_se_reduce\n",
            "Renamed layer: efficientnet_block4c_se_expand\n",
            "Renamed layer: efficientnet_block4c_se_excite\n",
            "Renamed layer: efficientnet_block4c_project_conv\n",
            "Renamed layer: efficientnet_block4c_project_bn\n",
            "Renamed layer: efficientnet_block4c_drop\n",
            "Renamed layer: efficientnet_block4c_add\n",
            "Renamed layer: efficientnet_block5a_expand_conv\n",
            "Renamed layer: efficientnet_block5a_expand_bn\n",
            "Renamed layer: efficientnet_block5a_expand_activation\n",
            "Renamed layer: efficientnet_block5a_dwconv\n",
            "Renamed layer: efficientnet_block5a_bn\n",
            "Renamed layer: efficientnet_block5a_activation\n",
            "Renamed layer: efficientnet_block5a_se_squeeze\n",
            "Renamed layer: efficientnet_block5a_se_reshape\n",
            "Renamed layer: efficientnet_block5a_se_reduce\n",
            "Renamed layer: efficientnet_block5a_se_expand\n",
            "Renamed layer: efficientnet_block5a_se_excite\n",
            "Renamed layer: efficientnet_block5a_project_conv\n",
            "Renamed layer: efficientnet_block5a_project_bn\n",
            "Renamed layer: efficientnet_block5b_expand_conv\n",
            "Renamed layer: efficientnet_block5b_expand_bn\n",
            "Renamed layer: efficientnet_block5b_expand_activation\n",
            "Renamed layer: efficientnet_block5b_dwconv\n",
            "Renamed layer: efficientnet_block5b_bn\n",
            "Renamed layer: efficientnet_block5b_activation\n",
            "Renamed layer: efficientnet_block5b_se_squeeze\n",
            "Renamed layer: efficientnet_block5b_se_reshape\n",
            "Renamed layer: efficientnet_block5b_se_reduce\n",
            "Renamed layer: efficientnet_block5b_se_expand\n",
            "Renamed layer: efficientnet_block5b_se_excite\n",
            "Renamed layer: efficientnet_block5b_project_conv\n",
            "Renamed layer: efficientnet_block5b_project_bn\n",
            "Renamed layer: efficientnet_block5b_drop\n",
            "Renamed layer: efficientnet_block5b_add\n",
            "Renamed layer: efficientnet_block5c_expand_conv\n",
            "Renamed layer: efficientnet_block5c_expand_bn\n",
            "Renamed layer: efficientnet_block5c_expand_activation\n",
            "Renamed layer: efficientnet_block5c_dwconv\n",
            "Renamed layer: efficientnet_block5c_bn\n",
            "Renamed layer: efficientnet_block5c_activation\n",
            "Renamed layer: efficientnet_block5c_se_squeeze\n",
            "Renamed layer: efficientnet_block5c_se_reshape\n",
            "Renamed layer: efficientnet_block5c_se_reduce\n",
            "Renamed layer: efficientnet_block5c_se_expand\n",
            "Renamed layer: efficientnet_block5c_se_excite\n",
            "Renamed layer: efficientnet_block5c_project_conv\n",
            "Renamed layer: efficientnet_block5c_project_bn\n",
            "Renamed layer: efficientnet_block5c_drop\n",
            "Renamed layer: efficientnet_block5c_add\n",
            "Renamed layer: efficientnet_block6a_expand_conv\n",
            "Renamed layer: efficientnet_block6a_expand_bn\n",
            "Renamed layer: efficientnet_block6a_expand_activation\n",
            "Renamed layer: efficientnet_block6a_dwconv_pad\n",
            "Renamed layer: efficientnet_block6a_dwconv\n",
            "Renamed layer: efficientnet_block6a_bn\n",
            "Renamed layer: efficientnet_block6a_activation\n",
            "Renamed layer: efficientnet_block6a_se_squeeze\n",
            "Renamed layer: efficientnet_block6a_se_reshape\n",
            "Renamed layer: efficientnet_block6a_se_reduce\n",
            "Renamed layer: efficientnet_block6a_se_expand\n",
            "Renamed layer: efficientnet_block6a_se_excite\n",
            "Renamed layer: efficientnet_block6a_project_conv\n",
            "Renamed layer: efficientnet_block6a_project_bn\n",
            "Renamed layer: efficientnet_block6b_expand_conv\n",
            "Renamed layer: efficientnet_block6b_expand_bn\n",
            "Renamed layer: efficientnet_block6b_expand_activation\n",
            "Renamed layer: efficientnet_block6b_dwconv\n",
            "Renamed layer: efficientnet_block6b_bn\n",
            "Renamed layer: efficientnet_block6b_activation\n",
            "Renamed layer: efficientnet_block6b_se_squeeze\n",
            "Renamed layer: efficientnet_block6b_se_reshape\n",
            "Renamed layer: efficientnet_block6b_se_reduce\n",
            "Renamed layer: efficientnet_block6b_se_expand\n",
            "Renamed layer: efficientnet_block6b_se_excite\n",
            "Renamed layer: efficientnet_block6b_project_conv\n",
            "Renamed layer: efficientnet_block6b_project_bn\n",
            "Renamed layer: efficientnet_block6b_drop\n",
            "Renamed layer: efficientnet_block6b_add\n",
            "Renamed layer: efficientnet_block6c_expand_conv\n",
            "Renamed layer: efficientnet_block6c_expand_bn\n",
            "Renamed layer: efficientnet_block6c_expand_activation\n",
            "Renamed layer: efficientnet_block6c_dwconv\n",
            "Renamed layer: efficientnet_block6c_bn\n",
            "Renamed layer: efficientnet_block6c_activation\n",
            "Renamed layer: efficientnet_block6c_se_squeeze\n",
            "Renamed layer: efficientnet_block6c_se_reshape\n",
            "Renamed layer: efficientnet_block6c_se_reduce\n",
            "Renamed layer: efficientnet_block6c_se_expand\n",
            "Renamed layer: efficientnet_block6c_se_excite\n",
            "Renamed layer: efficientnet_block6c_project_conv\n",
            "Renamed layer: efficientnet_block6c_project_bn\n",
            "Renamed layer: efficientnet_block6c_drop\n",
            "Renamed layer: efficientnet_block6c_add\n",
            "Renamed layer: efficientnet_block6d_expand_conv\n",
            "Renamed layer: efficientnet_block6d_expand_bn\n",
            "Renamed layer: efficientnet_block6d_expand_activation\n",
            "Renamed layer: efficientnet_block6d_dwconv\n",
            "Renamed layer: efficientnet_block6d_bn\n",
            "Renamed layer: efficientnet_block6d_activation\n",
            "Renamed layer: efficientnet_block6d_se_squeeze\n",
            "Renamed layer: efficientnet_block6d_se_reshape\n",
            "Renamed layer: efficientnet_block6d_se_reduce\n",
            "Renamed layer: efficientnet_block6d_se_expand\n",
            "Renamed layer: efficientnet_block6d_se_excite\n",
            "Renamed layer: efficientnet_block6d_project_conv\n",
            "Renamed layer: efficientnet_block6d_project_bn\n",
            "Renamed layer: efficientnet_block6d_drop\n",
            "Renamed layer: efficientnet_block6d_add\n",
            "Renamed layer: efficientnet_block7a_expand_conv\n",
            "Renamed layer: efficientnet_block7a_expand_bn\n",
            "Renamed layer: efficientnet_block7a_expand_activation\n",
            "Renamed layer: efficientnet_block7a_dwconv\n",
            "Renamed layer: efficientnet_block7a_bn\n",
            "Renamed layer: efficientnet_block7a_activation\n",
            "Renamed layer: efficientnet_block7a_se_squeeze\n",
            "Renamed layer: efficientnet_block7a_se_reshape\n",
            "Renamed layer: efficientnet_block7a_se_reduce\n",
            "Renamed layer: efficientnet_block7a_se_expand\n",
            "Renamed layer: efficientnet_block7a_se_excite\n",
            "Renamed layer: efficientnet_block7a_project_conv\n",
            "Renamed layer: efficientnet_block7a_project_bn\n",
            "Renamed layer: efficientnet_top_conv\n",
            "Renamed layer: efficientnet_top_bn\n",
            "Renamed layer: efficientnet_top_activation\n",
            "Renamed layer: efficientnet_global_average_pooling2d_6\n",
            "Renamed layer: efficientnet_dense_12\n",
            "Renamed layer: efficientnet_dropout_6\n",
            "Renamed layer: efficientnet_dense_13\n",
            "Cloned model name: efficientnet_model\n",
            "Renamed layer: resnet_input_layer\n",
            "Renamed layer: resnet_conv1_pad\n",
            "Renamed layer: resnet_conv1_conv\n",
            "Renamed layer: resnet_conv1_bn\n",
            "Renamed layer: resnet_conv1_relu\n",
            "Renamed layer: resnet_pool1_pad\n",
            "Renamed layer: resnet_pool1_pool\n",
            "Renamed layer: resnet_conv2_block1_1_conv\n",
            "Renamed layer: resnet_conv2_block1_1_bn\n",
            "Renamed layer: resnet_conv2_block1_1_relu\n",
            "Renamed layer: resnet_conv2_block1_2_conv\n",
            "Renamed layer: resnet_conv2_block1_2_bn\n",
            "Renamed layer: resnet_conv2_block1_2_relu\n",
            "Renamed layer: resnet_conv2_block1_0_conv\n",
            "Renamed layer: resnet_conv2_block1_3_conv\n",
            "Renamed layer: resnet_conv2_block1_0_bn\n",
            "Renamed layer: resnet_conv2_block1_3_bn\n",
            "Renamed layer: resnet_conv2_block1_add\n",
            "Renamed layer: resnet_conv2_block1_out\n",
            "Renamed layer: resnet_conv2_block2_1_conv\n",
            "Renamed layer: resnet_conv2_block2_1_bn\n",
            "Renamed layer: resnet_conv2_block2_1_relu\n",
            "Renamed layer: resnet_conv2_block2_2_conv\n",
            "Renamed layer: resnet_conv2_block2_2_bn\n",
            "Renamed layer: resnet_conv2_block2_2_relu\n",
            "Renamed layer: resnet_conv2_block2_3_conv\n",
            "Renamed layer: resnet_conv2_block2_3_bn\n",
            "Renamed layer: resnet_conv2_block2_add\n",
            "Renamed layer: resnet_conv2_block2_out\n",
            "Renamed layer: resnet_conv2_block3_1_conv\n",
            "Renamed layer: resnet_conv2_block3_1_bn\n",
            "Renamed layer: resnet_conv2_block3_1_relu\n",
            "Renamed layer: resnet_conv2_block3_2_conv\n",
            "Renamed layer: resnet_conv2_block3_2_bn\n",
            "Renamed layer: resnet_conv2_block3_2_relu\n",
            "Renamed layer: resnet_conv2_block3_3_conv\n",
            "Renamed layer: resnet_conv2_block3_3_bn\n",
            "Renamed layer: resnet_conv2_block3_add\n",
            "Renamed layer: resnet_conv2_block3_out\n",
            "Renamed layer: resnet_conv3_block1_1_conv\n",
            "Renamed layer: resnet_conv3_block1_1_bn\n",
            "Renamed layer: resnet_conv3_block1_1_relu\n",
            "Renamed layer: resnet_conv3_block1_2_conv\n",
            "Renamed layer: resnet_conv3_block1_2_bn\n",
            "Renamed layer: resnet_conv3_block1_2_relu\n",
            "Renamed layer: resnet_conv3_block1_0_conv\n",
            "Renamed layer: resnet_conv3_block1_3_conv\n",
            "Renamed layer: resnet_conv3_block1_0_bn\n",
            "Renamed layer: resnet_conv3_block1_3_bn\n",
            "Renamed layer: resnet_conv3_block1_add\n",
            "Renamed layer: resnet_conv3_block1_out\n",
            "Renamed layer: resnet_conv3_block2_1_conv\n",
            "Renamed layer: resnet_conv3_block2_1_bn\n",
            "Renamed layer: resnet_conv3_block2_1_relu\n",
            "Renamed layer: resnet_conv3_block2_2_conv\n",
            "Renamed layer: resnet_conv3_block2_2_bn\n",
            "Renamed layer: resnet_conv3_block2_2_relu\n",
            "Renamed layer: resnet_conv3_block2_3_conv\n",
            "Renamed layer: resnet_conv3_block2_3_bn\n",
            "Renamed layer: resnet_conv3_block2_add\n",
            "Renamed layer: resnet_conv3_block2_out\n",
            "Renamed layer: resnet_conv3_block3_1_conv\n",
            "Renamed layer: resnet_conv3_block3_1_bn\n",
            "Renamed layer: resnet_conv3_block3_1_relu\n",
            "Renamed layer: resnet_conv3_block3_2_conv\n",
            "Renamed layer: resnet_conv3_block3_2_bn\n",
            "Renamed layer: resnet_conv3_block3_2_relu\n",
            "Renamed layer: resnet_conv3_block3_3_conv\n",
            "Renamed layer: resnet_conv3_block3_3_bn\n",
            "Renamed layer: resnet_conv3_block3_add\n",
            "Renamed layer: resnet_conv3_block3_out\n",
            "Renamed layer: resnet_conv3_block4_1_conv\n",
            "Renamed layer: resnet_conv3_block4_1_bn\n",
            "Renamed layer: resnet_conv3_block4_1_relu\n",
            "Renamed layer: resnet_conv3_block4_2_conv\n",
            "Renamed layer: resnet_conv3_block4_2_bn\n",
            "Renamed layer: resnet_conv3_block4_2_relu\n",
            "Renamed layer: resnet_conv3_block4_3_conv\n",
            "Renamed layer: resnet_conv3_block4_3_bn\n",
            "Renamed layer: resnet_conv3_block4_add\n",
            "Renamed layer: resnet_conv3_block4_out\n",
            "Renamed layer: resnet_conv4_block1_1_conv\n",
            "Renamed layer: resnet_conv4_block1_1_bn\n",
            "Renamed layer: resnet_conv4_block1_1_relu\n",
            "Renamed layer: resnet_conv4_block1_2_conv\n",
            "Renamed layer: resnet_conv4_block1_2_bn\n",
            "Renamed layer: resnet_conv4_block1_2_relu\n",
            "Renamed layer: resnet_conv4_block1_0_conv\n",
            "Renamed layer: resnet_conv4_block1_3_conv\n",
            "Renamed layer: resnet_conv4_block1_0_bn\n",
            "Renamed layer: resnet_conv4_block1_3_bn\n",
            "Renamed layer: resnet_conv4_block1_add\n",
            "Renamed layer: resnet_conv4_block1_out\n",
            "Renamed layer: resnet_conv4_block2_1_conv\n",
            "Renamed layer: resnet_conv4_block2_1_bn\n",
            "Renamed layer: resnet_conv4_block2_1_relu\n",
            "Renamed layer: resnet_conv4_block2_2_conv\n",
            "Renamed layer: resnet_conv4_block2_2_bn\n",
            "Renamed layer: resnet_conv4_block2_2_relu\n",
            "Renamed layer: resnet_conv4_block2_3_conv\n",
            "Renamed layer: resnet_conv4_block2_3_bn\n",
            "Renamed layer: resnet_conv4_block2_add\n",
            "Renamed layer: resnet_conv4_block2_out\n",
            "Renamed layer: resnet_conv4_block3_1_conv\n",
            "Renamed layer: resnet_conv4_block3_1_bn\n",
            "Renamed layer: resnet_conv4_block3_1_relu\n",
            "Renamed layer: resnet_conv4_block3_2_conv\n",
            "Renamed layer: resnet_conv4_block3_2_bn\n",
            "Renamed layer: resnet_conv4_block3_2_relu\n",
            "Renamed layer: resnet_conv4_block3_3_conv\n",
            "Renamed layer: resnet_conv4_block3_3_bn\n",
            "Renamed layer: resnet_conv4_block3_add\n",
            "Renamed layer: resnet_conv4_block3_out\n",
            "Renamed layer: resnet_conv4_block4_1_conv\n",
            "Renamed layer: resnet_conv4_block4_1_bn\n",
            "Renamed layer: resnet_conv4_block4_1_relu\n",
            "Renamed layer: resnet_conv4_block4_2_conv\n",
            "Renamed layer: resnet_conv4_block4_2_bn\n",
            "Renamed layer: resnet_conv4_block4_2_relu\n",
            "Renamed layer: resnet_conv4_block4_3_conv\n",
            "Renamed layer: resnet_conv4_block4_3_bn\n",
            "Renamed layer: resnet_conv4_block4_add\n",
            "Renamed layer: resnet_conv4_block4_out\n",
            "Renamed layer: resnet_conv4_block5_1_conv\n",
            "Renamed layer: resnet_conv4_block5_1_bn\n",
            "Renamed layer: resnet_conv4_block5_1_relu\n",
            "Renamed layer: resnet_conv4_block5_2_conv\n",
            "Renamed layer: resnet_conv4_block5_2_bn\n",
            "Renamed layer: resnet_conv4_block5_2_relu\n",
            "Renamed layer: resnet_conv4_block5_3_conv\n",
            "Renamed layer: resnet_conv4_block5_3_bn\n",
            "Renamed layer: resnet_conv4_block5_add\n",
            "Renamed layer: resnet_conv4_block5_out\n",
            "Renamed layer: resnet_conv4_block6_1_conv\n",
            "Renamed layer: resnet_conv4_block6_1_bn\n",
            "Renamed layer: resnet_conv4_block6_1_relu\n",
            "Renamed layer: resnet_conv4_block6_2_conv\n",
            "Renamed layer: resnet_conv4_block6_2_bn\n",
            "Renamed layer: resnet_conv4_block6_2_relu\n",
            "Renamed layer: resnet_conv4_block6_3_conv\n",
            "Renamed layer: resnet_conv4_block6_3_bn\n",
            "Renamed layer: resnet_conv4_block6_add\n",
            "Renamed layer: resnet_conv4_block6_out\n",
            "Renamed layer: resnet_conv5_block1_1_conv\n",
            "Renamed layer: resnet_conv5_block1_1_bn\n",
            "Renamed layer: resnet_conv5_block1_1_relu\n",
            "Renamed layer: resnet_conv5_block1_2_conv\n",
            "Renamed layer: resnet_conv5_block1_2_bn\n",
            "Renamed layer: resnet_conv5_block1_2_relu\n",
            "Renamed layer: resnet_conv5_block1_0_conv\n",
            "Renamed layer: resnet_conv5_block1_3_conv\n",
            "Renamed layer: resnet_conv5_block1_0_bn\n",
            "Renamed layer: resnet_conv5_block1_3_bn\n",
            "Renamed layer: resnet_conv5_block1_add\n",
            "Renamed layer: resnet_conv5_block1_out\n",
            "Renamed layer: resnet_conv5_block2_1_conv\n",
            "Renamed layer: resnet_conv5_block2_1_bn\n",
            "Renamed layer: resnet_conv5_block2_1_relu\n",
            "Renamed layer: resnet_conv5_block2_2_conv\n",
            "Renamed layer: resnet_conv5_block2_2_bn\n",
            "Renamed layer: resnet_conv5_block2_2_relu\n",
            "Renamed layer: resnet_conv5_block2_3_conv\n",
            "Renamed layer: resnet_conv5_block2_3_bn\n",
            "Renamed layer: resnet_conv5_block2_add\n",
            "Renamed layer: resnet_conv5_block2_out\n",
            "Renamed layer: resnet_conv5_block3_1_conv\n",
            "Renamed layer: resnet_conv5_block3_1_bn\n",
            "Renamed layer: resnet_conv5_block3_1_relu\n",
            "Renamed layer: resnet_conv5_block3_2_conv\n",
            "Renamed layer: resnet_conv5_block3_2_bn\n",
            "Renamed layer: resnet_conv5_block3_2_relu\n",
            "Renamed layer: resnet_conv5_block3_3_conv\n",
            "Renamed layer: resnet_conv5_block3_3_bn\n",
            "Renamed layer: resnet_conv5_block3_add\n",
            "Renamed layer: resnet_conv5_block3_out\n",
            "Renamed layer: resnet_global_average_pooling2d\n",
            "Renamed layer: resnet_dense\n",
            "Renamed layer: resnet_dropout\n",
            "Renamed layer: resnet_dense_1\n",
            "Cloned model name: resnet_model\n",
            "Renamed layer: densenet_input_layer\n",
            "Renamed layer: densenet_zero_padding2d\n",
            "Renamed layer: densenet_conv1_conv\n",
            "Renamed layer: densenet_conv1_bn\n",
            "Renamed layer: densenet_conv1_relu\n",
            "Renamed layer: densenet_zero_padding2d_1\n",
            "Renamed layer: densenet_pool1\n",
            "Renamed layer: densenet_conv2_block1_0_bn\n",
            "Renamed layer: densenet_conv2_block1_0_relu\n",
            "Renamed layer: densenet_conv2_block1_1_conv\n",
            "Renamed layer: densenet_conv2_block1_1_bn\n",
            "Renamed layer: densenet_conv2_block1_1_relu\n",
            "Renamed layer: densenet_conv2_block1_2_conv\n",
            "Renamed layer: densenet_conv2_block1_concat\n",
            "Renamed layer: densenet_conv2_block2_0_bn\n",
            "Renamed layer: densenet_conv2_block2_0_relu\n",
            "Renamed layer: densenet_conv2_block2_1_conv\n",
            "Renamed layer: densenet_conv2_block2_1_bn\n",
            "Renamed layer: densenet_conv2_block2_1_relu\n",
            "Renamed layer: densenet_conv2_block2_2_conv\n",
            "Renamed layer: densenet_conv2_block2_concat\n",
            "Renamed layer: densenet_conv2_block3_0_bn\n",
            "Renamed layer: densenet_conv2_block3_0_relu\n",
            "Renamed layer: densenet_conv2_block3_1_conv\n",
            "Renamed layer: densenet_conv2_block3_1_bn\n",
            "Renamed layer: densenet_conv2_block3_1_relu\n",
            "Renamed layer: densenet_conv2_block3_2_conv\n",
            "Renamed layer: densenet_conv2_block3_concat\n",
            "Renamed layer: densenet_conv2_block4_0_bn\n",
            "Renamed layer: densenet_conv2_block4_0_relu\n",
            "Renamed layer: densenet_conv2_block4_1_conv\n",
            "Renamed layer: densenet_conv2_block4_1_bn\n",
            "Renamed layer: densenet_conv2_block4_1_relu\n",
            "Renamed layer: densenet_conv2_block4_2_conv\n",
            "Renamed layer: densenet_conv2_block4_concat\n",
            "Renamed layer: densenet_conv2_block5_0_bn\n",
            "Renamed layer: densenet_conv2_block5_0_relu\n",
            "Renamed layer: densenet_conv2_block5_1_conv\n",
            "Renamed layer: densenet_conv2_block5_1_bn\n",
            "Renamed layer: densenet_conv2_block5_1_relu\n",
            "Renamed layer: densenet_conv2_block5_2_conv\n",
            "Renamed layer: densenet_conv2_block5_concat\n",
            "Renamed layer: densenet_conv2_block6_0_bn\n",
            "Renamed layer: densenet_conv2_block6_0_relu\n",
            "Renamed layer: densenet_conv2_block6_1_conv\n",
            "Renamed layer: densenet_conv2_block6_1_bn\n",
            "Renamed layer: densenet_conv2_block6_1_relu\n",
            "Renamed layer: densenet_conv2_block6_2_conv\n",
            "Renamed layer: densenet_conv2_block6_concat\n",
            "Renamed layer: densenet_pool2_bn\n",
            "Renamed layer: densenet_pool2_relu\n",
            "Renamed layer: densenet_pool2_conv\n",
            "Renamed layer: densenet_pool2_pool\n",
            "Renamed layer: densenet_conv3_block1_0_bn\n",
            "Renamed layer: densenet_conv3_block1_0_relu\n",
            "Renamed layer: densenet_conv3_block1_1_conv\n",
            "Renamed layer: densenet_conv3_block1_1_bn\n",
            "Renamed layer: densenet_conv3_block1_1_relu\n",
            "Renamed layer: densenet_conv3_block1_2_conv\n",
            "Renamed layer: densenet_conv3_block1_concat\n",
            "Renamed layer: densenet_conv3_block2_0_bn\n",
            "Renamed layer: densenet_conv3_block2_0_relu\n",
            "Renamed layer: densenet_conv3_block2_1_conv\n",
            "Renamed layer: densenet_conv3_block2_1_bn\n",
            "Renamed layer: densenet_conv3_block2_1_relu\n",
            "Renamed layer: densenet_conv3_block2_2_conv\n",
            "Renamed layer: densenet_conv3_block2_concat\n",
            "Renamed layer: densenet_conv3_block3_0_bn\n",
            "Renamed layer: densenet_conv3_block3_0_relu\n",
            "Renamed layer: densenet_conv3_block3_1_conv\n",
            "Renamed layer: densenet_conv3_block3_1_bn\n",
            "Renamed layer: densenet_conv3_block3_1_relu\n",
            "Renamed layer: densenet_conv3_block3_2_conv\n",
            "Renamed layer: densenet_conv3_block3_concat\n",
            "Renamed layer: densenet_conv3_block4_0_bn\n",
            "Renamed layer: densenet_conv3_block4_0_relu\n",
            "Renamed layer: densenet_conv3_block4_1_conv\n",
            "Renamed layer: densenet_conv3_block4_1_bn\n",
            "Renamed layer: densenet_conv3_block4_1_relu\n",
            "Renamed layer: densenet_conv3_block4_2_conv\n",
            "Renamed layer: densenet_conv3_block4_concat\n",
            "Renamed layer: densenet_conv3_block5_0_bn\n",
            "Renamed layer: densenet_conv3_block5_0_relu\n",
            "Renamed layer: densenet_conv3_block5_1_conv\n",
            "Renamed layer: densenet_conv3_block5_1_bn\n",
            "Renamed layer: densenet_conv3_block5_1_relu\n",
            "Renamed layer: densenet_conv3_block5_2_conv\n",
            "Renamed layer: densenet_conv3_block5_concat\n",
            "Renamed layer: densenet_conv3_block6_0_bn\n",
            "Renamed layer: densenet_conv3_block6_0_relu\n",
            "Renamed layer: densenet_conv3_block6_1_conv\n",
            "Renamed layer: densenet_conv3_block6_1_bn\n",
            "Renamed layer: densenet_conv3_block6_1_relu\n",
            "Renamed layer: densenet_conv3_block6_2_conv\n",
            "Renamed layer: densenet_conv3_block6_concat\n",
            "Renamed layer: densenet_conv3_block7_0_bn\n",
            "Renamed layer: densenet_conv3_block7_0_relu\n",
            "Renamed layer: densenet_conv3_block7_1_conv\n",
            "Renamed layer: densenet_conv3_block7_1_bn\n",
            "Renamed layer: densenet_conv3_block7_1_relu\n",
            "Renamed layer: densenet_conv3_block7_2_conv\n",
            "Renamed layer: densenet_conv3_block7_concat\n",
            "Renamed layer: densenet_conv3_block8_0_bn\n",
            "Renamed layer: densenet_conv3_block8_0_relu\n",
            "Renamed layer: densenet_conv3_block8_1_conv\n",
            "Renamed layer: densenet_conv3_block8_1_bn\n",
            "Renamed layer: densenet_conv3_block8_1_relu\n",
            "Renamed layer: densenet_conv3_block8_2_conv\n",
            "Renamed layer: densenet_conv3_block8_concat\n",
            "Renamed layer: densenet_conv3_block9_0_bn\n",
            "Renamed layer: densenet_conv3_block9_0_relu\n",
            "Renamed layer: densenet_conv3_block9_1_conv\n",
            "Renamed layer: densenet_conv3_block9_1_bn\n",
            "Renamed layer: densenet_conv3_block9_1_relu\n",
            "Renamed layer: densenet_conv3_block9_2_conv\n",
            "Renamed layer: densenet_conv3_block9_concat\n",
            "Renamed layer: densenet_conv3_block10_0_bn\n",
            "Renamed layer: densenet_conv3_block10_0_relu\n",
            "Renamed layer: densenet_conv3_block10_1_conv\n",
            "Renamed layer: densenet_conv3_block10_1_bn\n",
            "Renamed layer: densenet_conv3_block10_1_relu\n",
            "Renamed layer: densenet_conv3_block10_2_conv\n",
            "Renamed layer: densenet_conv3_block10_concat\n",
            "Renamed layer: densenet_conv3_block11_0_bn\n",
            "Renamed layer: densenet_conv3_block11_0_relu\n",
            "Renamed layer: densenet_conv3_block11_1_conv\n",
            "Renamed layer: densenet_conv3_block11_1_bn\n",
            "Renamed layer: densenet_conv3_block11_1_relu\n",
            "Renamed layer: densenet_conv3_block11_2_conv\n",
            "Renamed layer: densenet_conv3_block11_concat\n",
            "Renamed layer: densenet_conv3_block12_0_bn\n",
            "Renamed layer: densenet_conv3_block12_0_relu\n",
            "Renamed layer: densenet_conv3_block12_1_conv\n",
            "Renamed layer: densenet_conv3_block12_1_bn\n",
            "Renamed layer: densenet_conv3_block12_1_relu\n",
            "Renamed layer: densenet_conv3_block12_2_conv\n",
            "Renamed layer: densenet_conv3_block12_concat\n",
            "Renamed layer: densenet_pool3_bn\n",
            "Renamed layer: densenet_pool3_relu\n",
            "Renamed layer: densenet_pool3_conv\n",
            "Renamed layer: densenet_pool3_pool\n",
            "Renamed layer: densenet_conv4_block1_0_bn\n",
            "Renamed layer: densenet_conv4_block1_0_relu\n",
            "Renamed layer: densenet_conv4_block1_1_conv\n",
            "Renamed layer: densenet_conv4_block1_1_bn\n",
            "Renamed layer: densenet_conv4_block1_1_relu\n",
            "Renamed layer: densenet_conv4_block1_2_conv\n",
            "Renamed layer: densenet_conv4_block1_concat\n",
            "Renamed layer: densenet_conv4_block2_0_bn\n",
            "Renamed layer: densenet_conv4_block2_0_relu\n",
            "Renamed layer: densenet_conv4_block2_1_conv\n",
            "Renamed layer: densenet_conv4_block2_1_bn\n",
            "Renamed layer: densenet_conv4_block2_1_relu\n",
            "Renamed layer: densenet_conv4_block2_2_conv\n",
            "Renamed layer: densenet_conv4_block2_concat\n",
            "Renamed layer: densenet_conv4_block3_0_bn\n",
            "Renamed layer: densenet_conv4_block3_0_relu\n",
            "Renamed layer: densenet_conv4_block3_1_conv\n",
            "Renamed layer: densenet_conv4_block3_1_bn\n",
            "Renamed layer: densenet_conv4_block3_1_relu\n",
            "Renamed layer: densenet_conv4_block3_2_conv\n",
            "Renamed layer: densenet_conv4_block3_concat\n",
            "Renamed layer: densenet_conv4_block4_0_bn\n",
            "Renamed layer: densenet_conv4_block4_0_relu\n",
            "Renamed layer: densenet_conv4_block4_1_conv\n",
            "Renamed layer: densenet_conv4_block4_1_bn\n",
            "Renamed layer: densenet_conv4_block4_1_relu\n",
            "Renamed layer: densenet_conv4_block4_2_conv\n",
            "Renamed layer: densenet_conv4_block4_concat\n",
            "Renamed layer: densenet_conv4_block5_0_bn\n",
            "Renamed layer: densenet_conv4_block5_0_relu\n",
            "Renamed layer: densenet_conv4_block5_1_conv\n",
            "Renamed layer: densenet_conv4_block5_1_bn\n",
            "Renamed layer: densenet_conv4_block5_1_relu\n",
            "Renamed layer: densenet_conv4_block5_2_conv\n",
            "Renamed layer: densenet_conv4_block5_concat\n",
            "Renamed layer: densenet_conv4_block6_0_bn\n",
            "Renamed layer: densenet_conv4_block6_0_relu\n",
            "Renamed layer: densenet_conv4_block6_1_conv\n",
            "Renamed layer: densenet_conv4_block6_1_bn\n",
            "Renamed layer: densenet_conv4_block6_1_relu\n",
            "Renamed layer: densenet_conv4_block6_2_conv\n",
            "Renamed layer: densenet_conv4_block6_concat\n",
            "Renamed layer: densenet_conv4_block7_0_bn\n",
            "Renamed layer: densenet_conv4_block7_0_relu\n",
            "Renamed layer: densenet_conv4_block7_1_conv\n",
            "Renamed layer: densenet_conv4_block7_1_bn\n",
            "Renamed layer: densenet_conv4_block7_1_relu\n",
            "Renamed layer: densenet_conv4_block7_2_conv\n",
            "Renamed layer: densenet_conv4_block7_concat\n",
            "Renamed layer: densenet_conv4_block8_0_bn\n",
            "Renamed layer: densenet_conv4_block8_0_relu\n",
            "Renamed layer: densenet_conv4_block8_1_conv\n",
            "Renamed layer: densenet_conv4_block8_1_bn\n",
            "Renamed layer: densenet_conv4_block8_1_relu\n",
            "Renamed layer: densenet_conv4_block8_2_conv\n",
            "Renamed layer: densenet_conv4_block8_concat\n",
            "Renamed layer: densenet_conv4_block9_0_bn\n",
            "Renamed layer: densenet_conv4_block9_0_relu\n",
            "Renamed layer: densenet_conv4_block9_1_conv\n",
            "Renamed layer: densenet_conv4_block9_1_bn\n",
            "Renamed layer: densenet_conv4_block9_1_relu\n",
            "Renamed layer: densenet_conv4_block9_2_conv\n",
            "Renamed layer: densenet_conv4_block9_concat\n",
            "Renamed layer: densenet_conv4_block10_0_bn\n",
            "Renamed layer: densenet_conv4_block10_0_relu\n",
            "Renamed layer: densenet_conv4_block10_1_conv\n",
            "Renamed layer: densenet_conv4_block10_1_bn\n",
            "Renamed layer: densenet_conv4_block10_1_relu\n",
            "Renamed layer: densenet_conv4_block10_2_conv\n",
            "Renamed layer: densenet_conv4_block10_concat\n",
            "Renamed layer: densenet_conv4_block11_0_bn\n",
            "Renamed layer: densenet_conv4_block11_0_relu\n",
            "Renamed layer: densenet_conv4_block11_1_conv\n",
            "Renamed layer: densenet_conv4_block11_1_bn\n",
            "Renamed layer: densenet_conv4_block11_1_relu\n",
            "Renamed layer: densenet_conv4_block11_2_conv\n",
            "Renamed layer: densenet_conv4_block11_concat\n",
            "Renamed layer: densenet_conv4_block12_0_bn\n",
            "Renamed layer: densenet_conv4_block12_0_relu\n",
            "Renamed layer: densenet_conv4_block12_1_conv\n",
            "Renamed layer: densenet_conv4_block12_1_bn\n",
            "Renamed layer: densenet_conv4_block12_1_relu\n",
            "Renamed layer: densenet_conv4_block12_2_conv\n",
            "Renamed layer: densenet_conv4_block12_concat\n",
            "Renamed layer: densenet_conv4_block13_0_bn\n",
            "Renamed layer: densenet_conv4_block13_0_relu\n",
            "Renamed layer: densenet_conv4_block13_1_conv\n",
            "Renamed layer: densenet_conv4_block13_1_bn\n",
            "Renamed layer: densenet_conv4_block13_1_relu\n",
            "Renamed layer: densenet_conv4_block13_2_conv\n",
            "Renamed layer: densenet_conv4_block13_concat\n",
            "Renamed layer: densenet_conv4_block14_0_bn\n",
            "Renamed layer: densenet_conv4_block14_0_relu\n",
            "Renamed layer: densenet_conv4_block14_1_conv\n",
            "Renamed layer: densenet_conv4_block14_1_bn\n",
            "Renamed layer: densenet_conv4_block14_1_relu\n",
            "Renamed layer: densenet_conv4_block14_2_conv\n",
            "Renamed layer: densenet_conv4_block14_concat\n",
            "Renamed layer: densenet_conv4_block15_0_bn\n",
            "Renamed layer: densenet_conv4_block15_0_relu\n",
            "Renamed layer: densenet_conv4_block15_1_conv\n",
            "Renamed layer: densenet_conv4_block15_1_bn\n",
            "Renamed layer: densenet_conv4_block15_1_relu\n",
            "Renamed layer: densenet_conv4_block15_2_conv\n",
            "Renamed layer: densenet_conv4_block15_concat\n",
            "Renamed layer: densenet_conv4_block16_0_bn\n",
            "Renamed layer: densenet_conv4_block16_0_relu\n",
            "Renamed layer: densenet_conv4_block16_1_conv\n",
            "Renamed layer: densenet_conv4_block16_1_bn\n",
            "Renamed layer: densenet_conv4_block16_1_relu\n",
            "Renamed layer: densenet_conv4_block16_2_conv\n",
            "Renamed layer: densenet_conv4_block16_concat\n",
            "Renamed layer: densenet_conv4_block17_0_bn\n",
            "Renamed layer: densenet_conv4_block17_0_relu\n",
            "Renamed layer: densenet_conv4_block17_1_conv\n",
            "Renamed layer: densenet_conv4_block17_1_bn\n",
            "Renamed layer: densenet_conv4_block17_1_relu\n",
            "Renamed layer: densenet_conv4_block17_2_conv\n",
            "Renamed layer: densenet_conv4_block17_concat\n",
            "Renamed layer: densenet_conv4_block18_0_bn\n",
            "Renamed layer: densenet_conv4_block18_0_relu\n",
            "Renamed layer: densenet_conv4_block18_1_conv\n",
            "Renamed layer: densenet_conv4_block18_1_bn\n",
            "Renamed layer: densenet_conv4_block18_1_relu\n",
            "Renamed layer: densenet_conv4_block18_2_conv\n",
            "Renamed layer: densenet_conv4_block18_concat\n",
            "Renamed layer: densenet_conv4_block19_0_bn\n",
            "Renamed layer: densenet_conv4_block19_0_relu\n",
            "Renamed layer: densenet_conv4_block19_1_conv\n",
            "Renamed layer: densenet_conv4_block19_1_bn\n",
            "Renamed layer: densenet_conv4_block19_1_relu\n",
            "Renamed layer: densenet_conv4_block19_2_conv\n",
            "Renamed layer: densenet_conv4_block19_concat\n",
            "Renamed layer: densenet_conv4_block20_0_bn\n",
            "Renamed layer: densenet_conv4_block20_0_relu\n",
            "Renamed layer: densenet_conv4_block20_1_conv\n",
            "Renamed layer: densenet_conv4_block20_1_bn\n",
            "Renamed layer: densenet_conv4_block20_1_relu\n",
            "Renamed layer: densenet_conv4_block20_2_conv\n",
            "Renamed layer: densenet_conv4_block20_concat\n",
            "Renamed layer: densenet_conv4_block21_0_bn\n",
            "Renamed layer: densenet_conv4_block21_0_relu\n",
            "Renamed layer: densenet_conv4_block21_1_conv\n",
            "Renamed layer: densenet_conv4_block21_1_bn\n",
            "Renamed layer: densenet_conv4_block21_1_relu\n",
            "Renamed layer: densenet_conv4_block21_2_conv\n",
            "Renamed layer: densenet_conv4_block21_concat\n",
            "Renamed layer: densenet_conv4_block22_0_bn\n",
            "Renamed layer: densenet_conv4_block22_0_relu\n",
            "Renamed layer: densenet_conv4_block22_1_conv\n",
            "Renamed layer: densenet_conv4_block22_1_bn\n",
            "Renamed layer: densenet_conv4_block22_1_relu\n",
            "Renamed layer: densenet_conv4_block22_2_conv\n",
            "Renamed layer: densenet_conv4_block22_concat\n",
            "Renamed layer: densenet_conv4_block23_0_bn\n",
            "Renamed layer: densenet_conv4_block23_0_relu\n",
            "Renamed layer: densenet_conv4_block23_1_conv\n",
            "Renamed layer: densenet_conv4_block23_1_bn\n",
            "Renamed layer: densenet_conv4_block23_1_relu\n",
            "Renamed layer: densenet_conv4_block23_2_conv\n",
            "Renamed layer: densenet_conv4_block23_concat\n",
            "Renamed layer: densenet_conv4_block24_0_bn\n",
            "Renamed layer: densenet_conv4_block24_0_relu\n",
            "Renamed layer: densenet_conv4_block24_1_conv\n",
            "Renamed layer: densenet_conv4_block24_1_bn\n",
            "Renamed layer: densenet_conv4_block24_1_relu\n",
            "Renamed layer: densenet_conv4_block24_2_conv\n",
            "Renamed layer: densenet_conv4_block24_concat\n",
            "Renamed layer: densenet_pool4_bn\n",
            "Renamed layer: densenet_pool4_relu\n",
            "Renamed layer: densenet_pool4_conv\n",
            "Renamed layer: densenet_pool4_pool\n",
            "Renamed layer: densenet_conv5_block1_0_bn\n",
            "Renamed layer: densenet_conv5_block1_0_relu\n",
            "Renamed layer: densenet_conv5_block1_1_conv\n",
            "Renamed layer: densenet_conv5_block1_1_bn\n",
            "Renamed layer: densenet_conv5_block1_1_relu\n",
            "Renamed layer: densenet_conv5_block1_2_conv\n",
            "Renamed layer: densenet_conv5_block1_concat\n",
            "Renamed layer: densenet_conv5_block2_0_bn\n",
            "Renamed layer: densenet_conv5_block2_0_relu\n",
            "Renamed layer: densenet_conv5_block2_1_conv\n",
            "Renamed layer: densenet_conv5_block2_1_bn\n",
            "Renamed layer: densenet_conv5_block2_1_relu\n",
            "Renamed layer: densenet_conv5_block2_2_conv\n",
            "Renamed layer: densenet_conv5_block2_concat\n",
            "Renamed layer: densenet_conv5_block3_0_bn\n",
            "Renamed layer: densenet_conv5_block3_0_relu\n",
            "Renamed layer: densenet_conv5_block3_1_conv\n",
            "Renamed layer: densenet_conv5_block3_1_bn\n",
            "Renamed layer: densenet_conv5_block3_1_relu\n",
            "Renamed layer: densenet_conv5_block3_2_conv\n",
            "Renamed layer: densenet_conv5_block3_concat\n",
            "Renamed layer: densenet_conv5_block4_0_bn\n",
            "Renamed layer: densenet_conv5_block4_0_relu\n",
            "Renamed layer: densenet_conv5_block4_1_conv\n",
            "Renamed layer: densenet_conv5_block4_1_bn\n",
            "Renamed layer: densenet_conv5_block4_1_relu\n",
            "Renamed layer: densenet_conv5_block4_2_conv\n",
            "Renamed layer: densenet_conv5_block4_concat\n",
            "Renamed layer: densenet_conv5_block5_0_bn\n",
            "Renamed layer: densenet_conv5_block5_0_relu\n",
            "Renamed layer: densenet_conv5_block5_1_conv\n",
            "Renamed layer: densenet_conv5_block5_1_bn\n",
            "Renamed layer: densenet_conv5_block5_1_relu\n",
            "Renamed layer: densenet_conv5_block5_2_conv\n",
            "Renamed layer: densenet_conv5_block5_concat\n",
            "Renamed layer: densenet_conv5_block6_0_bn\n",
            "Renamed layer: densenet_conv5_block6_0_relu\n",
            "Renamed layer: densenet_conv5_block6_1_conv\n",
            "Renamed layer: densenet_conv5_block6_1_bn\n",
            "Renamed layer: densenet_conv5_block6_1_relu\n",
            "Renamed layer: densenet_conv5_block6_2_conv\n",
            "Renamed layer: densenet_conv5_block6_concat\n",
            "Renamed layer: densenet_conv5_block7_0_bn\n",
            "Renamed layer: densenet_conv5_block7_0_relu\n",
            "Renamed layer: densenet_conv5_block7_1_conv\n",
            "Renamed layer: densenet_conv5_block7_1_bn\n",
            "Renamed layer: densenet_conv5_block7_1_relu\n",
            "Renamed layer: densenet_conv5_block7_2_conv\n",
            "Renamed layer: densenet_conv5_block7_concat\n",
            "Renamed layer: densenet_conv5_block8_0_bn\n",
            "Renamed layer: densenet_conv5_block8_0_relu\n",
            "Renamed layer: densenet_conv5_block8_1_conv\n",
            "Renamed layer: densenet_conv5_block8_1_bn\n",
            "Renamed layer: densenet_conv5_block8_1_relu\n",
            "Renamed layer: densenet_conv5_block8_2_conv\n",
            "Renamed layer: densenet_conv5_block8_concat\n",
            "Renamed layer: densenet_conv5_block9_0_bn\n",
            "Renamed layer: densenet_conv5_block9_0_relu\n",
            "Renamed layer: densenet_conv5_block9_1_conv\n",
            "Renamed layer: densenet_conv5_block9_1_bn\n",
            "Renamed layer: densenet_conv5_block9_1_relu\n",
            "Renamed layer: densenet_conv5_block9_2_conv\n",
            "Renamed layer: densenet_conv5_block9_concat\n",
            "Renamed layer: densenet_conv5_block10_0_bn\n",
            "Renamed layer: densenet_conv5_block10_0_relu\n",
            "Renamed layer: densenet_conv5_block10_1_conv\n",
            "Renamed layer: densenet_conv5_block10_1_bn\n",
            "Renamed layer: densenet_conv5_block10_1_relu\n",
            "Renamed layer: densenet_conv5_block10_2_conv\n",
            "Renamed layer: densenet_conv5_block10_concat\n",
            "Renamed layer: densenet_conv5_block11_0_bn\n",
            "Renamed layer: densenet_conv5_block11_0_relu\n",
            "Renamed layer: densenet_conv5_block11_1_conv\n",
            "Renamed layer: densenet_conv5_block11_1_bn\n",
            "Renamed layer: densenet_conv5_block11_1_relu\n",
            "Renamed layer: densenet_conv5_block11_2_conv\n",
            "Renamed layer: densenet_conv5_block11_concat\n",
            "Renamed layer: densenet_conv5_block12_0_bn\n",
            "Renamed layer: densenet_conv5_block12_0_relu\n",
            "Renamed layer: densenet_conv5_block12_1_conv\n",
            "Renamed layer: densenet_conv5_block12_1_bn\n",
            "Renamed layer: densenet_conv5_block12_1_relu\n",
            "Renamed layer: densenet_conv5_block12_2_conv\n",
            "Renamed layer: densenet_conv5_block12_concat\n",
            "Renamed layer: densenet_conv5_block13_0_bn\n",
            "Renamed layer: densenet_conv5_block13_0_relu\n",
            "Renamed layer: densenet_conv5_block13_1_conv\n",
            "Renamed layer: densenet_conv5_block13_1_bn\n",
            "Renamed layer: densenet_conv5_block13_1_relu\n",
            "Renamed layer: densenet_conv5_block13_2_conv\n",
            "Renamed layer: densenet_conv5_block13_concat\n",
            "Renamed layer: densenet_conv5_block14_0_bn\n",
            "Renamed layer: densenet_conv5_block14_0_relu\n",
            "Renamed layer: densenet_conv5_block14_1_conv\n",
            "Renamed layer: densenet_conv5_block14_1_bn\n",
            "Renamed layer: densenet_conv5_block14_1_relu\n",
            "Renamed layer: densenet_conv5_block14_2_conv\n",
            "Renamed layer: densenet_conv5_block14_concat\n",
            "Renamed layer: densenet_conv5_block15_0_bn\n",
            "Renamed layer: densenet_conv5_block15_0_relu\n",
            "Renamed layer: densenet_conv5_block15_1_conv\n",
            "Renamed layer: densenet_conv5_block15_1_bn\n",
            "Renamed layer: densenet_conv5_block15_1_relu\n",
            "Renamed layer: densenet_conv5_block15_2_conv\n",
            "Renamed layer: densenet_conv5_block15_concat\n",
            "Renamed layer: densenet_conv5_block16_0_bn\n",
            "Renamed layer: densenet_conv5_block16_0_relu\n",
            "Renamed layer: densenet_conv5_block16_1_conv\n",
            "Renamed layer: densenet_conv5_block16_1_bn\n",
            "Renamed layer: densenet_conv5_block16_1_relu\n",
            "Renamed layer: densenet_conv5_block16_2_conv\n",
            "Renamed layer: densenet_conv5_block16_concat\n",
            "Renamed layer: densenet_bn\n",
            "Renamed layer: densenet_relu\n",
            "Renamed layer: densenet_global_average_pooling2d\n",
            "Renamed layer: densenet_dropout\n",
            "Renamed layer: densenet_dense\n",
            "Renamed layer: densenet_dropout_1\n",
            "Renamed layer: densenet_dense_1\n",
            "Cloned model name: densenet_model\n",
            "Debugging Cloned EfficientNet model...\n",
            "Model name: functional_6\n",
            "Layer name: input_layer_6\n",
            "Layer name: rescaling_12\n",
            "Layer name: normalization_6\n",
            "Layer name: rescaling_13\n",
            "Layer name: stem_conv_pad\n",
            "Layer name: stem_conv\n",
            "Layer name: stem_bn\n",
            "Layer name: stem_activation\n",
            "Layer name: block1a_dwconv\n",
            "Layer name: block1a_bn\n",
            "Layer name: block1a_activation\n",
            "Layer name: block1a_se_squeeze\n",
            "Layer name: block1a_se_reshape\n",
            "Layer name: block1a_se_reduce\n",
            "Layer name: block1a_se_expand\n",
            "Layer name: block1a_se_excite\n",
            "Layer name: block1a_project_conv\n",
            "Layer name: block1a_project_bn\n",
            "Layer name: block2a_expand_conv\n",
            "Layer name: block2a_expand_bn\n",
            "Layer name: block2a_expand_activation\n",
            "Layer name: block2a_dwconv_pad\n",
            "Layer name: block2a_dwconv\n",
            "Layer name: block2a_bn\n",
            "Layer name: block2a_activation\n",
            "Layer name: block2a_se_squeeze\n",
            "Layer name: block2a_se_reshape\n",
            "Layer name: block2a_se_reduce\n",
            "Layer name: block2a_se_expand\n",
            "Layer name: block2a_se_excite\n",
            "Layer name: block2a_project_conv\n",
            "Layer name: block2a_project_bn\n",
            "Layer name: block2b_expand_conv\n",
            "Layer name: block2b_expand_bn\n",
            "Layer name: block2b_expand_activation\n",
            "Layer name: block2b_dwconv\n",
            "Layer name: block2b_bn\n",
            "Layer name: block2b_activation\n",
            "Layer name: block2b_se_squeeze\n",
            "Layer name: block2b_se_reshape\n",
            "Layer name: block2b_se_reduce\n",
            "Layer name: block2b_se_expand\n",
            "Layer name: block2b_se_excite\n",
            "Layer name: block2b_project_conv\n",
            "Layer name: block2b_project_bn\n",
            "Layer name: block2b_drop\n",
            "Layer name: block2b_add\n",
            "Layer name: block3a_expand_conv\n",
            "Layer name: block3a_expand_bn\n",
            "Layer name: block3a_expand_activation\n",
            "Layer name: block3a_dwconv_pad\n",
            "Layer name: block3a_dwconv\n",
            "Layer name: block3a_bn\n",
            "Layer name: block3a_activation\n",
            "Layer name: block3a_se_squeeze\n",
            "Layer name: block3a_se_reshape\n",
            "Layer name: block3a_se_reduce\n",
            "Layer name: block3a_se_expand\n",
            "Layer name: block3a_se_excite\n",
            "Layer name: block3a_project_conv\n",
            "Layer name: block3a_project_bn\n",
            "Layer name: block3b_expand_conv\n",
            "Layer name: block3b_expand_bn\n",
            "Layer name: block3b_expand_activation\n",
            "Layer name: block3b_dwconv\n",
            "Layer name: block3b_bn\n",
            "Layer name: block3b_activation\n",
            "Layer name: block3b_se_squeeze\n",
            "Layer name: block3b_se_reshape\n",
            "Layer name: block3b_se_reduce\n",
            "Layer name: block3b_se_expand\n",
            "Layer name: block3b_se_excite\n",
            "Layer name: block3b_project_conv\n",
            "Layer name: block3b_project_bn\n",
            "Layer name: block3b_drop\n",
            "Layer name: block3b_add\n",
            "Layer name: block4a_expand_conv\n",
            "Layer name: block4a_expand_bn\n",
            "Layer name: block4a_expand_activation\n",
            "Layer name: block4a_dwconv_pad\n",
            "Layer name: block4a_dwconv\n",
            "Layer name: block4a_bn\n",
            "Layer name: block4a_activation\n",
            "Layer name: block4a_se_squeeze\n",
            "Layer name: block4a_se_reshape\n",
            "Layer name: block4a_se_reduce\n",
            "Layer name: block4a_se_expand\n",
            "Layer name: block4a_se_excite\n",
            "Layer name: block4a_project_conv\n",
            "Layer name: block4a_project_bn\n",
            "Layer name: block4b_expand_conv\n",
            "Layer name: block4b_expand_bn\n",
            "Layer name: block4b_expand_activation\n",
            "Layer name: block4b_dwconv\n",
            "Layer name: block4b_bn\n",
            "Layer name: block4b_activation\n",
            "Layer name: block4b_se_squeeze\n",
            "Layer name: block4b_se_reshape\n",
            "Layer name: block4b_se_reduce\n",
            "Layer name: block4b_se_expand\n",
            "Layer name: block4b_se_excite\n",
            "Layer name: block4b_project_conv\n",
            "Layer name: block4b_project_bn\n",
            "Layer name: block4b_drop\n",
            "Layer name: block4b_add\n",
            "Layer name: block4c_expand_conv\n",
            "Layer name: block4c_expand_bn\n",
            "Layer name: block4c_expand_activation\n",
            "Layer name: block4c_dwconv\n",
            "Layer name: block4c_bn\n",
            "Layer name: block4c_activation\n",
            "Layer name: block4c_se_squeeze\n",
            "Layer name: block4c_se_reshape\n",
            "Layer name: block4c_se_reduce\n",
            "Layer name: block4c_se_expand\n",
            "Layer name: block4c_se_excite\n",
            "Layer name: block4c_project_conv\n",
            "Layer name: block4c_project_bn\n",
            "Layer name: block4c_drop\n",
            "Layer name: block4c_add\n",
            "Layer name: block5a_expand_conv\n",
            "Layer name: block5a_expand_bn\n",
            "Layer name: block5a_expand_activation\n",
            "Layer name: block5a_dwconv\n",
            "Layer name: block5a_bn\n",
            "Layer name: block5a_activation\n",
            "Layer name: block5a_se_squeeze\n",
            "Layer name: block5a_se_reshape\n",
            "Layer name: block5a_se_reduce\n",
            "Layer name: block5a_se_expand\n",
            "Layer name: block5a_se_excite\n",
            "Layer name: block5a_project_conv\n",
            "Layer name: block5a_project_bn\n",
            "Layer name: block5b_expand_conv\n",
            "Layer name: block5b_expand_bn\n",
            "Layer name: block5b_expand_activation\n",
            "Layer name: block5b_dwconv\n",
            "Layer name: block5b_bn\n",
            "Layer name: block5b_activation\n",
            "Layer name: block5b_se_squeeze\n",
            "Layer name: block5b_se_reshape\n",
            "Layer name: block5b_se_reduce\n",
            "Layer name: block5b_se_expand\n",
            "Layer name: block5b_se_excite\n",
            "Layer name: block5b_project_conv\n",
            "Layer name: block5b_project_bn\n",
            "Layer name: block5b_drop\n",
            "Layer name: block5b_add\n",
            "Layer name: block5c_expand_conv\n",
            "Layer name: block5c_expand_bn\n",
            "Layer name: block5c_expand_activation\n",
            "Layer name: block5c_dwconv\n",
            "Layer name: block5c_bn\n",
            "Layer name: block5c_activation\n",
            "Layer name: block5c_se_squeeze\n",
            "Layer name: block5c_se_reshape\n",
            "Layer name: block5c_se_reduce\n",
            "Layer name: block5c_se_expand\n",
            "Layer name: block5c_se_excite\n",
            "Layer name: block5c_project_conv\n",
            "Layer name: block5c_project_bn\n",
            "Layer name: block5c_drop\n",
            "Layer name: block5c_add\n",
            "Layer name: block6a_expand_conv\n",
            "Layer name: block6a_expand_bn\n",
            "Layer name: block6a_expand_activation\n",
            "Layer name: block6a_dwconv_pad\n",
            "Layer name: block6a_dwconv\n",
            "Layer name: block6a_bn\n",
            "Layer name: block6a_activation\n",
            "Layer name: block6a_se_squeeze\n",
            "Layer name: block6a_se_reshape\n",
            "Layer name: block6a_se_reduce\n",
            "Layer name: block6a_se_expand\n",
            "Layer name: block6a_se_excite\n",
            "Layer name: block6a_project_conv\n",
            "Layer name: block6a_project_bn\n",
            "Layer name: block6b_expand_conv\n",
            "Layer name: block6b_expand_bn\n",
            "Layer name: block6b_expand_activation\n",
            "Layer name: block6b_dwconv\n",
            "Layer name: block6b_bn\n",
            "Layer name: block6b_activation\n",
            "Layer name: block6b_se_squeeze\n",
            "Layer name: block6b_se_reshape\n",
            "Layer name: block6b_se_reduce\n",
            "Layer name: block6b_se_expand\n",
            "Layer name: block6b_se_excite\n",
            "Layer name: block6b_project_conv\n",
            "Layer name: block6b_project_bn\n",
            "Layer name: block6b_drop\n",
            "Layer name: block6b_add\n",
            "Layer name: block6c_expand_conv\n",
            "Layer name: block6c_expand_bn\n",
            "Layer name: block6c_expand_activation\n",
            "Layer name: block6c_dwconv\n",
            "Layer name: block6c_bn\n",
            "Layer name: block6c_activation\n",
            "Layer name: block6c_se_squeeze\n",
            "Layer name: block6c_se_reshape\n",
            "Layer name: block6c_se_reduce\n",
            "Layer name: block6c_se_expand\n",
            "Layer name: block6c_se_excite\n",
            "Layer name: block6c_project_conv\n",
            "Layer name: block6c_project_bn\n",
            "Layer name: block6c_drop\n",
            "Layer name: block6c_add\n",
            "Layer name: block6d_expand_conv\n",
            "Layer name: block6d_expand_bn\n",
            "Layer name: block6d_expand_activation\n",
            "Layer name: block6d_dwconv\n",
            "Layer name: block6d_bn\n",
            "Layer name: block6d_activation\n",
            "Layer name: block6d_se_squeeze\n",
            "Layer name: block6d_se_reshape\n",
            "Layer name: block6d_se_reduce\n",
            "Layer name: block6d_se_expand\n",
            "Layer name: block6d_se_excite\n",
            "Layer name: block6d_project_conv\n",
            "Layer name: block6d_project_bn\n",
            "Layer name: block6d_drop\n",
            "Layer name: block6d_add\n",
            "Layer name: block7a_expand_conv\n",
            "Layer name: block7a_expand_bn\n",
            "Layer name: block7a_expand_activation\n",
            "Layer name: block7a_dwconv\n",
            "Layer name: block7a_bn\n",
            "Layer name: block7a_activation\n",
            "Layer name: block7a_se_squeeze\n",
            "Layer name: block7a_se_reshape\n",
            "Layer name: block7a_se_reduce\n",
            "Layer name: block7a_se_expand\n",
            "Layer name: block7a_se_excite\n",
            "Layer name: block7a_project_conv\n",
            "Layer name: block7a_project_bn\n",
            "Layer name: top_conv\n",
            "Layer name: top_bn\n",
            "Layer name: top_activation\n",
            "Layer name: global_average_pooling2d_6\n",
            "Layer name: dense_12\n",
            "Layer name: dropout_6\n",
            "Layer name: dense_13\n",
            "Total layers in Cloned EfficientNet model: 242\n",
            "\n",
            "Debugging Cloned ResNet model...\n",
            "Model name: functional\n",
            "Layer name: input_layer\n",
            "Layer name: conv1_pad\n",
            "Layer name: conv1_conv\n",
            "Layer name: conv1_bn\n",
            "Layer name: conv1_relu\n",
            "Layer name: pool1_pad\n",
            "Layer name: pool1_pool\n",
            "Layer name: conv2_block1_1_conv\n",
            "Layer name: conv2_block1_1_bn\n",
            "Layer name: conv2_block1_1_relu\n",
            "Layer name: conv2_block1_2_conv\n",
            "Layer name: conv2_block1_2_bn\n",
            "Layer name: conv2_block1_2_relu\n",
            "Layer name: conv2_block1_0_conv\n",
            "Layer name: conv2_block1_3_conv\n",
            "Layer name: conv2_block1_0_bn\n",
            "Layer name: conv2_block1_3_bn\n",
            "Layer name: conv2_block1_add\n",
            "Layer name: conv2_block1_out\n",
            "Layer name: conv2_block2_1_conv\n",
            "Layer name: conv2_block2_1_bn\n",
            "Layer name: conv2_block2_1_relu\n",
            "Layer name: conv2_block2_2_conv\n",
            "Layer name: conv2_block2_2_bn\n",
            "Layer name: conv2_block2_2_relu\n",
            "Layer name: conv2_block2_3_conv\n",
            "Layer name: conv2_block2_3_bn\n",
            "Layer name: conv2_block2_add\n",
            "Layer name: conv2_block2_out\n",
            "Layer name: conv2_block3_1_conv\n",
            "Layer name: conv2_block3_1_bn\n",
            "Layer name: conv2_block3_1_relu\n",
            "Layer name: conv2_block3_2_conv\n",
            "Layer name: conv2_block3_2_bn\n",
            "Layer name: conv2_block3_2_relu\n",
            "Layer name: conv2_block3_3_conv\n",
            "Layer name: conv2_block3_3_bn\n",
            "Layer name: conv2_block3_add\n",
            "Layer name: conv2_block3_out\n",
            "Layer name: conv3_block1_1_conv\n",
            "Layer name: conv3_block1_1_bn\n",
            "Layer name: conv3_block1_1_relu\n",
            "Layer name: conv3_block1_2_conv\n",
            "Layer name: conv3_block1_2_bn\n",
            "Layer name: conv3_block1_2_relu\n",
            "Layer name: conv3_block1_0_conv\n",
            "Layer name: conv3_block1_3_conv\n",
            "Layer name: conv3_block1_0_bn\n",
            "Layer name: conv3_block1_3_bn\n",
            "Layer name: conv3_block1_add\n",
            "Layer name: conv3_block1_out\n",
            "Layer name: conv3_block2_1_conv\n",
            "Layer name: conv3_block2_1_bn\n",
            "Layer name: conv3_block2_1_relu\n",
            "Layer name: conv3_block2_2_conv\n",
            "Layer name: conv3_block2_2_bn\n",
            "Layer name: conv3_block2_2_relu\n",
            "Layer name: conv3_block2_3_conv\n",
            "Layer name: conv3_block2_3_bn\n",
            "Layer name: conv3_block2_add\n",
            "Layer name: conv3_block2_out\n",
            "Layer name: conv3_block3_1_conv\n",
            "Layer name: conv3_block3_1_bn\n",
            "Layer name: conv3_block3_1_relu\n",
            "Layer name: conv3_block3_2_conv\n",
            "Layer name: conv3_block3_2_bn\n",
            "Layer name: conv3_block3_2_relu\n",
            "Layer name: conv3_block3_3_conv\n",
            "Layer name: conv3_block3_3_bn\n",
            "Layer name: conv3_block3_add\n",
            "Layer name: conv3_block3_out\n",
            "Layer name: conv3_block4_1_conv\n",
            "Layer name: conv3_block4_1_bn\n",
            "Layer name: conv3_block4_1_relu\n",
            "Layer name: conv3_block4_2_conv\n",
            "Layer name: conv3_block4_2_bn\n",
            "Layer name: conv3_block4_2_relu\n",
            "Layer name: conv3_block4_3_conv\n",
            "Layer name: conv3_block4_3_bn\n",
            "Layer name: conv3_block4_add\n",
            "Layer name: conv3_block4_out\n",
            "Layer name: conv4_block1_1_conv\n",
            "Layer name: conv4_block1_1_bn\n",
            "Layer name: conv4_block1_1_relu\n",
            "Layer name: conv4_block1_2_conv\n",
            "Layer name: conv4_block1_2_bn\n",
            "Layer name: conv4_block1_2_relu\n",
            "Layer name: conv4_block1_0_conv\n",
            "Layer name: conv4_block1_3_conv\n",
            "Layer name: conv4_block1_0_bn\n",
            "Layer name: conv4_block1_3_bn\n",
            "Layer name: conv4_block1_add\n",
            "Layer name: conv4_block1_out\n",
            "Layer name: conv4_block2_1_conv\n",
            "Layer name: conv4_block2_1_bn\n",
            "Layer name: conv4_block2_1_relu\n",
            "Layer name: conv4_block2_2_conv\n",
            "Layer name: conv4_block2_2_bn\n",
            "Layer name: conv4_block2_2_relu\n",
            "Layer name: conv4_block2_3_conv\n",
            "Layer name: conv4_block2_3_bn\n",
            "Layer name: conv4_block2_add\n",
            "Layer name: conv4_block2_out\n",
            "Layer name: conv4_block3_1_conv\n",
            "Layer name: conv4_block3_1_bn\n",
            "Layer name: conv4_block3_1_relu\n",
            "Layer name: conv4_block3_2_conv\n",
            "Layer name: conv4_block3_2_bn\n",
            "Layer name: conv4_block3_2_relu\n",
            "Layer name: conv4_block3_3_conv\n",
            "Layer name: conv4_block3_3_bn\n",
            "Layer name: conv4_block3_add\n",
            "Layer name: conv4_block3_out\n",
            "Layer name: conv4_block4_1_conv\n",
            "Layer name: conv4_block4_1_bn\n",
            "Layer name: conv4_block4_1_relu\n",
            "Layer name: conv4_block4_2_conv\n",
            "Layer name: conv4_block4_2_bn\n",
            "Layer name: conv4_block4_2_relu\n",
            "Layer name: conv4_block4_3_conv\n",
            "Layer name: conv4_block4_3_bn\n",
            "Layer name: conv4_block4_add\n",
            "Layer name: conv4_block4_out\n",
            "Layer name: conv4_block5_1_conv\n",
            "Layer name: conv4_block5_1_bn\n",
            "Layer name: conv4_block5_1_relu\n",
            "Layer name: conv4_block5_2_conv\n",
            "Layer name: conv4_block5_2_bn\n",
            "Layer name: conv4_block5_2_relu\n",
            "Layer name: conv4_block5_3_conv\n",
            "Layer name: conv4_block5_3_bn\n",
            "Layer name: conv4_block5_add\n",
            "Layer name: conv4_block5_out\n",
            "Layer name: conv4_block6_1_conv\n",
            "Layer name: conv4_block6_1_bn\n",
            "Layer name: conv4_block6_1_relu\n",
            "Layer name: conv4_block6_2_conv\n",
            "Layer name: conv4_block6_2_bn\n",
            "Layer name: conv4_block6_2_relu\n",
            "Layer name: conv4_block6_3_conv\n",
            "Layer name: conv4_block6_3_bn\n",
            "Layer name: conv4_block6_add\n",
            "Layer name: conv4_block6_out\n",
            "Layer name: conv5_block1_1_conv\n",
            "Layer name: conv5_block1_1_bn\n",
            "Layer name: conv5_block1_1_relu\n",
            "Layer name: conv5_block1_2_conv\n",
            "Layer name: conv5_block1_2_bn\n",
            "Layer name: conv5_block1_2_relu\n",
            "Layer name: conv5_block1_0_conv\n",
            "Layer name: conv5_block1_3_conv\n",
            "Layer name: conv5_block1_0_bn\n",
            "Layer name: conv5_block1_3_bn\n",
            "Layer name: conv5_block1_add\n",
            "Layer name: conv5_block1_out\n",
            "Layer name: conv5_block2_1_conv\n",
            "Layer name: conv5_block2_1_bn\n",
            "Layer name: conv5_block2_1_relu\n",
            "Layer name: conv5_block2_2_conv\n",
            "Layer name: conv5_block2_2_bn\n",
            "Layer name: conv5_block2_2_relu\n",
            "Layer name: conv5_block2_3_conv\n",
            "Layer name: conv5_block2_3_bn\n",
            "Layer name: conv5_block2_add\n",
            "Layer name: conv5_block2_out\n",
            "Layer name: conv5_block3_1_conv\n",
            "Layer name: conv5_block3_1_bn\n",
            "Layer name: conv5_block3_1_relu\n",
            "Layer name: conv5_block3_2_conv\n",
            "Layer name: conv5_block3_2_bn\n",
            "Layer name: conv5_block3_2_relu\n",
            "Layer name: conv5_block3_3_conv\n",
            "Layer name: conv5_block3_3_bn\n",
            "Layer name: conv5_block3_add\n",
            "Layer name: conv5_block3_out\n",
            "Layer name: global_average_pooling2d\n",
            "Layer name: dense\n",
            "Layer name: dropout\n",
            "Layer name: dense_1\n",
            "Total layers in Cloned ResNet model: 179\n",
            "\n",
            "Debugging Cloned DenseNet model...\n",
            "Model name: functional\n",
            "Layer name: input_layer\n",
            "Layer name: zero_padding2d\n",
            "Layer name: conv1_conv\n",
            "Layer name: conv1_bn\n",
            "Layer name: conv1_relu\n",
            "Layer name: zero_padding2d_1\n",
            "Layer name: pool1\n",
            "Layer name: conv2_block1_0_bn\n",
            "Layer name: conv2_block1_0_relu\n",
            "Layer name: conv2_block1_1_conv\n",
            "Layer name: conv2_block1_1_bn\n",
            "Layer name: conv2_block1_1_relu\n",
            "Layer name: conv2_block1_2_conv\n",
            "Layer name: conv2_block1_concat\n",
            "Layer name: conv2_block2_0_bn\n",
            "Layer name: conv2_block2_0_relu\n",
            "Layer name: conv2_block2_1_conv\n",
            "Layer name: conv2_block2_1_bn\n",
            "Layer name: conv2_block2_1_relu\n",
            "Layer name: conv2_block2_2_conv\n",
            "Layer name: conv2_block2_concat\n",
            "Layer name: conv2_block3_0_bn\n",
            "Layer name: conv2_block3_0_relu\n",
            "Layer name: conv2_block3_1_conv\n",
            "Layer name: conv2_block3_1_bn\n",
            "Layer name: conv2_block3_1_relu\n",
            "Layer name: conv2_block3_2_conv\n",
            "Layer name: conv2_block3_concat\n",
            "Layer name: conv2_block4_0_bn\n",
            "Layer name: conv2_block4_0_relu\n",
            "Layer name: conv2_block4_1_conv\n",
            "Layer name: conv2_block4_1_bn\n",
            "Layer name: conv2_block4_1_relu\n",
            "Layer name: conv2_block4_2_conv\n",
            "Layer name: conv2_block4_concat\n",
            "Layer name: conv2_block5_0_bn\n",
            "Layer name: conv2_block5_0_relu\n",
            "Layer name: conv2_block5_1_conv\n",
            "Layer name: conv2_block5_1_bn\n",
            "Layer name: conv2_block5_1_relu\n",
            "Layer name: conv2_block5_2_conv\n",
            "Layer name: conv2_block5_concat\n",
            "Layer name: conv2_block6_0_bn\n",
            "Layer name: conv2_block6_0_relu\n",
            "Layer name: conv2_block6_1_conv\n",
            "Layer name: conv2_block6_1_bn\n",
            "Layer name: conv2_block6_1_relu\n",
            "Layer name: conv2_block6_2_conv\n",
            "Layer name: conv2_block6_concat\n",
            "Layer name: pool2_bn\n",
            "Layer name: pool2_relu\n",
            "Layer name: pool2_conv\n",
            "Layer name: pool2_pool\n",
            "Layer name: conv3_block1_0_bn\n",
            "Layer name: conv3_block1_0_relu\n",
            "Layer name: conv3_block1_1_conv\n",
            "Layer name: conv3_block1_1_bn\n",
            "Layer name: conv3_block1_1_relu\n",
            "Layer name: conv3_block1_2_conv\n",
            "Layer name: conv3_block1_concat\n",
            "Layer name: conv3_block2_0_bn\n",
            "Layer name: conv3_block2_0_relu\n",
            "Layer name: conv3_block2_1_conv\n",
            "Layer name: conv3_block2_1_bn\n",
            "Layer name: conv3_block2_1_relu\n",
            "Layer name: conv3_block2_2_conv\n",
            "Layer name: conv3_block2_concat\n",
            "Layer name: conv3_block3_0_bn\n",
            "Layer name: conv3_block3_0_relu\n",
            "Layer name: conv3_block3_1_conv\n",
            "Layer name: conv3_block3_1_bn\n",
            "Layer name: conv3_block3_1_relu\n",
            "Layer name: conv3_block3_2_conv\n",
            "Layer name: conv3_block3_concat\n",
            "Layer name: conv3_block4_0_bn\n",
            "Layer name: conv3_block4_0_relu\n",
            "Layer name: conv3_block4_1_conv\n",
            "Layer name: conv3_block4_1_bn\n",
            "Layer name: conv3_block4_1_relu\n",
            "Layer name: conv3_block4_2_conv\n",
            "Layer name: conv3_block4_concat\n",
            "Layer name: conv3_block5_0_bn\n",
            "Layer name: conv3_block5_0_relu\n",
            "Layer name: conv3_block5_1_conv\n",
            "Layer name: conv3_block5_1_bn\n",
            "Layer name: conv3_block5_1_relu\n",
            "Layer name: conv3_block5_2_conv\n",
            "Layer name: conv3_block5_concat\n",
            "Layer name: conv3_block6_0_bn\n",
            "Layer name: conv3_block6_0_relu\n",
            "Layer name: conv3_block6_1_conv\n",
            "Layer name: conv3_block6_1_bn\n",
            "Layer name: conv3_block6_1_relu\n",
            "Layer name: conv3_block6_2_conv\n",
            "Layer name: conv3_block6_concat\n",
            "Layer name: conv3_block7_0_bn\n",
            "Layer name: conv3_block7_0_relu\n",
            "Layer name: conv3_block7_1_conv\n",
            "Layer name: conv3_block7_1_bn\n",
            "Layer name: conv3_block7_1_relu\n",
            "Layer name: conv3_block7_2_conv\n",
            "Layer name: conv3_block7_concat\n",
            "Layer name: conv3_block8_0_bn\n",
            "Layer name: conv3_block8_0_relu\n",
            "Layer name: conv3_block8_1_conv\n",
            "Layer name: conv3_block8_1_bn\n",
            "Layer name: conv3_block8_1_relu\n",
            "Layer name: conv3_block8_2_conv\n",
            "Layer name: conv3_block8_concat\n",
            "Layer name: conv3_block9_0_bn\n",
            "Layer name: conv3_block9_0_relu\n",
            "Layer name: conv3_block9_1_conv\n",
            "Layer name: conv3_block9_1_bn\n",
            "Layer name: conv3_block9_1_relu\n",
            "Layer name: conv3_block9_2_conv\n",
            "Layer name: conv3_block9_concat\n",
            "Layer name: conv3_block10_0_bn\n",
            "Layer name: conv3_block10_0_relu\n",
            "Layer name: conv3_block10_1_conv\n",
            "Layer name: conv3_block10_1_bn\n",
            "Layer name: conv3_block10_1_relu\n",
            "Layer name: conv3_block10_2_conv\n",
            "Layer name: conv3_block10_concat\n",
            "Layer name: conv3_block11_0_bn\n",
            "Layer name: conv3_block11_0_relu\n",
            "Layer name: conv3_block11_1_conv\n",
            "Layer name: conv3_block11_1_bn\n",
            "Layer name: conv3_block11_1_relu\n",
            "Layer name: conv3_block11_2_conv\n",
            "Layer name: conv3_block11_concat\n",
            "Layer name: conv3_block12_0_bn\n",
            "Layer name: conv3_block12_0_relu\n",
            "Layer name: conv3_block12_1_conv\n",
            "Layer name: conv3_block12_1_bn\n",
            "Layer name: conv3_block12_1_relu\n",
            "Layer name: conv3_block12_2_conv\n",
            "Layer name: conv3_block12_concat\n",
            "Layer name: pool3_bn\n",
            "Layer name: pool3_relu\n",
            "Layer name: pool3_conv\n",
            "Layer name: pool3_pool\n",
            "Layer name: conv4_block1_0_bn\n",
            "Layer name: conv4_block1_0_relu\n",
            "Layer name: conv4_block1_1_conv\n",
            "Layer name: conv4_block1_1_bn\n",
            "Layer name: conv4_block1_1_relu\n",
            "Layer name: conv4_block1_2_conv\n",
            "Layer name: conv4_block1_concat\n",
            "Layer name: conv4_block2_0_bn\n",
            "Layer name: conv4_block2_0_relu\n",
            "Layer name: conv4_block2_1_conv\n",
            "Layer name: conv4_block2_1_bn\n",
            "Layer name: conv4_block2_1_relu\n",
            "Layer name: conv4_block2_2_conv\n",
            "Layer name: conv4_block2_concat\n",
            "Layer name: conv4_block3_0_bn\n",
            "Layer name: conv4_block3_0_relu\n",
            "Layer name: conv4_block3_1_conv\n",
            "Layer name: conv4_block3_1_bn\n",
            "Layer name: conv4_block3_1_relu\n",
            "Layer name: conv4_block3_2_conv\n",
            "Layer name: conv4_block3_concat\n",
            "Layer name: conv4_block4_0_bn\n",
            "Layer name: conv4_block4_0_relu\n",
            "Layer name: conv4_block4_1_conv\n",
            "Layer name: conv4_block4_1_bn\n",
            "Layer name: conv4_block4_1_relu\n",
            "Layer name: conv4_block4_2_conv\n",
            "Layer name: conv4_block4_concat\n",
            "Layer name: conv4_block5_0_bn\n",
            "Layer name: conv4_block5_0_relu\n",
            "Layer name: conv4_block5_1_conv\n",
            "Layer name: conv4_block5_1_bn\n",
            "Layer name: conv4_block5_1_relu\n",
            "Layer name: conv4_block5_2_conv\n",
            "Layer name: conv4_block5_concat\n",
            "Layer name: conv4_block6_0_bn\n",
            "Layer name: conv4_block6_0_relu\n",
            "Layer name: conv4_block6_1_conv\n",
            "Layer name: conv4_block6_1_bn\n",
            "Layer name: conv4_block6_1_relu\n",
            "Layer name: conv4_block6_2_conv\n",
            "Layer name: conv4_block6_concat\n",
            "Layer name: conv4_block7_0_bn\n",
            "Layer name: conv4_block7_0_relu\n",
            "Layer name: conv4_block7_1_conv\n",
            "Layer name: conv4_block7_1_bn\n",
            "Layer name: conv4_block7_1_relu\n",
            "Layer name: conv4_block7_2_conv\n",
            "Layer name: conv4_block7_concat\n",
            "Layer name: conv4_block8_0_bn\n",
            "Layer name: conv4_block8_0_relu\n",
            "Layer name: conv4_block8_1_conv\n",
            "Layer name: conv4_block8_1_bn\n",
            "Layer name: conv4_block8_1_relu\n",
            "Layer name: conv4_block8_2_conv\n",
            "Layer name: conv4_block8_concat\n",
            "Layer name: conv4_block9_0_bn\n",
            "Layer name: conv4_block9_0_relu\n",
            "Layer name: conv4_block9_1_conv\n",
            "Layer name: conv4_block9_1_bn\n",
            "Layer name: conv4_block9_1_relu\n",
            "Layer name: conv4_block9_2_conv\n",
            "Layer name: conv4_block9_concat\n",
            "Layer name: conv4_block10_0_bn\n",
            "Layer name: conv4_block10_0_relu\n",
            "Layer name: conv4_block10_1_conv\n",
            "Layer name: conv4_block10_1_bn\n",
            "Layer name: conv4_block10_1_relu\n",
            "Layer name: conv4_block10_2_conv\n",
            "Layer name: conv4_block10_concat\n",
            "Layer name: conv4_block11_0_bn\n",
            "Layer name: conv4_block11_0_relu\n",
            "Layer name: conv4_block11_1_conv\n",
            "Layer name: conv4_block11_1_bn\n",
            "Layer name: conv4_block11_1_relu\n",
            "Layer name: conv4_block11_2_conv\n",
            "Layer name: conv4_block11_concat\n",
            "Layer name: conv4_block12_0_bn\n",
            "Layer name: conv4_block12_0_relu\n",
            "Layer name: conv4_block12_1_conv\n",
            "Layer name: conv4_block12_1_bn\n",
            "Layer name: conv4_block12_1_relu\n",
            "Layer name: conv4_block12_2_conv\n",
            "Layer name: conv4_block12_concat\n",
            "Layer name: conv4_block13_0_bn\n",
            "Layer name: conv4_block13_0_relu\n",
            "Layer name: conv4_block13_1_conv\n",
            "Layer name: conv4_block13_1_bn\n",
            "Layer name: conv4_block13_1_relu\n",
            "Layer name: conv4_block13_2_conv\n",
            "Layer name: conv4_block13_concat\n",
            "Layer name: conv4_block14_0_bn\n",
            "Layer name: conv4_block14_0_relu\n",
            "Layer name: conv4_block14_1_conv\n",
            "Layer name: conv4_block14_1_bn\n",
            "Layer name: conv4_block14_1_relu\n",
            "Layer name: conv4_block14_2_conv\n",
            "Layer name: conv4_block14_concat\n",
            "Layer name: conv4_block15_0_bn\n",
            "Layer name: conv4_block15_0_relu\n",
            "Layer name: conv4_block15_1_conv\n",
            "Layer name: conv4_block15_1_bn\n",
            "Layer name: conv4_block15_1_relu\n",
            "Layer name: conv4_block15_2_conv\n",
            "Layer name: conv4_block15_concat\n",
            "Layer name: conv4_block16_0_bn\n",
            "Layer name: conv4_block16_0_relu\n",
            "Layer name: conv4_block16_1_conv\n",
            "Layer name: conv4_block16_1_bn\n",
            "Layer name: conv4_block16_1_relu\n",
            "Layer name: conv4_block16_2_conv\n",
            "Layer name: conv4_block16_concat\n",
            "Layer name: conv4_block17_0_bn\n",
            "Layer name: conv4_block17_0_relu\n",
            "Layer name: conv4_block17_1_conv\n",
            "Layer name: conv4_block17_1_bn\n",
            "Layer name: conv4_block17_1_relu\n",
            "Layer name: conv4_block17_2_conv\n",
            "Layer name: conv4_block17_concat\n",
            "Layer name: conv4_block18_0_bn\n",
            "Layer name: conv4_block18_0_relu\n",
            "Layer name: conv4_block18_1_conv\n",
            "Layer name: conv4_block18_1_bn\n",
            "Layer name: conv4_block18_1_relu\n",
            "Layer name: conv4_block18_2_conv\n",
            "Layer name: conv4_block18_concat\n",
            "Layer name: conv4_block19_0_bn\n",
            "Layer name: conv4_block19_0_relu\n",
            "Layer name: conv4_block19_1_conv\n",
            "Layer name: conv4_block19_1_bn\n",
            "Layer name: conv4_block19_1_relu\n",
            "Layer name: conv4_block19_2_conv\n",
            "Layer name: conv4_block19_concat\n",
            "Layer name: conv4_block20_0_bn\n",
            "Layer name: conv4_block20_0_relu\n",
            "Layer name: conv4_block20_1_conv\n",
            "Layer name: conv4_block20_1_bn\n",
            "Layer name: conv4_block20_1_relu\n",
            "Layer name: conv4_block20_2_conv\n",
            "Layer name: conv4_block20_concat\n",
            "Layer name: conv4_block21_0_bn\n",
            "Layer name: conv4_block21_0_relu\n",
            "Layer name: conv4_block21_1_conv\n",
            "Layer name: conv4_block21_1_bn\n",
            "Layer name: conv4_block21_1_relu\n",
            "Layer name: conv4_block21_2_conv\n",
            "Layer name: conv4_block21_concat\n",
            "Layer name: conv4_block22_0_bn\n",
            "Layer name: conv4_block22_0_relu\n",
            "Layer name: conv4_block22_1_conv\n",
            "Layer name: conv4_block22_1_bn\n",
            "Layer name: conv4_block22_1_relu\n",
            "Layer name: conv4_block22_2_conv\n",
            "Layer name: conv4_block22_concat\n",
            "Layer name: conv4_block23_0_bn\n",
            "Layer name: conv4_block23_0_relu\n",
            "Layer name: conv4_block23_1_conv\n",
            "Layer name: conv4_block23_1_bn\n",
            "Layer name: conv4_block23_1_relu\n",
            "Layer name: conv4_block23_2_conv\n",
            "Layer name: conv4_block23_concat\n",
            "Layer name: conv4_block24_0_bn\n",
            "Layer name: conv4_block24_0_relu\n",
            "Layer name: conv4_block24_1_conv\n",
            "Layer name: conv4_block24_1_bn\n",
            "Layer name: conv4_block24_1_relu\n",
            "Layer name: conv4_block24_2_conv\n",
            "Layer name: conv4_block24_concat\n",
            "Layer name: pool4_bn\n",
            "Layer name: pool4_relu\n",
            "Layer name: pool4_conv\n",
            "Layer name: pool4_pool\n",
            "Layer name: conv5_block1_0_bn\n",
            "Layer name: conv5_block1_0_relu\n",
            "Layer name: conv5_block1_1_conv\n",
            "Layer name: conv5_block1_1_bn\n",
            "Layer name: conv5_block1_1_relu\n",
            "Layer name: conv5_block1_2_conv\n",
            "Layer name: conv5_block1_concat\n",
            "Layer name: conv5_block2_0_bn\n",
            "Layer name: conv5_block2_0_relu\n",
            "Layer name: conv5_block2_1_conv\n",
            "Layer name: conv5_block2_1_bn\n",
            "Layer name: conv5_block2_1_relu\n",
            "Layer name: conv5_block2_2_conv\n",
            "Layer name: conv5_block2_concat\n",
            "Layer name: conv5_block3_0_bn\n",
            "Layer name: conv5_block3_0_relu\n",
            "Layer name: conv5_block3_1_conv\n",
            "Layer name: conv5_block3_1_bn\n",
            "Layer name: conv5_block3_1_relu\n",
            "Layer name: conv5_block3_2_conv\n",
            "Layer name: conv5_block3_concat\n",
            "Layer name: conv5_block4_0_bn\n",
            "Layer name: conv5_block4_0_relu\n",
            "Layer name: conv5_block4_1_conv\n",
            "Layer name: conv5_block4_1_bn\n",
            "Layer name: conv5_block4_1_relu\n",
            "Layer name: conv5_block4_2_conv\n",
            "Layer name: conv5_block4_concat\n",
            "Layer name: conv5_block5_0_bn\n",
            "Layer name: conv5_block5_0_relu\n",
            "Layer name: conv5_block5_1_conv\n",
            "Layer name: conv5_block5_1_bn\n",
            "Layer name: conv5_block5_1_relu\n",
            "Layer name: conv5_block5_2_conv\n",
            "Layer name: conv5_block5_concat\n",
            "Layer name: conv5_block6_0_bn\n",
            "Layer name: conv5_block6_0_relu\n",
            "Layer name: conv5_block6_1_conv\n",
            "Layer name: conv5_block6_1_bn\n",
            "Layer name: conv5_block6_1_relu\n",
            "Layer name: conv5_block6_2_conv\n",
            "Layer name: conv5_block6_concat\n",
            "Layer name: conv5_block7_0_bn\n",
            "Layer name: conv5_block7_0_relu\n",
            "Layer name: conv5_block7_1_conv\n",
            "Layer name: conv5_block7_1_bn\n",
            "Layer name: conv5_block7_1_relu\n",
            "Layer name: conv5_block7_2_conv\n",
            "Layer name: conv5_block7_concat\n",
            "Layer name: conv5_block8_0_bn\n",
            "Layer name: conv5_block8_0_relu\n",
            "Layer name: conv5_block8_1_conv\n",
            "Layer name: conv5_block8_1_bn\n",
            "Layer name: conv5_block8_1_relu\n",
            "Layer name: conv5_block8_2_conv\n",
            "Layer name: conv5_block8_concat\n",
            "Layer name: conv5_block9_0_bn\n",
            "Layer name: conv5_block9_0_relu\n",
            "Layer name: conv5_block9_1_conv\n",
            "Layer name: conv5_block9_1_bn\n",
            "Layer name: conv5_block9_1_relu\n",
            "Layer name: conv5_block9_2_conv\n",
            "Layer name: conv5_block9_concat\n",
            "Layer name: conv5_block10_0_bn\n",
            "Layer name: conv5_block10_0_relu\n",
            "Layer name: conv5_block10_1_conv\n",
            "Layer name: conv5_block10_1_bn\n",
            "Layer name: conv5_block10_1_relu\n",
            "Layer name: conv5_block10_2_conv\n",
            "Layer name: conv5_block10_concat\n",
            "Layer name: conv5_block11_0_bn\n",
            "Layer name: conv5_block11_0_relu\n",
            "Layer name: conv5_block11_1_conv\n",
            "Layer name: conv5_block11_1_bn\n",
            "Layer name: conv5_block11_1_relu\n",
            "Layer name: conv5_block11_2_conv\n",
            "Layer name: conv5_block11_concat\n",
            "Layer name: conv5_block12_0_bn\n",
            "Layer name: conv5_block12_0_relu\n",
            "Layer name: conv5_block12_1_conv\n",
            "Layer name: conv5_block12_1_bn\n",
            "Layer name: conv5_block12_1_relu\n",
            "Layer name: conv5_block12_2_conv\n",
            "Layer name: conv5_block12_concat\n",
            "Layer name: conv5_block13_0_bn\n",
            "Layer name: conv5_block13_0_relu\n",
            "Layer name: conv5_block13_1_conv\n",
            "Layer name: conv5_block13_1_bn\n",
            "Layer name: conv5_block13_1_relu\n",
            "Layer name: conv5_block13_2_conv\n",
            "Layer name: conv5_block13_concat\n",
            "Layer name: conv5_block14_0_bn\n",
            "Layer name: conv5_block14_0_relu\n",
            "Layer name: conv5_block14_1_conv\n",
            "Layer name: conv5_block14_1_bn\n",
            "Layer name: conv5_block14_1_relu\n",
            "Layer name: conv5_block14_2_conv\n",
            "Layer name: conv5_block14_concat\n",
            "Layer name: conv5_block15_0_bn\n",
            "Layer name: conv5_block15_0_relu\n",
            "Layer name: conv5_block15_1_conv\n",
            "Layer name: conv5_block15_1_bn\n",
            "Layer name: conv5_block15_1_relu\n",
            "Layer name: conv5_block15_2_conv\n",
            "Layer name: conv5_block15_concat\n",
            "Layer name: conv5_block16_0_bn\n",
            "Layer name: conv5_block16_0_relu\n",
            "Layer name: conv5_block16_1_conv\n",
            "Layer name: conv5_block16_1_bn\n",
            "Layer name: conv5_block16_1_relu\n",
            "Layer name: conv5_block16_2_conv\n",
            "Layer name: conv5_block16_concat\n",
            "Layer name: bn\n",
            "Layer name: relu\n",
            "Layer name: global_average_pooling2d\n",
            "Layer name: dropout\n",
            "Layer name: dense\n",
            "Layer name: dropout_1\n",
            "Layer name: dense_1\n",
            "Total layers in Cloned DenseNet model: 432\n",
            "\n",
            "Input tensor name: ensemble_input\n",
            "Passing input through each model...\n",
            "Outputs from each model:\n",
            "EfficientNet output shape: (None, 4)\n",
            "ResNet output shape: (None, 4)\n",
            "DenseNet output shape: (None, 4)\n",
            "Averaging outputs...\n",
            "Adding dense layer...\n",
            "Creating and compiling ensemble model...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The name \"functional\" is used 2 times in the model. All operation names should be unique.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-5a52706c7396>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Create and compile ensemble model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating and compiling ensemble model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mensemble_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensemble_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ensemble_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0mensemble_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/tracking.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mDotNotTrackScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_self_setattr_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         (nodes, nodes_by_depth, operations, operations_by_depth) = map_graph(\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/function.py\u001b[0m in \u001b[0;36mmap_graph\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;34mf'The name \"{name}\" is used {all_names.count(name)} '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0;34m\"times in the model. All operation names should be unique.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The name \"functional\" is used 2 times in the model. All operation names should be unique."
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model, Model, clone_model\n",
        "from tensorflow.keras.layers import Average, Input, Dense\n",
        "\n",
        "def clone_model_with_prefix(model, prefix):\n",
        "    \"\"\"\n",
        "    Clone the given model and add a unique prefix to all layer names and the model itself.\n",
        "    \"\"\"\n",
        "    cloned_model = clone_model(model)\n",
        "    cloned_model.set_weights(model.get_weights())\n",
        "\n",
        "    # Rename layers with the prefix\n",
        "    for layer in cloned_model.layers:\n",
        "        layer._name = prefix + \"_\" + layer.name  # Add prefix to each layer's name\n",
        "        print(f\"Renamed layer: {layer._name}\")  # Debug: print renamed layer names\n",
        "\n",
        "    # Assign a unique name to the model itself\n",
        "    cloned_model._name = prefix + \"_model\"\n",
        "    print(f\"Cloned model name: {cloned_model._name}\")  # Debug: print model name\n",
        "\n",
        "    return cloned_model\n",
        "\n",
        "\n",
        "\n",
        "def debug_model(model, prefix):\n",
        "    \"\"\"\n",
        "    Print debug information for the model, including its name and layers.\n",
        "    \"\"\"\n",
        "    print(f\"Debugging {prefix} model...\")\n",
        "    print(f\"Model name: {model.name}\")\n",
        "    for layer in model.layers:\n",
        "        print(f\"Layer name: {layer.name}\")\n",
        "    print(f\"Total layers in {prefix} model: {len(model.layers)}\")\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "# Load pre-trained models\n",
        "print(\"Loading models...\")\n",
        "efficientnet_model = load_model('efficientnet.h5', compile=False)\n",
        "efficientnet_model._name = \"efficientnet_model\"  # Assign unique name\n",
        "\n",
        "resnet_model = load_model('/content/drive/MyDrive/DSGP Files/Models/resnet.h5', compile=False)\n",
        "resnet_model._name = \"resnet_model\"  # Assign unique name\n",
        "\n",
        "densenet_model = load_model('densenet_model.h5', compile=False)\n",
        "densenet_model._name = \"densenet_model\"  # Assign unique name\n",
        "\n",
        "# Debugging loaded models\n",
        "debug_model(efficientnet_model, \"EfficientNet\")\n",
        "debug_model(resnet_model, \"ResNet\")\n",
        "debug_model(densenet_model, \"DenseNet\")\n",
        "\n",
        "# Clone models with unique prefixes and unique model names\n",
        "print(\"Cloning models with unique prefixes...\")\n",
        "efficientnet_model = clone_model_with_prefix(efficientnet_model, \"efficientnet\")\n",
        "resnet_model = clone_model_with_prefix(resnet_model, \"resnet\")\n",
        "densenet_model = clone_model_with_prefix(densenet_model, \"densenet\")\n",
        "\n",
        "# Debugging cloned models\n",
        "debug_model(efficientnet_model, \"Cloned EfficientNet\")\n",
        "debug_model(resnet_model, \"Cloned ResNet\")\n",
        "debug_model(densenet_model, \"Cloned DenseNet\")\n",
        "\n",
        "# Define shared input\n",
        "input_tensor = Input(shape=(224, 224, 3), name=\"ensemble_input\")\n",
        "print(f\"Input tensor name: {input_tensor.name}\")\n",
        "\n",
        "# Pass input through each model\n",
        "print(\"Passing input through each model...\")\n",
        "efficientnet_output = efficientnet_model(input_tensor)\n",
        "resnet_output = resnet_model(input_tensor)\n",
        "densenet_output = densenet_model(input_tensor)\n",
        "\n",
        "# Debugging outputs\n",
        "print(\"Outputs from each model:\")\n",
        "print(f\"EfficientNet output shape: {efficientnet_output.shape}\")\n",
        "print(f\"ResNet output shape: {resnet_output.shape}\")\n",
        "print(f\"DenseNet output shape: {densenet_output.shape}\")\n",
        "\n",
        "# Average outputs\n",
        "print(\"Averaging outputs...\")\n",
        "ensemble_output = Average(name=\"average_output\")([efficientnet_output, resnet_output, densenet_output])\n",
        "\n",
        "# Optional dense layer\n",
        "print(\"Adding dense layer...\")\n",
        "ensemble_output = Dense(4, activation='softmax', name=\"final_dense\")(ensemble_output)\n",
        "\n",
        "# Create and compile ensemble model\n",
        "print(\"Creating and compiling ensemble model...\")\n",
        "ensemble_model = Model(inputs=input_tensor, outputs=ensemble_output, name=\"ensemble_model\")\n",
        "ensemble_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Debugging ensemble model\n",
        "print(f\"Ensemble model created successfully. Model name: {ensemble_model.name}\")\n",
        "debug_model(ensemble_model, \"Ensemble Model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZrU6KYX-cPH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "def compare_models(model1, model2):\n",
        "    \"\"\"\n",
        "    Compares two models for identical structure and weights.\n",
        "    Args:\n",
        "        model1: First model to compare.\n",
        "        model2: Second model to compare.\n",
        "    Returns:\n",
        "        True if the models are identical, False otherwise.\n",
        "    \"\"\"\n",
        "    # Check if their configurations are the same\n",
        "    config1 = model1.get_config()\n",
        "    config2 = model2.get_config()\n",
        "    if config1 != config2:\n",
        "        print(\"Models have different configurations.\")\n",
        "        return False\n",
        "\n",
        "    # Check if their weights are the same\n",
        "    weights1 = model1.get_weights()\n",
        "    weights2 = model2.get_weights()\n",
        "    for w1, w2 in zip(weights1, weights2):\n",
        "        if not np.array_equal(w1, w2):\n",
        "            print(\"Models have different weights.\")\n",
        "            return False\n",
        "\n",
        "    print(\"Models are identical.\")\n",
        "    return True\n",
        "\n",
        "def compare_all_models(model_paths):\n",
        "    \"\"\"\n",
        "    Compares all models in the provided paths pairwise.\n",
        "    Args:\n",
        "        model_paths: List of paths to the models.\n",
        "    Returns:\n",
        "        None, prints comparison results.\n",
        "    \"\"\"\n",
        "    models = [load_model(path, compile=False) for path in model_paths]\n",
        "    names = ['EfficientNet', 'ResNet', 'DenseNet']\n",
        "    for i in range(len(models)):\n",
        "        for j in range(i + 1, len(models)):\n",
        "            print(f\"\\nComparing {names[i]} and {names[j]}:\")\n",
        "            identical = compare_models(models[i], models[j])\n",
        "            if identical:\n",
        "                print(f\"{names[i]} and {names[j]} are identical.\")\n",
        "            else:\n",
        "                print(f\"{names[i]} and {names[j]} are NOT identical.\")\n",
        "\n",
        "# Paths to the models\n",
        "model_paths = ['efficientnet.h5', 'resnet.h5', 'densenet_model.h5']\n",
        "\n",
        "# Compare all models\n",
        "compare_all_models(model_paths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzHtx2VjbMgx"
      },
      "source": [
        "### Stacking approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q_lUrNpMbPAW",
        "outputId": "3d2fda6c-d3b4-4e2b-c140-bd7ecd4b9ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading models...\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "Unable to synchronously open file (truncated file: eof = 90177536, sblock->base_addr = 0, stored_eof = 91800496)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-32bd66c9ff28>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mresnet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"resnet_model\"\u001b[0m  \u001b[0;31m# Assign unique name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mdensenet_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'densenet_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mdensenet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"densenet_model\"\u001b[0m  \u001b[0;31m# Assign unique name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    192\u001b[0m         )\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    559\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to synchronously open file (truncated file: eof = 90177536, sblock->base_addr = 0, stored_eof = 91800496)"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model, Model, clone_model\n",
        "from tensorflow.keras.layers import Concatenate, Input, Dense\n",
        "\n",
        "def clone_model_with_prefix(model, prefix):\n",
        "    \"\"\"\n",
        "    Clone the given model and add a unique prefix to all layer names and the model itself.\n",
        "    \"\"\"\n",
        "    cloned_model = clone_model(model)\n",
        "    cloned_model.set_weights(model.get_weights())\n",
        "\n",
        "    # Rename layers with the prefix\n",
        "    for layer in cloned_model.layers:\n",
        "        layer._name = prefix + \"_\" + layer.name  # Add prefix to each layer's name\n",
        "        print(f\"Renamed layer: {layer._name}\")  # Debug: print renamed layer names\n",
        "\n",
        "    # Assign a unique name to the model itself\n",
        "    cloned_model._name = prefix + \"_model\"\n",
        "    print(f\"Cloned model name: {cloned_model._name}\")  # Debug: print model name\n",
        "\n",
        "    return cloned_model\n",
        "\n",
        "\n",
        "def debug_model(model, prefix):\n",
        "    \"\"\"\n",
        "    Print debug information for the model, including its name and layers.\n",
        "    \"\"\"\n",
        "    print(f\"Debugging {prefix} model...\")\n",
        "    print(f\"Model name: {model.name}\")\n",
        "    for layer in model.layers:\n",
        "        print(f\"Layer name: {layer.name}\")\n",
        "    print(f\"Total layers in {prefix} model: {len(model.layers)}\")\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "# Load pre-trained models\n",
        "print(\"Loading models...\")\n",
        "efficientnet_model = load_model('efficientnet.h5', compile=False)\n",
        "efficientnet_model._name = \"efficientnet_model\"  # Assign unique name\n",
        "\n",
        "resnet_model = load_model('/content/drive/MyDrive/DSGP Files/Models/resnet.h5', compile=False)\n",
        "resnet_model._name = \"resnet_model\"  # Assign unique name\n",
        "\n",
        "densenet_model = load_model('densenet_model.h5', compile=False)\n",
        "densenet_model._name = \"densenet_model\"  # Assign unique name\n",
        "\n",
        "# Debugging loaded models\n",
        "debug_model(efficientnet_model, \"EfficientNet\")\n",
        "debug_model(resnet_model, \"ResNet\")\n",
        "debug_model(densenet_model, \"DenseNet\")\n",
        "\n",
        "# Clone models with unique prefixes and unique model names\n",
        "print(\"Cloning models with unique prefixes...\")\n",
        "efficientnet_model = clone_model_with_prefix(efficientnet_model, \"efficientnet\")\n",
        "resnet_model = clone_model_with_prefix(resnet_model, \"resnet\")\n",
        "densenet_model = clone_model_with_prefix(densenet_model, \"densenet\")\n",
        "\n",
        "# Debugging cloned models\n",
        "debug_model(efficientnet_model, \"Cloned EfficientNet\")\n",
        "debug_model(resnet_model, \"Cloned ResNet\")\n",
        "debug_model(densenet_model, \"Cloned DenseNet\")\n",
        "\n",
        "# Define shared input\n",
        "input_tensor = Input(shape=(224, 224, 3), name=\"ensemble_input\")\n",
        "print(f\"Input tensor name: {input_tensor.name}\")\n",
        "\n",
        "# Pass input through each model\n",
        "print(\"Passing input through each model...\")\n",
        "efficientnet_output = efficientnet_model(input_tensor)\n",
        "resnet_output = resnet_model(input_tensor)\n",
        "densenet_output = densenet_model(input_tensor)\n",
        "\n",
        "# Debugging outputs\n",
        "print(\"Outputs from each model:\")\n",
        "print(f\"EfficientNet output shape: {efficientnet_output.shape}\")\n",
        "print(f\"ResNet output shape: {resnet_output.shape}\")\n",
        "print(f\"DenseNet output shape: {densenet_output.shape}\")\n",
        "\n",
        "# Concatenate outputs from all models\n",
        "print(\"Concatenating outputs...\")\n",
        "stacked_output = Concatenate(name=\"stacked_output\")([efficientnet_output, resnet_output, densenet_output])\n",
        "\n",
        "# Add meta-model (dense layer) for stacking\n",
        "print(\"Adding meta-model...\")\n",
        "meta_output = Dense(4, activation='softmax', name=\"meta_model\")(stacked_output)\n",
        "\n",
        "# Create and compile stacking ensemble model\n",
        "print(\"Creating and compiling stacking ensemble model...\")\n",
        "ensemble_model = Model(inputs=input_tensor, outputs=meta_output, name=\"stacking_ensemble_model\")\n",
        "ensemble_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Debugging ensemble model\n",
        "print(f\"Stacking ensemble model created successfully. Model name: {ensemble_model.name}\")\n",
        "debug_model(ensemble_model, \"Stacking Ensemble Model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3 Custom CNN"
      ],
      "metadata": {
        "id": "VFPly1k5aHlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Define CNN Model\n",
        "def build_custom_cnn(input_shape=(224, 224, 3), num_classes=4):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001), input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        GlobalAveragePooling2D(),  # Better than Flatten to reduce overfitting\n",
        "\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.4),\n",
        "        Dense(num_classes, activation='softmax')  # Multi-class classification\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build Model\n",
        "model = build_custom_cnn()\n",
        "\n",
        "# Compile Model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=30,  # Adjust as needed\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Save Model\n",
        "model.save('/content/custom_cnn_model.h5')\n",
        "\n",
        "print(\"Custom CNN model trained and saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlrqmvhbaMjT",
        "outputId": "7afc3516-4ee3-4196-c507-b6738702e24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 542ms/step - accuracy: 0.4666 - loss: 1.4267 - val_accuracy: 0.5000 - val_loss: 1.5222 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 559ms/step - accuracy: 0.7708 - loss: 0.7638 - val_accuracy: 0.9375 - val_loss: 0.4208 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m533s\u001b[0m 533ms/step - accuracy: 0.8063 - loss: 0.6813 - val_accuracy: 0.7500 - val_loss: 0.8077 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 533ms/step - accuracy: 0.8180 - loss: 0.6623 - val_accuracy: 0.9375 - val_loss: 0.3534 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m532s\u001b[0m 532ms/step - accuracy: 0.8295 - loss: 0.6395 - val_accuracy: 0.9375 - val_loss: 0.4867 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 534ms/step - accuracy: 0.8347 - loss: 0.6280 - val_accuracy: 0.9062 - val_loss: 0.7450 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530ms/step - accuracy: 0.8412 - loss: 0.6113\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 531ms/step - accuracy: 0.8412 - loss: 0.6113 - val_accuracy: 0.8750 - val_loss: 0.4633 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m532s\u001b[0m 532ms/step - accuracy: 0.8617 - loss: 0.5460 - val_accuracy: 0.9062 - val_loss: 0.3063 - learning_rate: 5.0000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 521ms/step - accuracy: 0.8669 - loss: 0.5132 - val_accuracy: 0.9375 - val_loss: 0.3401 - learning_rate: 5.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 525ms/step - accuracy: 0.8683 - loss: 0.4976 - val_accuracy: 0.9375 - val_loss: 0.3707 - learning_rate: 5.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m 224/1000\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:45\u001b[0m 523ms/step - accuracy: 0.8643 - loss: 0.4809"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Define CNN Model\n",
        "def build_custom_cnn(input_shape=(224, 224, 3), num_classes=4):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001), input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        GlobalAveragePooling2D(),  # Better than Flatten to reduce overfitting\n",
        "\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.4),\n",
        "        Dense(num_classes, activation='softmax')  # Multi-class classification\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build Model\n",
        "model = build_custom_cnn()\n",
        "\n",
        "# Compile Model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=30,  # Adjust as needed\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Save Model\n",
        "model.save('/content/custom_cnn_model.h5')\n",
        "\n",
        "print(\"Custom CNN model trained and saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afc3516-4ee3-4196-c507-b6738702e24c",
        "id": "ZHFqyaUArdVd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 542ms/step - accuracy: 0.4666 - loss: 1.4267 - val_accuracy: 0.5000 - val_loss: 1.5222 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 559ms/step - accuracy: 0.7708 - loss: 0.7638 - val_accuracy: 0.9375 - val_loss: 0.4208 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m533s\u001b[0m 533ms/step - accuracy: 0.8063 - loss: 0.6813 - val_accuracy: 0.7500 - val_loss: 0.8077 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 533ms/step - accuracy: 0.8180 - loss: 0.6623 - val_accuracy: 0.9375 - val_loss: 0.3534 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m532s\u001b[0m 532ms/step - accuracy: 0.8295 - loss: 0.6395 - val_accuracy: 0.9375 - val_loss: 0.4867 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 534ms/step - accuracy: 0.8347 - loss: 0.6280 - val_accuracy: 0.9062 - val_loss: 0.7450 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m 100/1000\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:47\u001b[0m 520ms/step - accuracy: 0.8420 - loss: 0.5952"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the trained model (if not already in memory)\n",
        "model = load_model('/content/my_trained_model.h5')  # Replace with your model's path\n",
        "\n",
        "# Get true labels and predictions\n",
        "true_labels = test_generator.classes\n",
        "class_names = list(test_generator.class_indices.keys())  # Get class names\n",
        "pred_probs = model.predict(test_generator)  # Get probabilities\n",
        "pred_labels = np.argmax(pred_probs, axis=1)  # Convert probabilities to class labels\n",
        "\n",
        "# Compute classification report\n",
        "print(\"\\n📊 Classification Report:\")\n",
        "print(classification_report(true_labels, pred_labels, target_names=class_names))\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Compute AUC-ROC for multi-class classification\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(len(class_names)):\n",
        "    fpr, tpr, _ = roc_curve(true_labels == i, pred_probs[:, i])  # Compute ROC for each class\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for reference\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"AUC-ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Training & Validation Accuracy and Loss\n",
        "history = model.history  # Ensure history object is available (or load from training)\n",
        "\n",
        "# Accuracy Plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "# Loss Plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ke8f1Y6Cdk-Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1R53UTIiHVB8eS87AMzjdm3zcHzx4B13A",
      "authorship_tag": "ABX9TyMKN0FJQYpc3zvG7jRI4FY2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}